{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (2.23.0)\n",
      "Requirement already satisfied: black in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (19.10b0)\n",
      "Requirement already satisfied: nb_black in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (1.0.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from requests) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from requests) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: regex in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from black) (2020.4.4)\n",
      "Requirement already satisfied: typed-ast>=1.4.0 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from black) (1.4.1)\n",
      "Requirement already satisfied: click>=6.5 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from black) (7.1.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from black) (19.3.0)\n",
      "Requirement already satisfied: toml>=0.9.4 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from black) (0.10.0)\n",
      "Requirement already satisfied: appdirs in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from black) (1.4.3)\n",
      "Requirement already satisfied: pathspec<1,>=0.6 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from black) (0.8.0)\n",
      "Requirement already satisfied: ipython in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from nb_black) (7.13.0)\n",
      "Requirement already satisfied: pygments in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (2.6.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (4.8.0)\n",
      "Requirement already satisfied: decorator in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (4.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (4.3.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (41.2.0)\n",
      "Requirement already satisfied: backcall in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (3.0.3)\n",
      "Requirement already satisfied: pickleshare in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (0.16.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->nb_black) (0.6.0)\n",
      "Requirement already satisfied: six in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from traitlets>=4.2->ipython->nb_black) (1.14.0)\n",
      "Requirement already satisfied: ipython-genutils in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from traitlets>=4.2->ipython->nb_black) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nb_black) (0.1.8)\n",
      "Requirement already satisfied: parso>=0.5.2 in ./.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages (from jedi>=0.10->ipython->nb_black) (0.6.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"!pip install requests black nb_black\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"!pip install requests black nb_black\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install requests black nb_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import os\\nfrom pathlib import Path\\n\\nfrom requests import get\\nimport pandas as pd\\nimport numpy as np\\nimport torch\\n\\nnp.random.seed(0)\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.model_selection import (\\n    RandomizedSearchCV,\\n    StratifiedKFold,\\n    ParameterSampler,\\n    StratifiedShuffleSplit,\\n)\\nfrom pytorch_tabnet.tab_model import TabNetClassifier\";\n",
       "                var nbb_formatted_code = \"import os\\nfrom pathlib import Path\\n\\nfrom requests import get\\nimport pandas as pd\\nimport numpy as np\\nimport torch\\n\\nnp.random.seed(0)\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.model_selection import (\\n    RandomizedSearchCV,\\n    StratifiedKFold,\\n    ParameterSampler,\\n    StratifiedShuffleSplit,\\n)\\nfrom pytorch_tabnet.tab_model import TabNetClassifier\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    ParameterSampler,\n",
    "    StratifiedShuffleSplit,\n",
    ")\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"def download(url, out, force=False, verify=True):\\n    out.parent.mkdir(parents=True, exist_ok=True)\\n    if force:\\n        print(f\\\"Removing file at {str(out)}\\\")\\n        out.unlink()\\n\\n    if out.exists():\\n        print(\\\"File already exists.\\\")\\n        return\\n    print(f\\\"Downloading {url} at {str(out)} ...\\\")\\n    # open in binary mode\\n    with out.open(mode=\\\"wb\\\") as file:\\n        # get request\\n        response = get(url, verify=verify)\\n        for chunk in response.iter_content(100000):\\n            # write to file\\n            file.write(chunk)\";\n",
       "                var nbb_formatted_code = \"def download(url, out, force=False, verify=True):\\n    out.parent.mkdir(parents=True, exist_ok=True)\\n    if force:\\n        print(f\\\"Removing file at {str(out)}\\\")\\n        out.unlink()\\n\\n    if out.exists():\\n        print(\\\"File already exists.\\\")\\n        return\\n    print(f\\\"Downloading {url} at {str(out)} ...\\\")\\n    # open in binary mode\\n    with out.open(mode=\\\"wb\\\") as file:\\n        # get request\\n        response = get(url, verify=verify)\\n        for chunk in response.iter_content(100000):\\n            # write to file\\n            file.write(chunk)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download(url, out, force=False, verify=True):\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if force:\n",
    "        print(f\"Removing file at {str(out)}\")\n",
    "        out.unlink()\n",
    "\n",
    "    if out.exists():\n",
    "        print(\"File already exists.\")\n",
    "        return\n",
    "    print(f\"Downloading {url} at {str(out)} ...\")\n",
    "    # open in binary mode\n",
    "    with out.open(mode=\"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url, verify=verify)\n",
    "        for chunk in response.iter_content(100000):\n",
    "            # write to file\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"UNKNOWN_VALUE = [\\\"Unkn0wnV@lue\\\"]\\n\\n\\nclass SafeLabelEncoder(LabelEncoder):\\n    \\\"\\\"\\\"\\n    Safe label encoder, encoding every unknown value as Unkn0wnV@lue.\\n    \\\"\\\"\\\"\\n\\n    def fit(self, y):\\n        \\\"\\\"\\\"\\n        Fit the label encoder, by casting the numpy array as a string, then adding the code for unknown.\\n        \\n        Parameters\\n        ----------\\n        y : numpy array\\n            the values to fit\\n        \\n        Returns\\n        -------\\n        SafeLabelEncoder\\n            itself, fitted\\n        \\\"\\\"\\\"\\n        return super().fit(np.concatenate((y.astype(\\\"str\\\"), UNKNOWN_VALUE)))\\n\\n    def fit_transform(self, y):\\n        \\\"\\\"\\\"\\n        Fit the encoder, then transform the input data and returns it.\\n        \\n        Parameters\\n        ----------\\n        y : numpy array\\n            the values to fit\\n        \\n        Returns\\n        -------\\n        numpy array\\n            the encoded data\\n        \\\"\\\"\\\"\\n        self.fit(y)\\n        return super().transform(y)\\n\\n    def transform(self, y):\\n        \\\"\\\"\\\"\\n        Transform the input data and returns it.\\n        \\n        Parameters\\n        ----------\\n        y : numpy array\\n            the values to fit\\n        \\n        Returns\\n        -------\\n        numpy array\\n            the encoded data\\n        \\\"\\\"\\\"\\n        return super().transform(\\n            np.where(\\n                np.isin(y.astype(\\\"str\\\"), self.classes_), y.astype(\\\"str\\\"), UNKNOWN_VALUE\\n            )\\n        )\";\n",
       "                var nbb_formatted_code = \"UNKNOWN_VALUE = [\\\"Unkn0wnV@lue\\\"]\\n\\n\\nclass SafeLabelEncoder(LabelEncoder):\\n    \\\"\\\"\\\"\\n    Safe label encoder, encoding every unknown value as Unkn0wnV@lue.\\n    \\\"\\\"\\\"\\n\\n    def fit(self, y):\\n        \\\"\\\"\\\"\\n        Fit the label encoder, by casting the numpy array as a string, then adding the code for unknown.\\n        \\n        Parameters\\n        ----------\\n        y : numpy array\\n            the values to fit\\n        \\n        Returns\\n        -------\\n        SafeLabelEncoder\\n            itself, fitted\\n        \\\"\\\"\\\"\\n        return super().fit(np.concatenate((y.astype(\\\"str\\\"), UNKNOWN_VALUE)))\\n\\n    def fit_transform(self, y):\\n        \\\"\\\"\\\"\\n        Fit the encoder, then transform the input data and returns it.\\n        \\n        Parameters\\n        ----------\\n        y : numpy array\\n            the values to fit\\n        \\n        Returns\\n        -------\\n        numpy array\\n            the encoded data\\n        \\\"\\\"\\\"\\n        self.fit(y)\\n        return super().transform(y)\\n\\n    def transform(self, y):\\n        \\\"\\\"\\\"\\n        Transform the input data and returns it.\\n        \\n        Parameters\\n        ----------\\n        y : numpy array\\n            the values to fit\\n        \\n        Returns\\n        -------\\n        numpy array\\n            the encoded data\\n        \\\"\\\"\\\"\\n        return super().transform(\\n            np.where(\\n                np.isin(y.astype(\\\"str\\\"), self.classes_), y.astype(\\\"str\\\"), UNKNOWN_VALUE\\n            )\\n        )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "UNKNOWN_VALUE = [\"Unkn0wnV@lue\"]\n",
    "\n",
    "\n",
    "class SafeLabelEncoder(LabelEncoder):\n",
    "    \"\"\"\n",
    "    Safe label encoder, encoding every unknown value as Unkn0wnV@lue.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, y):\n",
    "        \"\"\"\n",
    "        Fit the label encoder, by casting the numpy array as a string, then adding the code for unknown.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            the values to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        SafeLabelEncoder\n",
    "            itself, fitted\n",
    "        \"\"\"\n",
    "        return super().fit(np.concatenate((y.astype(\"str\"), UNKNOWN_VALUE)))\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        \"\"\"\n",
    "        Fit the encoder, then transform the input data and returns it.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            the values to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy array\n",
    "            the encoded data\n",
    "        \"\"\"\n",
    "        self.fit(y)\n",
    "        return super().transform(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "        \"\"\"\n",
    "        Transform the input data and returns it.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            the values to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy array\n",
    "            the encoded data\n",
    "        \"\"\"\n",
    "        return super().transform(\n",
    "            np.where(\n",
    "                np.isin(y.astype(\"str\"), self.classes_), y.astype(\"str\"), UNKNOWN_VALUE\n",
    "            )\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download census-income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing file at /work/data/census-income.csv\n",
      "Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data at /work/data/census-income.csv ...\n",
      "Removing file at /work/data/census-income_test.csv\n",
      "Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test at /work/data/census-income_test.csv ...\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"url = \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\\\"\\nurl_test = \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\\\"\\n\\ndataset_name = \\\"census-income\\\"\\nout = Path(os.getcwd() + \\\"/data/\\\" + dataset_name + \\\".csv\\\")\\nout_test = Path(os.getcwd() + \\\"/data/\\\" + dataset_name + \\\"_test.csv\\\")\\n\\ndownload(url, out, force=True)\\ndownload(url_test, out_test, force=True)\";\n",
       "                var nbb_formatted_code = \"url = \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\\\"\\nurl_test = \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\\\"\\n\\ndataset_name = \\\"census-income\\\"\\nout = Path(os.getcwd() + \\\"/data/\\\" + dataset_name + \\\".csv\\\")\\nout_test = Path(os.getcwd() + \\\"/data/\\\" + dataset_name + \\\"_test.csv\\\")\\n\\ndownload(url, out, force=True)\\ndownload(url_test, out_test, force=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "\n",
    "dataset_name = \"census-income\"\n",
    "out = Path(os.getcwd() + \"/data/\" + dataset_name + \".csv\")\n",
    "out_test = Path(os.getcwd() + \"/data/\" + dataset_name + \"_test.csv\")\n",
    "\n",
    "download(url, out, force=True)\n",
    "download(url_test, out_test, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"cols = [\\n    \\\"age\\\",\\n    \\\"workclass\\\",\\n    \\\"fnlwgt\\\",\\n    \\\"education\\\",\\n    \\\"education-num\\\",\\n    \\\"marital-status\\\",\\n    \\\"occupation\\\",\\n    \\\"relationship\\\",\\n    \\\"race\\\",\\n    \\\"sex\\\",\\n    \\\"capital-gain\\\",\\n    \\\"capital-loss\\\",\\n    \\\"hours-per-week\\\",\\n    \\\"native-country\\\",\\n    \\\"target\\\",\\n]\";\n",
       "                var nbb_formatted_code = \"cols = [\\n    \\\"age\\\",\\n    \\\"workclass\\\",\\n    \\\"fnlwgt\\\",\\n    \\\"education\\\",\\n    \\\"education-num\\\",\\n    \\\"marital-status\\\",\\n    \\\"occupation\\\",\\n    \\\"relationship\\\",\\n    \\\"race\\\",\\n    \\\"sex\\\",\\n    \\\"capital-gain\\\",\\n    \\\"capital-loss\\\",\\n    \\\"hours-per-week\\\",\\n    \\\"native-country\\\",\\n    \\\"target\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"target\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"train = pd.read_csv(out, names=cols)\\ntest = pd.read_csv(out_test, names=cols, skiprows=2)\\ntarget = \\\"target\\\"\\n\\ntrain[target] = train[target].str.strip()\\n# Test has . in label, let's clean it\\ntest[target] = test[target].str.strip().str.strip(\\\".\\\")\";\n",
       "                var nbb_formatted_code = \"train = pd.read_csv(out, names=cols)\\ntest = pd.read_csv(out_test, names=cols, skiprows=2)\\ntarget = \\\"target\\\"\\n\\ntrain[target] = train[target].str.strip()\\n# Test has . in label, let's clean it\\ntest[target] = test[target].str.strip().str.strip(\\\".\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(out, names=cols)\n",
    "test = pd.read_csv(out_test, names=cols, skiprows=2)\n",
    "target = \"target\"\n",
    "\n",
    "train[target] = train[target].str.strip()\n",
    "# Test has . in label, let's clean it\n",
    "test[target] = test[target].str.strip().str.strip(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['native-country',\n",
       " 'relationship',\n",
       " 'fnlwgt',\n",
       " 'workclass',\n",
       " 'hours-per-week',\n",
       " 'education-num',\n",
       " 'age',\n",
       " 'marital-status',\n",
       " 'race',\n",
       " 'capital-gain',\n",
       " 'education',\n",
       " 'occupation',\n",
       " 'sex',\n",
       " 'capital-loss']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"used_columns = list(set(train.columns.tolist()) - set([target]) - set([\\\"Set\\\"]))\\nused_columns\";\n",
       "                var nbb_formatted_code = \"used_columns = list(set(train.columns.tolist()) - set([target]) - set([\\\"Set\\\"]))\\nused_columns\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "used_columns = list(set(train.columns.tolist()) - set([target]) - set([\"Set\"]))\n",
    "used_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "Label encode categorical features and fill empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['native-country', 'relationship', 'workclass', 'hours-per-week',\n",
      "       'education-num', 'age', 'marital-status', 'race', 'capital-gain',\n",
      "       'education', 'occupation', 'sex', 'capital-loss'],\n",
      "      dtype='object')\n",
      "Index(['fnlwgt'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"nunique = train[used_columns].nunique()\\ntypes = train[used_columns].dtypes\\n\\ncat_cols = train[used_columns].columns[(nunique < 200) | (types == \\\"object\\\")]\\nother_cols = train[used_columns].columns[~train[used_columns].columns.isin(cat_cols)]\\nprint(cat_cols)\\nprint(other_cols)\";\n",
       "                var nbb_formatted_code = \"nunique = train[used_columns].nunique()\\ntypes = train[used_columns].dtypes\\n\\ncat_cols = train[used_columns].columns[(nunique < 200) | (types == \\\"object\\\")]\\nother_cols = train[used_columns].columns[~train[used_columns].columns.isin(cat_cols)]\\nprint(cat_cols)\\nprint(other_cols)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nunique = train[used_columns].nunique()\n",
    "types = train[used_columns].dtypes\n",
    "\n",
    "cat_cols = train[used_columns].columns[(nunique < 200) | (types == \"object\")]\n",
    "other_cols = train[used_columns].columns[~train[used_columns].columns.isin(cat_cols)]\n",
    "print(cat_cols)\n",
    "print(other_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"nunique[\\\"education\\\"]\";\n",
       "                var nbb_formatted_code = \"nunique[\\\"education\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nunique[\"education\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# Fillna\\ntrain[cat_cols] = train[cat_cols].astype(\\\"str\\\")\\ntrain[other_cols] = train[other_cols].fillna(train[other_cols].mean())\\n\\ntest[cat_cols] = test[cat_cols].astype(\\\"str\\\")\\ntest[other_cols] = test[other_cols].fillna(train[other_cols].mean())\";\n",
       "                var nbb_formatted_code = \"# Fillna\\ntrain[cat_cols] = train[cat_cols].astype(\\\"str\\\")\\ntrain[other_cols] = train[other_cols].fillna(train[other_cols].mean())\\n\\ntest[cat_cols] = test[cat_cols].astype(\\\"str\\\")\\ntest[other_cols] = test[other_cols].fillna(train[other_cols].mean())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fillna\n",
    "train[cat_cols] = train[cat_cols].astype(\"str\")\n",
    "train[other_cols] = train[other_cols].fillna(train[other_cols].mean())\n",
    "\n",
    "test[cat_cols] = test[cat_cols].astype(\"str\")\n",
    "test[other_cols] = test[other_cols].fillna(train[other_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"train.isnull().sum().sum()\";\n",
       "                var nbb_formatted_code = \"train.isnull().sum().sum()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'native-country': SafeLabelEncoder(),\n",
       " 'relationship': SafeLabelEncoder(),\n",
       " 'workclass': SafeLabelEncoder(),\n",
       " 'hours-per-week': SafeLabelEncoder(),\n",
       " 'education-num': SafeLabelEncoder(),\n",
       " 'age': SafeLabelEncoder(),\n",
       " 'marital-status': SafeLabelEncoder(),\n",
       " 'race': SafeLabelEncoder(),\n",
       " 'capital-gain': SafeLabelEncoder(),\n",
       " 'education': SafeLabelEncoder(),\n",
       " 'occupation': SafeLabelEncoder(),\n",
       " 'sex': SafeLabelEncoder(),\n",
       " 'capital-loss': SafeLabelEncoder(),\n",
       " 'target': SafeLabelEncoder()}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"enc = {}\\nfor col in cat_cols:\\n    label_enc = SafeLabelEncoder()\\n    enc[col] = label_enc\\n    train[col] = label_enc.fit_transform(train[col])\\n    test[col] = label_enc.transform(test[col])\\nenc[target] = SafeLabelEncoder()\\ntrain[target] = enc[target].fit_transform(train[target])\\ntest[target] = enc[target].transform(test[target])\\n\\nenc\";\n",
       "                var nbb_formatted_code = \"enc = {}\\nfor col in cat_cols:\\n    label_enc = SafeLabelEncoder()\\n    enc[col] = label_enc\\n    train[col] = label_enc.fit_transform(train[col])\\n    test[col] = label_enc.transform(test[col])\\nenc[target] = SafeLabelEncoder()\\ntrain[target] = enc[target].fit_transform(train[target])\\ntest[target] = enc[target].transform(test[target])\\n\\nenc\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = {}\n",
    "for col in cat_cols:\n",
    "    label_enc = SafeLabelEncoder()\n",
    "    enc[col] = label_enc\n",
    "    train[col] = label_enc.fit_transform(train[col])\n",
    "    test[col] = label_enc.transform(test[col])\n",
    "enc[target] = SafeLabelEncoder()\n",
    "train[target] = enc[target].fit_transform(train[target])\n",
    "test[target] = enc[target].transform(test[target])\n",
    "\n",
    "enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define categorical features for categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "[43, 7, 10, 95, 17, 74, 8, 6, 120, 17, 16, 3, 93]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"unused_feat = [\\\"Set\\\"]\\n\\ncat_idxs = [i for i, f in enumerate(used_columns) if f in cat_cols]\\ncat_dims = [len(enc[f].classes_) for f in used_columns if f in cat_cols]\\nprint(cat_idxs)\\nprint(cat_dims)\";\n",
       "                var nbb_formatted_code = \"unused_feat = [\\\"Set\\\"]\\n\\ncat_idxs = [i for i, f in enumerate(used_columns) if f in cat_cols]\\ncat_dims = [len(enc[f].classes_) for f in used_columns if f in cat_cols]\\nprint(cat_idxs)\\nprint(cat_dims)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unused_feat = [\"Set\"]\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(used_columns) if f in cat_cols]\n",
    "cat_dims = [len(enc[f].classes_) for f in used_columns if f in cat_cols]\n",
    "print(cat_idxs)\n",
    "print(cat_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"X = train[used_columns].values\\ny = train[target].values\\n\\n# Test here should be ignored for training, only purpose is benching with paper values\\nX_test = test[used_columns].values\\ny_test = test[target].values\";\n",
       "                var nbb_formatted_code = \"X = train[used_columns].values\\ny = train[target].values\\n\\n# Test here should be ignored for training, only purpose is benching with paper values\\nX_test = test[used_columns].values\\ny_test = test[target].values\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = train[used_columns].values\n",
    "y = train[target].values\n",
    "\n",
    "# Test here should be ignored for training, only purpose is benching with paper values\n",
    "X_test = test[used_columns].values\n",
    "y_test = test[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"from scipy.stats import randint, uniform, loguniform\";\n",
       "                var nbb_formatted_code = \"from scipy.stats import randint, uniform, loguniform\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import randint, uniform, loguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08558895, 0.1640912 , 0.10570046, 0.08428304, 0.05245378,\n",
       "       0.12512806, 0.05539206, 0.32741274, 0.43374464, 0.04481833])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"loguniform(0.01, 0.5).rvs(size=10)  # 1e-2 ou 1e-3 \\u00e0 1e-6\";\n",
       "                var nbb_formatted_code = \"loguniform(0.01, 0.5).rvs(size=10)  # 1e-2 ou 1e-3 \\u00e0 1e-6\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loguniform(0.01, 0.5).rvs(size=10)  # 1e-2 ou 1e-3 Ã  1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"def emb_generator(cat_dim_list, max_dim):\\n    return [min(nb // 2, max_dim) for nb in cat_dim_list]\";\n",
       "                var nbb_formatted_code = \"def emb_generator(cat_dim_list, max_dim):\\n    return [min(nb // 2, max_dim) for nb in cat_dim_list]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def emb_generator(cat_dim_list, max_dim):\n",
    "    return [min(nb // 2, max_dim) for nb in cat_dim_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"def log_emb_generator(cat_dim_list, max_dim):\\n    return [min(np.log2(nb).astype(\\\"int\\\"), max_dim) for nb in cat_dim_list]\";\n",
       "                var nbb_formatted_code = \"def log_emb_generator(cat_dim_list, max_dim):\\n    return [min(np.log2(nb).astype(\\\"int\\\"), max_dim) for nb in cat_dim_list]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def log_emb_generator(cat_dim_list, max_dim):\n",
    "    return [min(np.log2(nb).astype(\"int\"), max_dim) for nb in cat_dim_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 3, 5, 4, 5, 3, 2, 5, 4, 4, 1, 5]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"log_emb_generator(cat_dims, 5)\";\n",
       "                var nbb_formatted_code = \"log_emb_generator(cat_dims, 5)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_emb_generator(cat_dims, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"# Let's generate embedding size based on cat dims\\n# cat_emb_dim_list = []\\n# for max_dim in [1, 5, 10, 20, 50]:\\n#    cat_emb_dim_list.append([min(nb // 2, max_dim) for nb in cat_dims])\\n\\nnum_workers = os.cpu_count() if torch.cuda.is_available() else 0\\n\\ngrid = {\\n    # Model params\\n    \\\"n_a\\\": randint(8,65),\\n    # \\\"n_d\\\": [8], #\\n    \\\"emb_generator\\\": [emb_generator, log_emb_generator],\\n    \\\"max_emd_dims\\\": [1, 2, 5, 10, 20, 50],\\n    #\\\"cat_emb_dim\\\": cat_emb_dim_list,\\n    \\\"n_independent\\\": randint(1,6),\\n    \\\"n_shared\\\": randint(0,6),\\n    \\\"n_steps\\\": randint(2,11),\\n    \\\"clip_value\\\": [1],\\n    \\\"gamma\\\": uniform(1, 2),\\n    \\\"momentum\\\": loguniform(0.01, 0.5), # [0.1, 0.05, 0.02, 0.005],\\n    \\\"lambda_sparse\\\": loguniform(1e-6, 1e-1), # [0.1, 0.01, 0.001], 1e-2 ou 1e-3 \\u00e0 1e-6\\n    \\\"lr\\\": [0.1], #, 0.02], #, 0.02, 0.001],\\n    \\\"verbose\\\": [1],\\n    # optimizer_fn\\n    \\n    # Fit params\\n    \\\"patience\\\":[5],\\n    \\\"max_epochs\\\":[1000],\\n    \\\"num_workers\\\":[num_workers],\\n    \\\"drop_last\\\":[False],\\n    \\\"batch_size\\\":[1024, 2048, 4096, 8192],\\n    \\\"virtual_batch_size\\\":[128, 256, 512],\\n}\";\n",
       "                var nbb_formatted_code = \"# Let's generate embedding size based on cat dims\\n# cat_emb_dim_list = []\\n# for max_dim in [1, 5, 10, 20, 50]:\\n#    cat_emb_dim_list.append([min(nb // 2, max_dim) for nb in cat_dims])\\n\\nnum_workers = os.cpu_count() if torch.cuda.is_available() else 0\\n\\ngrid = {\\n    # Model params\\n    \\\"n_a\\\": randint(8, 65),\\n    # \\\"n_d\\\": [8], #\\n    \\\"emb_generator\\\": [emb_generator, log_emb_generator],\\n    \\\"max_emd_dims\\\": [1, 2, 5, 10, 20, 50],\\n    # \\\"cat_emb_dim\\\": cat_emb_dim_list,\\n    \\\"n_independent\\\": randint(1, 6),\\n    \\\"n_shared\\\": randint(0, 6),\\n    \\\"n_steps\\\": randint(2, 11),\\n    \\\"clip_value\\\": [1],\\n    \\\"gamma\\\": uniform(1, 2),\\n    \\\"momentum\\\": loguniform(0.01, 0.5),  # [0.1, 0.05, 0.02, 0.005],\\n    \\\"lambda_sparse\\\": loguniform(1e-6, 1e-1),  # [0.1, 0.01, 0.001], 1e-2 ou 1e-3 \\u00e0 1e-6\\n    \\\"lr\\\": [0.1],  # , 0.02], #, 0.02, 0.001],\\n    \\\"verbose\\\": [1],\\n    # optimizer_fn\\n    # Fit params\\n    \\\"patience\\\": [5],\\n    \\\"max_epochs\\\": [1000],\\n    \\\"num_workers\\\": [num_workers],\\n    \\\"drop_last\\\": [False],\\n    \\\"batch_size\\\": [1024, 2048, 4096, 8192],\\n    \\\"virtual_batch_size\\\": [128, 256, 512],\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's generate embedding size based on cat dims\n",
    "# cat_emb_dim_list = []\n",
    "# for max_dim in [1, 5, 10, 20, 50]:\n",
    "#    cat_emb_dim_list.append([min(nb // 2, max_dim) for nb in cat_dims])\n",
    "\n",
    "num_workers = os.cpu_count() if torch.cuda.is_available() else 0\n",
    "\n",
    "grid = {\n",
    "    # Model params\n",
    "    \"n_a\": randint(8,65),\n",
    "    # \"n_d\": [8], #\n",
    "    \"emb_generator\": [emb_generator, log_emb_generator],\n",
    "    \"max_emd_dims\": [1, 2, 5, 10, 20, 50],\n",
    "    #\"cat_emb_dim\": cat_emb_dim_list,\n",
    "    \"n_independent\": randint(1,6),\n",
    "    \"n_shared\": randint(0,6),\n",
    "    \"n_steps\": randint(2,11),\n",
    "    \"clip_value\": [1],\n",
    "    \"gamma\": uniform(1, 2),\n",
    "    \"momentum\": loguniform(0.01, 0.5), # [0.1, 0.05, 0.02, 0.005],\n",
    "    \"lambda_sparse\": loguniform(1e-6, 1e-1), # [0.1, 0.01, 0.001], 1e-2 ou 1e-3 Ã  1e-6\n",
    "    \"lr\": [0.1], #, 0.02], #, 0.02, 0.001],\n",
    "    \"verbose\": [1],\n",
    "    # optimizer_fn\n",
    "    \n",
    "    # Fit params\n",
    "    \"patience\":[5],\n",
    "    \"max_epochs\":[1000],\n",
    "    \"num_workers\":[num_workers],\n",
    "    \"drop_last\":[False],\n",
    "    \"batch_size\":[1024, 2048, 4096, 8192],\n",
    "    \"virtual_batch_size\":[128, 256, 512],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"outer_split = 2\\nouter_test = 0.2\\ninner_split = 1\\ninner_test = 0.2\\nn_iter = 60\";\n",
       "                var nbb_formatted_code = \"outer_split = 2\\nouter_test = 0.2\\ninner_split = 1\\ninner_test = 0.2\\nn_iter = 60\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outer_split = 2\n",
    "outer_test = 0.2\n",
    "inner_split = 1\n",
    "inner_test = 0.2\n",
    "n_iter = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"MODEL_PARAMS_KEYS = list(TabNetClassifier().get_params().keys())\";\n",
       "                var nbb_formatted_code = \"MODEL_PARAMS_KEYS = list(TabNetClassifier().get_params().keys())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_PARAMS_KEYS = list(TabNetClassifier().get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54126 |  0.63383 |   6.1       \n",
      "| 2     | 0.77811 |  0.76281 |   11.3      \n",
      "| 3     | 0.81333 |  0.78549 |   16.9      \n",
      "| 4     | 0.83735 |  0.80371 |   22.3      \n",
      "| 5     | 0.85264 |  0.82765 |   27.5      \n",
      "| 6     | 0.86921 |  0.82433 |   32.5      \n",
      "| 7     | 0.88870 |  0.86473 |   37.7      \n",
      "| 8     | 0.89177 |  0.86091 |   43.0      \n",
      "| 9     | 0.88985 |  0.82435 |   48.3      \n",
      "| 10    | 0.88559 |  0.89799 |   53.5      \n",
      "| 11    | 0.90417 |  0.89571 |   58.5      \n",
      "| 12    | 0.89117 |  0.88243 |   63.5      \n",
      "| 13    | 0.89903 |  0.87936 |   68.5      \n",
      "| 14    | 0.90031 |  0.89862 |   73.6      \n",
      "| 15    | 0.90562 |  0.90060 |   78.8      \n",
      "| 16    | 0.90780 |  0.89944 |   83.9      \n",
      "| 17    | 0.89975 |  0.89452 |   89.0      \n",
      "| 18    | 0.89135 |  0.88898 |   94.1      \n",
      "| 19    | 0.88471 |  0.90109 |   99.3      \n",
      "| 20    | 0.88980 |  0.88482 |   104.5     \n",
      "| 21    | 0.89312 |  0.89058 |   109.7     \n",
      "| 22    | 0.89766 |  0.89792 |   114.9     \n",
      "| 23    | 0.89415 |  0.87748 |   120.1     \n",
      "| 24    | 0.89947 |  0.86356 |   125.3     \n",
      "Early stopping occured at epoch 24\n",
      "Training done in 125.301 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.58724 |  0.71553 |   5.4       \n",
      "| 2     | 0.79545 |  0.77419 |   10.7      \n",
      "| 3     | 0.82274 |  0.77025 |   15.9      \n",
      "| 4     | 0.84625 |  0.81527 |   21.4      \n",
      "| 5     | 0.87213 |  0.81997 |   26.7      \n",
      "| 6     | 0.87586 |  0.85419 |   31.9      \n",
      "| 7     | 0.87522 |  0.84024 |   37.0      \n",
      "| 8     | 0.87123 |  0.84493 |   42.0      \n",
      "| 9     | 0.87568 |  0.86939 |   47.3      \n",
      "| 10    | 0.88071 |  0.86929 |   52.4      \n",
      "| 11    | 0.88809 |  0.87604 |   57.6      \n",
      "| 12    | 0.89166 |  0.87947 |   62.8      \n",
      "| 13    | 0.88967 |  0.84978 |   68.1      \n",
      "| 14    | 0.89454 |  0.86314 |   73.1      \n",
      "| 15    | 0.90027 |  0.89266 |   78.5      \n",
      "| 16    | 0.90324 |  0.89202 |   83.6      \n",
      "| 17    | 0.90610 |  0.89953 |   88.7      \n",
      "| 18    | 0.90740 |  0.90341 |   93.8      \n",
      "| 19    | 0.90744 |  0.90156 |   98.9      \n",
      "| 20    | 0.90259 |  0.90265 |   104.1     \n",
      "| 21    | 0.90526 |  0.90324 |   109.2     \n",
      "| 22    | 0.90703 |  0.90209 |   114.5     \n",
      "| 23    | 0.91210 |  0.90616 |   119.8     \n",
      "| 24    | 0.91035 |  0.90571 |   125.0     \n",
      "| 25    | 0.91279 |  0.90529 |   130.2     \n",
      "| 26    | 0.91232 |  0.90630 |   135.5     \n",
      "| 27    | 0.91373 |  0.90871 |   141.2     \n",
      "| 28    | 0.91541 |  0.91099 |   146.4     \n",
      "| 29    | 0.91582 |  0.91050 |   151.5     \n",
      "| 30    | 0.91873 |  0.91031 |   156.8     \n",
      "| 31    | 0.91816 |  0.91176 |   162.5     \n",
      "| 32    | 0.91811 |  0.91285 |   168.1     \n",
      "| 33    | 0.91805 |  0.91213 |   173.6     \n",
      "| 34    | 0.91836 |  0.91260 |   179.0     \n",
      "| 35    | 0.91732 |  0.91207 |   184.4     \n",
      "| 36    | 0.91564 |  0.90715 |   189.7     \n",
      "| 37    | 0.91711 |  0.91042 |   195.2     \n",
      "Early stopping occured at epoch 37\n",
      "Training done in 195.156 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.66723 |  0.77256 |   3.6       \n",
      "| 2     | 0.82411 |  0.78006 |   7.2       \n",
      "| 3     | 0.83330 |  0.81935 |   10.8      \n",
      "| 4     | 0.84311 |  0.85549 |   14.1      \n",
      "| 5     | 0.85962 |  0.85709 |   17.5      \n",
      "| 6     | 0.86686 |  0.87625 |   20.9      \n",
      "| 7     | 0.87673 |  0.88285 |   24.4      \n",
      "| 8     | 0.88594 |  0.87392 |   27.9      \n",
      "| 9     | 0.88549 |  0.77833 |   31.3      \n",
      "| 10    | 0.89075 |  0.80646 |   34.7      \n",
      "| 11    | 0.89262 |  0.82216 |   38.2      \n",
      "| 12    | 0.89681 |  0.81558 |   41.5      \n",
      "Early stopping occured at epoch 12\n",
      "Training done in 41.539 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.66483 |  0.68863 |   3.5       \n",
      "| 2     | 0.82243 |  0.79819 |   7.0       \n",
      "| 3     | 0.85308 |  0.81365 |   10.4      \n",
      "| 4     | 0.86439 |  0.85958 |   14.0      \n",
      "| 5     | 0.87223 |  0.85433 |   17.4      \n",
      "| 6     | 0.86773 |  0.86981 |   20.9      \n",
      "| 7     | 0.87571 |  0.87425 |   24.5      \n",
      "| 8     | 0.88126 |  0.88531 |   28.1      \n",
      "| 9     | 0.88819 |  0.89210 |   31.5      \n",
      "| 10    | 0.88859 |  0.89115 |   35.2      \n",
      "| 11    | 0.88978 |  0.88871 |   38.7      \n",
      "| 12    | 0.89431 |  0.88820 |   42.1      \n",
      "| 13    | 0.88781 |  0.88320 |   45.7      \n",
      "| 14    | 0.89061 |  0.88917 |   49.2      \n",
      "Early stopping occured at epoch 14\n",
      "Training done in 49.177 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.50916 |  0.49317 |   3.2       \n",
      "| 2     | 0.54879 |  0.56014 |   5.7       \n",
      "| 3     | 0.62169 |  0.53887 |   8.0       \n",
      "| 4     | 0.56751 |  0.61973 |   10.5      \n",
      "| 5     | 0.66796 |  0.46865 |   12.8      \n",
      "| 6     | 0.68835 |  0.50857 |   15.1      \n",
      "| 7     | 0.69706 |  0.62753 |   17.6      \n",
      "| 8     | 0.72627 |  0.72327 |   19.9      \n",
      "| 9     | 0.74148 |  0.71783 |   22.4      \n",
      "| 10    | 0.73788 |  0.70691 |   24.8      \n",
      "| 11    | 0.76606 |  0.73948 |   27.2      \n",
      "| 12    | 0.76634 |  0.75052 |   29.6      \n",
      "| 13    | 0.78635 |  0.76808 |   31.9      \n",
      "| 14    | 0.79856 |  0.74951 |   34.3      \n",
      "| 15    | 0.80677 |  0.75018 |   36.6      \n",
      "| 16    | 0.80859 |  0.78110 |   39.1      \n",
      "| 17    | 0.81660 |  0.76853 |   41.5      \n",
      "| 18    | 0.81799 |  0.76500 |   43.8      \n",
      "| 19    | 0.81065 |  0.76108 |   46.3      \n",
      "| 20    | 0.81139 |  0.75556 |   48.6      \n",
      "| 21    | 0.80064 |  0.74116 |   51.0      \n",
      "Early stopping occured at epoch 21\n",
      "Training done in 51.005 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.49887 |  0.50908 |   2.4       \n",
      "| 2     | 0.53874 |  0.63234 |   4.9       \n",
      "| 3     | 0.58984 |  0.61615 |   7.4       \n",
      "| 4     | 0.62656 |  0.53983 |   10.0      \n",
      "| 5     | 0.65701 |  0.53349 |   12.6      \n",
      "| 6     | 0.65417 |  0.56847 |   15.1      \n",
      "| 7     | 0.72417 |  0.59971 |   17.6      \n",
      "Early stopping occured at epoch 7\n",
      "Training done in 17.553 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52578 |  0.53539 |   3.2       \n",
      "| 2     | 0.64233 |  0.63123 |   6.6       \n",
      "| 3     | 0.72671 |  0.65303 |   10.0      \n",
      "| 4     | 0.76975 |  0.59403 |   13.4      \n",
      "| 5     | 0.78685 |  0.74550 |   16.8      \n",
      "| 6     | 0.79618 |  0.75790 |   20.3      \n",
      "| 7     | 0.79915 |  0.76713 |   23.7      \n",
      "| 8     | 0.80984 |  0.74549 |   27.0      \n",
      "| 9     | 0.81922 |  0.78454 |   30.4      \n",
      "| 10    | 0.82294 |  0.78697 |   33.8      \n",
      "| 11    | 0.82512 |  0.78071 |   37.0      \n",
      "| 12    | 0.82792 |  0.77511 |   40.4      \n",
      "| 13    | 0.83460 |  0.79743 |   43.7      \n",
      "| 14    | 0.83598 |  0.80988 |   47.1      \n",
      "| 15    | 0.84618 |  0.83427 |   50.5      \n",
      "| 16    | 0.85035 |  0.83574 |   53.9      \n",
      "| 17    | 0.83999 |  0.83395 |   57.2      \n",
      "| 18    | 0.83917 |  0.84580 |   60.9      \n",
      "| 19    | 0.84940 |  0.83810 |   64.3      \n",
      "| 20    | 0.85210 |  0.84505 |   68.1      \n",
      "| 21    | 0.86024 |  0.85574 |   71.6      \n",
      "| 22    | 0.86724 |  0.85995 |   75.1      \n",
      "| 23    | 0.87270 |  0.86157 |   78.8      \n",
      "| 24    | 0.87603 |  0.86356 |   82.2      \n",
      "| 25    | 0.87881 |  0.86801 |   85.4      \n",
      "| 26    | 0.88119 |  0.87034 |   88.8      \n",
      "| 27    | 0.88249 |  0.86426 |   92.0      \n",
      "| 28    | 0.87166 |  0.86052 |   95.3      \n",
      "| 29    | 0.87601 |  0.86617 |   98.6      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 30    | 0.88087 |  0.87422 |   101.9     \n",
      "| 31    | 0.88492 |  0.86448 |   105.3     \n",
      "| 32    | 0.88469 |  0.88024 |   108.8     \n",
      "| 33    | 0.88828 |  0.88017 |   112.2     \n",
      "| 34    | 0.88988 |  0.87689 |   115.6     \n",
      "| 35    | 0.89042 |  0.87668 |   118.9     \n",
      "| 36    | 0.89367 |  0.87716 |   122.2     \n",
      "| 37    | 0.89447 |  0.86953 |   125.5     \n",
      "Early stopping occured at epoch 37\n",
      "Training done in 125.459 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.53374 |  0.58107 |   3.4       \n",
      "| 2     | 0.65300 |  0.71841 |   7.0       \n",
      "| 3     | 0.75700 |  0.69190 |   10.5      \n",
      "| 4     | 0.78429 |  0.74101 |   14.4      \n",
      "| 5     | 0.80479 |  0.74962 |   18.0      \n",
      "| 6     | 0.81545 |  0.74779 |   21.5      \n",
      "| 7     | 0.82584 |  0.73630 |   25.0      \n",
      "| 8     | 0.84685 |  0.75877 |   28.5      \n",
      "| 9     | 0.85586 |  0.75290 |   32.1      \n",
      "| 10    | 0.86364 |  0.76338 |   35.8      \n",
      "| 11    | 0.86849 |  0.76379 |   39.6      \n",
      "| 12    | 0.87375 |  0.76058 |   43.3      \n",
      "| 13    | 0.88110 |  0.76621 |   47.2      \n",
      "| 14    | 0.88222 |  0.80244 |   51.1      \n",
      "| 15    | 0.88465 |  0.77590 |   54.7      \n",
      "| 16    | 0.88580 |  0.78237 |   58.1      \n",
      "| 17    | 0.88242 |  0.78138 |   61.7      \n",
      "| 18    | 0.88002 |  0.73668 |   65.2      \n",
      "| 19    | 0.87128 |  0.73945 |   68.8      \n",
      "Early stopping occured at epoch 19\n",
      "Training done in 68.753 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.56626 |  0.58898 |   4.1       \n",
      "| 2     | 0.62114 |  0.69757 |   8.2       \n",
      "| 3     | 0.74111 |  0.69838 |   12.1      \n",
      "| 4     | 0.79870 |  0.76716 |   16.5      \n",
      "| 5     | 0.83057 |  0.80734 |   20.6      \n",
      "| 6     | 0.83776 |  0.78771 |   24.4      \n",
      "| 7     | 0.84431 |  0.83544 |   28.3      \n",
      "| 8     | 0.86411 |  0.84219 |   32.2      \n",
      "| 9     | 0.86926 |  0.82100 |   36.1      \n",
      "| 10    | 0.87416 |  0.85288 |   40.0      \n",
      "| 11    | 0.87964 |  0.85409 |   44.3      \n",
      "| 12    | 0.88451 |  0.86650 |   49.0      \n",
      "| 13    | 0.88669 |  0.85562 |   53.2      \n",
      "| 14    | 0.88996 |  0.87347 |   57.4      \n",
      "| 15    | 0.89061 |  0.86491 |   61.4      \n",
      "| 16    | 0.89003 |  0.88302 |   65.4      \n",
      "| 17    | 0.89389 |  0.87041 |   69.6      \n",
      "| 18    | 0.89493 |  0.88125 |   73.6      \n",
      "| 19    | 0.89700 |  0.87965 |   77.4      \n",
      "| 20    | 0.89160 |  0.86562 |   81.2      \n",
      "| 21    | 0.88935 |  0.84774 |   85.1      \n",
      "Early stopping occured at epoch 21\n",
      "Training done in 85.065 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57377 |  0.52657 |   3.8       \n",
      "| 2     | 0.65539 |  0.73955 |   7.9       \n",
      "| 3     | 0.77929 |  0.67259 |   11.8      \n",
      "| 4     | 0.80724 |  0.76719 |   15.7      \n",
      "| 5     | 0.84400 |  0.80089 |   19.6      \n",
      "| 6     | 0.84060 |  0.82230 |   23.6      \n",
      "| 7     | 0.84568 |  0.83399 |   27.6      \n",
      "| 8     | 0.85948 |  0.83225 |   31.7      \n",
      "| 9     | 0.86905 |  0.84629 |   35.8      \n",
      "| 10    | 0.86728 |  0.85243 |   40.5      \n",
      "| 11    | 0.87090 |  0.84136 |   44.4      \n",
      "| 12    | 0.87750 |  0.85725 |   49.3      \n",
      "| 13    | 0.88657 |  0.86024 |   53.2      \n",
      "| 14    | 0.87592 |  0.86561 |   57.2      \n",
      "| 15    | 0.88293 |  0.87462 |   61.4      \n",
      "| 16    | 0.88611 |  0.87564 |   65.3      \n",
      "| 17    | 0.89154 |  0.88351 |   69.5      \n",
      "| 18    | 0.89584 |  0.88308 |   73.8      \n",
      "| 19    | 0.89848 |  0.88899 |   78.0      \n",
      "| 20    | 0.90020 |  0.89205 |   82.4      \n",
      "| 21    | 0.90230 |  0.89199 |   86.4      \n",
      "| 22    | 0.90068 |  0.89174 |   90.4      \n",
      "| 23    | 0.89763 |  0.88930 |   94.2      \n",
      "| 24    | 0.89988 |  0.89564 |   98.2      \n",
      "| 25    | 0.90507 |  0.88639 |   101.9     \n",
      "| 26    | 0.90588 |  0.88465 |   105.8     \n",
      "| 27    | 0.90745 |  0.89418 |   109.7     \n",
      "| 28    | 0.90614 |  0.83806 |   113.7     \n",
      "| 29    | 0.90413 |  0.86433 |   117.8     \n",
      "Early stopping occured at epoch 29\n",
      "Training done in 117.795 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57844 |  0.56314 |   3.9       \n",
      "| 2     | 0.71740 |  0.70249 |   7.8       \n",
      "| 3     | 0.79233 |  0.76060 |   12.0      \n",
      "| 4     | 0.81700 |  0.80879 |   15.9      \n",
      "| 5     | 0.84668 |  0.81929 |   19.8      \n",
      "| 6     | 0.85054 |  0.83863 |   23.8      \n",
      "| 7     | 0.85312 |  0.81713 |   27.9      \n",
      "| 8     | 0.85349 |  0.82069 |   32.0      \n",
      "| 9     | 0.86197 |  0.85169 |   36.4      \n",
      "| 10    | 0.87328 |  0.84272 |   40.8      \n",
      "| 11    | 0.87979 |  0.85108 |   45.0      \n",
      "| 12    | 0.88381 |  0.86664 |   49.5      \n",
      "| 13    | 0.88511 |  0.87823 |   53.7      \n",
      "| 14    | 0.88884 |  0.87622 |   57.9      \n",
      "| 15    | 0.89075 |  0.88064 |   62.1      \n",
      "| 16    | 0.89283 |  0.88570 |   66.1      \n",
      "| 17    | 0.88398 |  0.87070 |   70.0      \n",
      "| 18    | 0.87406 |  0.87147 |   73.9      \n",
      "| 19    | 0.88462 |  0.87865 |   78.0      \n",
      "| 20    | 0.88889 |  0.87860 |   82.1      \n",
      "| 21    | 0.89197 |  0.87958 |   86.0      \n",
      "Early stopping occured at epoch 21\n",
      "Training done in 85.984 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57512 |  0.63476 |   9.3       \n",
      "| 2     | 0.72300 |  0.75246 |   13.3      \n",
      "| 3     | 0.79147 |  0.78395 |   17.4      \n",
      "| 4     | 0.82405 |  0.72790 |   21.6      \n",
      "| 5     | 0.85096 |  0.83176 |   25.7      \n",
      "| 6     | 0.85132 |  0.84809 |   29.7      \n",
      "| 7     | 0.86936 |  0.84074 |   33.5      \n",
      "| 8     | 0.87275 |  0.81158 |   38.1      \n",
      "| 9     | 0.87859 |  0.86305 |   46.0      \n",
      "| 10    | 0.88458 |  0.87285 |   49.9      \n",
      "| 11    | 0.88251 |  0.87203 |   53.9      \n",
      "| 12    | 0.88491 |  0.87269 |   58.1      \n",
      "| 13    | 0.89076 |  0.87697 |   62.1      \n",
      "| 14    | 0.90003 |  0.85760 |   66.0      \n",
      "| 15    | 0.90287 |  0.87434 |   70.2      \n",
      "| 16    | 0.89938 |  0.89610 |   74.1      \n",
      "| 17    | 0.90265 |  0.89471 |   80.0      \n",
      "| 18    | 0.90529 |  0.89844 |   84.3      \n",
      "| 19    | 0.90509 |  0.89763 |   88.4      \n",
      "| 20    | 0.90651 |  0.90204 |   92.4      \n",
      "| 21    | 0.91021 |  0.90640 |   96.4      \n",
      "| 22    | 0.90938 |  0.90025 |   100.5     \n",
      "| 23    | 0.90975 |  0.90190 |   105.0     \n",
      "| 24    | 0.91104 |  0.89470 |   109.2     \n",
      "| 25    | 0.91014 |  0.89966 |   113.2     \n",
      "| 26    | 0.90604 |  0.84814 |   117.4     \n",
      "Early stopping occured at epoch 26\n",
      "Training done in 117.361 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52539 |  0.56958 |   6.0       \n",
      "| 2     | 0.56862 |  0.63071 |   11.3      \n",
      "| 3     | 0.60842 |  0.62640 |   16.8      \n",
      "| 4     | 0.74150 |  0.80741 |   22.1      \n",
      "| 5     | 0.79825 |  0.64508 |   27.2      \n",
      "| 6     | 0.81131 |  0.80655 |   32.2      \n",
      "| 7     | 0.83330 |  0.83243 |   37.5      \n",
      "| 8     | 0.84829 |  0.85160 |   42.8      \n",
      "| 9     | 0.85904 |  0.77114 |   48.1      \n",
      "| 10    | 0.86657 |  0.84643 |   53.9      \n",
      "| 11    | 0.87056 |  0.85387 |   59.3      \n",
      "| 12    | 0.87922 |  0.86188 |   64.6      \n",
      "| 13    | 0.87883 |  0.86787 |   69.8      \n",
      "| 14    | 0.87689 |  0.84933 |   76.6      \n",
      "| 15    | 0.87197 |  0.86582 |   81.8      \n",
      "| 16    | 0.86177 |  0.86546 |   86.9      \n",
      "| 17    | 0.86923 |  0.85412 |   92.0      \n",
      "| 18    | 0.87602 |  0.85153 |   97.1      \n",
      "Early stopping occured at epoch 18\n",
      "Training done in 97.102 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1     | 0.52810 |  0.51790 |   5.2       \n",
      "| 2     | 0.53116 |  0.70392 |   10.4      \n",
      "| 3     | 0.67656 |  0.68164 |   15.7      \n",
      "| 4     | 0.77084 |  0.75730 |   21.2      \n",
      "| 5     | 0.79634 |  0.75861 |   26.5      \n",
      "| 6     | 0.80317 |  0.74815 |   31.9      \n",
      "| 7     | 0.81791 |  0.80175 |   37.4      \n",
      "| 8     | 0.82258 |  0.81927 |   42.9      \n",
      "| 9     | 0.81782 |  0.81971 |   49.0      \n",
      "| 10    | 0.82652 |  0.81207 |   54.6      \n",
      "| 11    | 0.83001 |  0.82716 |   59.9      \n",
      "| 12    | 0.84363 |  0.83528 |   65.6      \n",
      "| 13    | 0.83717 |  0.81220 |   71.1      \n",
      "| 14    | 0.84965 |  0.83303 |   76.4      \n",
      "| 15    | 0.86189 |  0.84313 |   81.6      \n",
      "| 16    | 0.86260 |  0.84956 |   87.0      \n",
      "| 17    | 0.86470 |  0.84895 |   92.6      \n",
      "| 18    | 0.86235 |  0.83786 |   98.1      \n",
      "| 19    | 0.84870 |  0.82738 |   103.2     \n",
      "| 20    | 0.83070 |  0.84170 |   108.3     \n",
      "| 21    | 0.84443 |  0.83934 |   113.6     \n",
      "Early stopping occured at epoch 21\n",
      "Training done in 113.610 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52602 |  0.53111 |   5.6       \n",
      "| 2     | 0.57760 |  0.57410 |   11.0      \n",
      "| 3     | 0.68002 |  0.66445 |   16.5      \n",
      "| 4     | 0.70467 |  0.69112 |   22.0      \n",
      "| 5     | 0.72872 |  0.74599 |   27.4      \n",
      "| 6     | 0.76900 |  0.74260 |   32.7      \n",
      "| 7     | 0.82947 |  0.83275 |   37.9      \n",
      "| 8     | 0.85328 |  0.84462 |   43.5      \n",
      "| 9     | 0.85183 |  0.79837 |   49.1      \n",
      "| 10    | 0.85772 |  0.71211 |   54.5      \n",
      "| 11    | 0.85830 |  0.81880 |   60.1      \n",
      "| 12    | 0.86000 |  0.83831 |   65.3      \n",
      "| 13    | 0.86700 |  0.85599 |   70.4      \n",
      "| 14    | 0.83595 |  0.80687 |   75.7      \n",
      "| 15    | 0.83340 |  0.82155 |   81.0      \n",
      "| 16    | 0.85169 |  0.83945 |   86.1      \n",
      "| 17    | 0.87165 |  0.87466 |   91.5      \n",
      "| 18    | 0.87873 |  0.88360 |   96.9      \n",
      "| 19    | 0.84291 |  0.81635 |   102.2     \n",
      "| 20    | 0.83911 |  0.80994 |   107.4     \n",
      "| 21    | 0.85185 |  0.85418 |   112.8     \n",
      "| 22    | 0.85634 |  0.86170 |   118.1     \n",
      "| 23    | 0.86880 |  0.82713 |   123.3     \n",
      "Early stopping occured at epoch 23\n",
      "Training done in 123.322 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54348 |  0.59008 |   5.4       \n",
      "| 2     | 0.72986 |  0.77923 |   11.1      \n",
      "| 3     | 0.81284 |  0.79825 |   16.6      \n",
      "| 4     | 0.82276 |  0.80118 |   21.8      \n",
      "| 5     | 0.79394 |  0.78459 |   27.1      \n",
      "| 6     | 0.80799 |  0.78533 |   32.5      \n",
      "| 7     | 0.81653 |  0.78061 |   38.0      \n",
      "| 8     | 0.83257 |  0.81574 |   43.0      \n",
      "| 9     | 0.85808 |  0.78700 |   48.3      \n",
      "| 10    | 0.87080 |  0.83398 |   53.6      \n",
      "| 11    | 0.87465 |  0.87290 |   59.2      \n",
      "| 12    | 0.88529 |  0.88228 |   64.8      \n",
      "| 13    | 0.89239 |  0.88071 |   70.1      \n",
      "| 14    | 0.89289 |  0.88741 |   75.5      \n",
      "| 15    | 0.89511 |  0.89219 |   80.5      \n",
      "| 16    | 0.89460 |  0.88695 |   85.8      \n",
      "| 17    | 0.89942 |  0.89225 |   91.4      \n",
      "| 18    | 0.90205 |  0.89638 |   96.8      \n",
      "| 19    | 0.90339 |  0.90178 |   102.4     \n",
      "| 20    | 0.90576 |  0.90108 |   107.8     \n",
      "| 21    | 0.90278 |  0.89905 |   113.4     \n",
      "| 22    | 0.90405 |  0.90300 |   118.7     \n",
      "| 23    | 0.89218 |  0.63069 |   124.1     \n",
      "| 24    | 0.86571 |  0.65573 |   129.5     \n",
      "| 25    | 0.88438 |  0.79566 |   135.0     \n",
      "| 26    | 0.89404 |  0.81970 |   140.5     \n",
      "| 27    | 0.89861 |  0.79490 |   146.0     \n",
      "Early stopping occured at epoch 27\n",
      "Training done in 146.024 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52520 |  0.57451 |   2.1       \n",
      "| 2     | 0.76855 |  0.74972 |   4.1       \n",
      "| 3     | 0.81922 |  0.76009 |   6.2       \n",
      "| 4     | 0.83420 |  0.80133 |   8.2       \n",
      "| 5     | 0.84949 |  0.82439 |   10.2      \n",
      "| 6     | 0.86136 |  0.84047 |   12.0      \n",
      "| 7     | 0.86936 |  0.85451 |   14.0      \n",
      "| 8     | 0.87667 |  0.86414 |   15.8      \n",
      "| 9     | 0.88504 |  0.86235 |   17.8      \n",
      "| 10    | 0.89162 |  0.86998 |   19.7      \n",
      "| 11    | 0.89660 |  0.87779 |   21.6      \n",
      "| 12    | 0.90491 |  0.87682 |   23.4      \n",
      "| 13    | 0.90828 |  0.88422 |   25.3      \n",
      "| 14    | 0.91095 |  0.88739 |   27.2      \n",
      "| 15    | 0.91429 |  0.88767 |   29.0      \n",
      "| 16    | 0.91638 |  0.88859 |   31.0      \n",
      "| 17    | 0.91950 |  0.88569 |   33.0      \n",
      "| 18    | 0.92112 |  0.88877 |   35.0      \n",
      "| 19    | 0.92300 |  0.89215 |   36.8      \n",
      "| 20    | 0.92216 |  0.89294 |   38.6      \n",
      "| 21    | 0.92368 |  0.88420 |   40.5      \n",
      "| 22    | 0.92569 |  0.89612 |   42.5      \n",
      "| 23    | 0.92657 |  0.89776 |   44.5      \n",
      "| 24    | 0.92798 |  0.89822 |   46.4      \n",
      "| 25    | 0.92978 |  0.89993 |   48.3      \n",
      "| 26    | 0.93066 |  0.90206 |   50.3      \n",
      "| 27    | 0.93175 |  0.90088 |   52.2      \n",
      "| 28    | 0.93205 |  0.90041 |   54.2      \n",
      "| 29    | 0.93433 |  0.89983 |   56.1      \n",
      "| 30    | 0.93404 |  0.89742 |   58.1      \n",
      "| 31    | 0.93598 |  0.89681 |   59.9      \n",
      "Early stopping occured at epoch 31\n",
      "Training done in 59.929 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.50892 |  0.56363 |   2.1       \n",
      "| 2     | 0.71063 |  0.68657 |   4.0       \n",
      "| 3     | 0.78419 |  0.72241 |   5.9       \n",
      "| 4     | 0.81042 |  0.75579 |   7.9       \n",
      "| 5     | 0.82739 |  0.79427 |   9.8       \n",
      "| 6     | 0.84364 |  0.81442 |   11.7      \n",
      "| 7     | 0.86229 |  0.82624 |   13.6      \n",
      "| 8     | 0.87398 |  0.80155 |   15.5      \n",
      "| 9     | 0.88026 |  0.80590 |   17.6      \n",
      "| 10    | 0.88661 |  0.80676 |   19.6      \n",
      "| 11    | 0.89291 |  0.79840 |   21.5      \n",
      "| 12    | 0.89822 |  0.78498 |   23.4      \n",
      "Early stopping occured at epoch 12\n",
      "Training done in 23.415 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.59338 |  0.56349 |   2.5       \n",
      "| 2     | 0.80295 |  0.75489 |   4.9       \n",
      "| 3     | 0.83326 |  0.82639 |   7.2       \n",
      "| 4     | 0.85450 |  0.82649 |   9.4       \n",
      "| 5     | 0.86620 |  0.84284 |   11.8      \n",
      "| 6     | 0.87176 |  0.83487 |   14.2      \n",
      "| 7     | 0.87717 |  0.84490 |   16.6      \n",
      "| 8     | 0.88042 |  0.85262 |   19.0      \n",
      "| 9     | 0.88895 |  0.86036 |   21.4      \n",
      "| 10    | 0.89221 |  0.87341 |   23.7      \n",
      "| 11    | 0.89357 |  0.87241 |   26.1      \n",
      "| 12    | 0.89962 |  0.88801 |   28.4      \n",
      "| 13    | 0.90504 |  0.89250 |   30.6      \n",
      "| 14    | 0.90983 |  0.89590 |   32.8      \n",
      "| 15    | 0.91168 |  0.89560 |   34.9      \n",
      "| 16    | 0.91164 |  0.90027 |   37.2      \n",
      "| 17    | 0.91172 |  0.90178 |   39.6      \n",
      "| 18    | 0.91293 |  0.90323 |   41.9      \n",
      "| 19    | 0.91666 |  0.90419 |   44.2      \n",
      "| 20    | 0.91664 |  0.90756 |   46.5      \n",
      "| 21    | 0.92023 |  0.90911 |   48.8      \n",
      "| 22    | 0.92251 |  0.91063 |   51.1      \n",
      "| 23    | 0.92340 |  0.91141 |   53.5      \n",
      "| 24    | 0.92448 |  0.91280 |   55.8      \n",
      "| 25    | 0.92380 |  0.91329 |   58.1      \n",
      "| 26    | 0.92524 |  0.91313 |   60.4      \n",
      "| 27    | 0.92739 |  0.91356 |   62.9      \n",
      "| 28    | 0.92751 |  0.91282 |   65.4      \n",
      "| 29    | 0.92717 |  0.91462 |   67.9      \n",
      "| 30    | 0.93000 |  0.91504 |   70.3      \n",
      "| 31    | 0.93101 |  0.91503 |   72.6      \n",
      "| 32    | 0.93100 |  0.91541 |   74.9      \n",
      "| 33    | 0.93114 |  0.91510 |   77.2      \n",
      "| 34    | 0.93233 |  0.91555 |   79.5      \n",
      "| 35    | 0.93175 |  0.91713 |   81.9      \n",
      "| 36    | 0.93328 |  0.91853 |   84.2      \n",
      "| 37    | 0.93329 |  0.91714 |   86.6      \n",
      "| 38    | 0.93375 |  0.91579 |   88.8      \n",
      "| 39    | 0.93474 |  0.91764 |   91.1      \n",
      "| 40    | 0.93398 |  0.91198 |   93.4      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 41    | 0.93422 |  0.91564 |   95.6      \n",
      "Early stopping occured at epoch 41\n",
      "Training done in 95.636 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.63349 |  0.69852 |   2.3       \n",
      "| 2     | 0.79917 |  0.78390 |   4.6       \n",
      "| 3     | 0.82512 |  0.80459 |   6.9       \n",
      "| 4     | 0.83836 |  0.80347 |   9.1       \n",
      "| 5     | 0.84780 |  0.80998 |   11.3      \n",
      "| 6     | 0.86036 |  0.83763 |   13.7      \n",
      "| 7     | 0.86587 |  0.84392 |   16.2      \n",
      "| 8     | 0.86932 |  0.78522 |   18.7      \n",
      "| 9     | 0.86828 |  0.86062 |   21.2      \n",
      "| 10    | 0.87442 |  0.86456 |   23.6      \n",
      "| 11    | 0.88599 |  0.87723 |   26.1      \n",
      "| 12    | 0.89101 |  0.87298 |   28.5      \n",
      "| 13    | 0.89279 |  0.86797 |   31.0      \n",
      "| 14    | 0.89513 |  0.85833 |   33.5      \n",
      "| 15    | 0.89600 |  0.87157 |   35.9      \n",
      "| 16    | 0.89764 |  0.89080 |   38.5      \n",
      "| 17    | 0.90015 |  0.89053 |   40.8      \n",
      "| 18    | 0.90188 |  0.89266 |   43.0      \n",
      "| 19    | 0.90058 |  0.88765 |   45.4      \n",
      "| 20    | 0.90228 |  0.88639 |   47.8      \n",
      "| 21    | 0.90435 |  0.88523 |   50.0      \n",
      "| 22    | 0.90626 |  0.88603 |   52.4      \n",
      "| 23    | 0.90728 |  0.88454 |   54.7      \n",
      "Early stopping occured at epoch 23\n",
      "Training done in 54.703 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.55183 |  0.72645 |   2.3       \n",
      "| 2     | 0.72203 |  0.47770 |   4.7       \n",
      "| 3     | 0.76723 |  0.50791 |   6.9       \n",
      "| 4     | 0.79659 |  0.56582 |   9.3       \n",
      "| 5     | 0.81488 |  0.78980 |   11.6      \n",
      "| 6     | 0.81835 |  0.78809 |   13.9      \n",
      "| 7     | 0.82346 |  0.78310 |   16.2      \n",
      "| 8     | 0.82474 |  0.78775 |   18.5      \n",
      "| 9     | 0.82885 |  0.78803 |   20.7      \n",
      "| 10    | 0.83376 |  0.79052 |   23.1      \n",
      "| 11    | 0.83597 |  0.79331 |   25.5      \n",
      "| 12    | 0.84446 |  0.80896 |   27.7      \n",
      "| 13    | 0.84846 |  0.80197 |   29.9      \n",
      "| 14    | 0.85015 |  0.79956 |   32.3      \n",
      "| 15    | 0.85014 |  0.80058 |   34.5      \n",
      "| 16    | 0.85282 |  0.80704 |   36.8      \n",
      "| 17    | 0.85647 |  0.80991 |   39.1      \n",
      "| 18    | 0.86297 |  0.80829 |   41.4      \n",
      "| 19    | 0.87001 |  0.81407 |   43.9      \n",
      "| 20    | 0.87293 |  0.81683 |   46.2      \n",
      "| 21    | 0.87194 |  0.82591 |   48.6      \n",
      "| 22    | 0.86635 |  0.82223 |   51.0      \n",
      "| 23    | 0.87388 |  0.83510 |   53.3      \n",
      "| 24    | 0.87995 |  0.84056 |   55.7      \n",
      "| 25    | 0.88524 |  0.84304 |   58.0      \n",
      "| 26    | 0.88606 |  0.83619 |   60.2      \n",
      "| 27    | 0.88954 |  0.85206 |   62.6      \n",
      "| 28    | 0.89104 |  0.85024 |   65.0      \n",
      "| 29    | 0.89236 |  0.84656 |   67.3      \n",
      "| 30    | 0.89476 |  0.84876 |   69.6      \n",
      "| 31    | 0.89628 |  0.85262 |   71.8      \n",
      "| 32    | 0.89749 |  0.86124 |   74.2      \n",
      "| 33    | 0.89987 |  0.85983 |   76.4      \n",
      "| 34    | 0.90005 |  0.86319 |   78.7      \n",
      "| 35    | 0.90236 |  0.86073 |   81.1      \n",
      "| 36    | 0.90392 |  0.87011 |   83.5      \n",
      "| 37    | 0.90424 |  0.86837 |   85.7      \n",
      "| 38    | 0.90554 |  0.86999 |   87.9      \n",
      "| 39    | 0.90616 |  0.87562 |   90.2      \n",
      "| 40    | 0.90720 |  0.87036 |   92.5      \n",
      "| 41    | 0.90907 |  0.87899 |   94.8      \n",
      "| 42    | 0.90923 |  0.87567 |   97.1      \n",
      "| 43    | 0.91039 |  0.88030 |   99.5      \n",
      "| 44    | 0.91237 |  0.88435 |   101.8     \n",
      "| 45    | 0.91310 |  0.88709 |   104.1     \n",
      "| 46    | 0.91236 |  0.87880 |   106.4     \n",
      "| 47    | 0.91256 |  0.88232 |   108.9     \n",
      "| 48    | 0.91462 |  0.88828 |   111.2     \n",
      "| 49    | 0.91526 |  0.88756 |   113.5     \n",
      "| 50    | 0.91564 |  0.88849 |   116.0     \n",
      "| 51    | 0.91661 |  0.89604 |   118.3     \n",
      "| 52    | 0.91740 |  0.89573 |   120.6     \n",
      "| 53    | 0.91777 |  0.89880 |   122.9     \n",
      "| 54    | 0.91811 |  0.89710 |   125.3     \n",
      "| 55    | 0.91922 |  0.89731 |   127.8     \n",
      "| 56    | 0.92029 |  0.90263 |   130.0     \n",
      "| 57    | 0.91994 |  0.90071 |   132.2     \n",
      "| 58    | 0.92082 |  0.90172 |   134.5     \n",
      "| 59    | 0.92150 |  0.90322 |   136.8     \n",
      "| 60    | 0.92179 |  0.90340 |   139.3     \n",
      "| 61    | 0.92198 |  0.90598 |   141.7     \n",
      "| 62    | 0.92291 |  0.90477 |   144.1     \n",
      "| 63    | 0.92242 |  0.90685 |   146.6     \n",
      "| 64    | 0.92296 |  0.90683 |   148.9     \n",
      "| 65    | 0.92353 |  0.90522 |   151.4     \n",
      "| 66    | 0.92447 |  0.90812 |   153.7     \n",
      "| 67    | 0.92427 |  0.90860 |   156.0     \n",
      "| 68    | 0.92472 |  0.90961 |   158.3     \n",
      "| 69    | 0.92429 |  0.90766 |   160.5     \n",
      "| 70    | 0.92548 |  0.91011 |   162.8     \n",
      "| 71    | 0.92631 |  0.91104 |   165.0     \n",
      "| 72    | 0.92680 |  0.90951 |   167.5     \n",
      "| 73    | 0.92695 |  0.91224 |   169.9     \n",
      "| 74    | 0.92730 |  0.91191 |   172.2     \n",
      "| 75    | 0.92730 |  0.91064 |   174.4     \n",
      "| 76    | 0.92863 |  0.91209 |   176.7     \n",
      "| 77    | 0.92821 |  0.91199 |   178.9     \n",
      "| 78    | 0.92902 |  0.91305 |   181.3     \n",
      "| 79    | 0.92963 |  0.91387 |   183.6     \n",
      "| 80    | 0.92962 |  0.91263 |   185.8     \n",
      "| 81    | 0.93068 |  0.91400 |   188.1     \n",
      "| 82    | 0.93079 |  0.91217 |   190.3     \n",
      "| 83    | 0.93112 |  0.91504 |   192.6     \n",
      "| 84    | 0.93193 |  0.91434 |   194.8     \n",
      "| 85    | 0.93243 |  0.91377 |   197.2     \n",
      "| 86    | 0.93106 |  0.91496 |   199.4     \n",
      "| 87    | 0.93274 |  0.91417 |   201.8     \n",
      "| 88    | 0.93309 |  0.91407 |   204.0     \n",
      "Early stopping occured at epoch 88\n",
      "Training done in 203.989 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.55411 |  0.62100 |   2.3       \n",
      "| 2     | 0.68338 |  0.67164 |   4.7       \n",
      "| 3     | 0.73485 |  0.76881 |   7.3       \n",
      "| 4     | 0.77371 |  0.78133 |   10.0      \n",
      "| 5     | 0.79704 |  0.79598 |   12.5      \n",
      "| 6     | 0.81149 |  0.79739 |   15.1      \n",
      "| 7     | 0.81887 |  0.79386 |   17.6      \n",
      "| 8     | 0.82767 |  0.79646 |   20.0      \n",
      "| 9     | 0.82984 |  0.78958 |   22.7      \n",
      "| 10    | 0.83295 |  0.78512 |   25.1      \n",
      "| 11    | 0.83736 |  0.79514 |   27.7      \n",
      "Early stopping occured at epoch 11\n",
      "Training done in 27.719 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51268 |  0.69296 |   3.2       \n",
      "| 2     | 0.67885 |  0.72355 |   6.3       \n",
      "| 3     | 0.74889 |  0.73660 |   9.3       \n",
      "| 4     | 0.77908 |  0.75104 |   12.4      \n",
      "| 5     | 0.79708 |  0.78389 |   15.6      \n",
      "| 6     | 0.79502 |  0.79257 |   18.6      \n",
      "| 7     | 0.81566 |  0.79455 |   21.7      \n",
      "| 8     | 0.82811 |  0.81498 |   24.9      \n",
      "| 9     | 0.84051 |  0.79502 |   28.0      \n",
      "| 10    | 0.84850 |  0.79078 |   30.9      \n",
      "| 11    | 0.85245 |  0.81269 |   33.8      \n",
      "| 12    | 0.86147 |  0.78506 |   36.8      \n",
      "| 13    | 0.86587 |  0.83241 |   39.7      \n",
      "| 14    | 0.87120 |  0.83332 |   42.7      \n",
      "| 15    | 0.87672 |  0.84647 |   45.8      \n",
      "| 16    | 0.87931 |  0.84694 |   48.9      \n",
      "| 17    | 0.87516 |  0.85262 |   52.0      \n",
      "| 18    | 0.87764 |  0.86241 |   55.1      \n",
      "| 19    | 0.88139 |  0.86244 |   58.5      \n",
      "| 20    | 0.88107 |  0.85655 |   61.5      \n",
      "| 21    | 0.87661 |  0.85727 |   64.5      \n",
      "| 22    | 0.87921 |  0.86443 |   67.7      \n",
      "| 23    | 0.88185 |  0.86408 |   70.9      \n",
      "| 24    | 0.88310 |  0.87205 |   73.9      \n",
      "| 25    | 0.88468 |  0.87348 |   77.0      \n",
      "| 26    | 0.87973 |  0.87517 |   80.2      \n",
      "| 27    | 0.88908 |  0.87414 |   83.2      \n",
      "| 28    | 0.88978 |  0.87287 |   86.2      \n",
      "| 29    | 0.89055 |  0.87489 |   89.2      \n",
      "| 30    | 0.89153 |  0.87912 |   92.1      \n",
      "| 31    | 0.89444 |  0.87859 |   95.1      \n",
      "| 32    | 0.89579 |  0.88385 |   98.0      \n",
      "| 33    | 0.89643 |  0.88253 |   100.6     \n",
      "| 34    | 0.89514 |  0.87893 |   103.5     \n",
      "| 35    | 0.89325 |  0.87236 |   106.5     \n",
      "| 36    | 0.89351 |  0.88238 |   109.3     \n",
      "| 37    | 0.89703 |  0.88171 |   112.4     \n",
      "Early stopping occured at epoch 37\n",
      "Training done in 112.412 seconds.\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51795 |  0.58565 |   3.2       \n",
      "| 2     | 0.59667 |  0.61090 |   6.3       \n",
      "| 3     | 0.61630 |  0.57645 |   9.5       \n",
      "| 4     | 0.67428 |  0.70001 |   12.5      \n",
      "| 5     | 0.76208 |  0.78001 |   15.6      \n",
      "| 6     | 0.80327 |  0.77647 |   18.8      \n",
      "| 7     | 0.81142 |  0.77351 |   22.2      \n",
      "| 8     | 0.80790 |  0.82467 |   25.5      \n",
      "| 9     | 0.82962 |  0.83290 |   28.7      \n",
      "| 10    | 0.84168 |  0.81118 |   31.7      \n",
      "| 11    | 0.84332 |  0.82788 |   34.6      \n",
      "| 12    | 0.84524 |  0.82906 |   37.5      \n",
      "| 13    | 0.84910 |  0.83605 |   40.4      \n",
      "| 14    | 0.85765 |  0.84655 |   43.4      \n",
      "| 15    | 0.86214 |  0.84878 |   46.5      \n",
      "| 16    | 0.86340 |  0.85465 |   49.4      \n",
      "| 17    | 0.86637 |  0.85934 |   52.4      \n",
      "| 18    | 0.87147 |  0.86583 |   55.5      \n",
      "| 19    | 0.87722 |  0.86968 |   58.6      \n",
      "| 20    | 0.88160 |  0.86509 |   61.7      \n",
      "| 21    | 0.88268 |  0.86873 |   64.7      \n",
      "| 22    | 0.88526 |  0.87768 |   67.7      \n",
      "| 23    | 0.89004 |  0.87016 |   70.7      \n",
      "| 24    | 0.88959 |  0.88083 |   73.8      \n",
      "| 25    | 0.89216 |  0.87952 |   77.0      \n",
      "| 26    | 0.89370 |  0.88315 |   80.1      \n",
      "| 27    | 0.89451 |  0.87872 |   83.3      \n",
      "| 28    | 0.89544 |  0.80569 |   86.7      \n",
      "| 29    | 0.89698 |  0.81920 |   89.9      \n",
      "| 30    | 0.89380 |  0.88428 |   93.1      \n",
      "| 31    | 0.89407 |  0.89062 |   96.3      \n",
      "| 32    | 0.89869 |  0.89018 |   99.5      \n",
      "| 33    | 0.89954 |  0.89077 |   102.6     \n",
      "| 34    | 0.89893 |  0.89219 |   105.6     \n",
      "| 35    | 0.89937 |  0.89445 |   108.7     \n",
      "| 36    | 0.90119 |  0.89695 |   111.8     \n",
      "| 37    | 0.89928 |  0.89466 |   114.6     \n",
      "| 38    | 0.89970 |  0.89320 |   117.4     \n",
      "| 39    | 0.90119 |  0.89670 |   120.3     \n",
      "| 40    | 0.90243 |  0.89945 |   123.2     \n",
      "| 41    | 0.90361 |  0.89927 |   126.0     \n",
      "| 42    | 0.90481 |  0.90100 |   129.0     \n",
      "| 43    | 0.90600 |  0.89855 |   131.9     \n",
      "| 44    | 0.90522 |  0.90135 |   134.8     \n",
      "| 45    | 0.90587 |  0.90223 |   137.6     \n",
      "| 46    | 0.90713 |  0.89763 |   140.6     \n",
      "| 47    | 0.90643 |  0.90118 |   143.7     \n",
      "| 48    | 0.90638 |  0.90125 |   146.6     \n",
      "| 49    | 0.90602 |  0.89894 |   149.5     \n",
      "| 50    | 0.90475 |  0.89701 |   152.5     \n",
      "Early stopping occured at epoch 50\n",
      "Training done in 152.497 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.71660 |  0.75036 |   3.9       \n",
      "| 2     | 0.84647 |  0.85050 |   7.8       \n",
      "| 3     | 0.87651 |  0.86780 |   11.8      \n",
      "| 4     | 0.89450 |  0.86641 |   15.7      \n",
      "| 5     | 0.90040 |  0.89328 |   19.6      \n",
      "| 6     | 0.90928 |  0.89751 |   23.5      \n",
      "| 7     | 0.91197 |  0.90127 |   27.4      \n",
      "| 8     | 0.91845 |  0.90132 |   31.4      \n",
      "| 9     | 0.91958 |  0.90721 |   35.2      \n",
      "| 10    | 0.92482 |  0.91621 |   39.2      \n",
      "| 11    | 0.92613 |  0.91582 |   43.0      \n",
      "| 12    | 0.92960 |  0.91659 |   47.1      \n",
      "| 13    | 0.93186 |  0.91740 |   50.7      \n",
      "| 14    | 0.93032 |  0.91875 |   54.6      \n",
      "| 15    | 0.93153 |  0.92187 |   58.4      \n",
      "| 16    | 0.93398 |  0.92226 |   62.5      \n",
      "| 17    | 0.93427 |  0.92137 |   66.5      \n",
      "| 18    | 0.93577 |  0.92031 |   70.4      \n",
      "| 19    | 0.93577 |  0.91772 |   74.2      \n",
      "| 20    | 0.93542 |  0.91855 |   78.2      \n",
      "| 21    | 0.93419 |  0.91923 |   82.0      \n",
      "Early stopping occured at epoch 21\n",
      "Training done in 82.023 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.68932 |  0.77770 |   3.9       \n",
      "| 2     | 0.83135 |  0.80691 |   7.7       \n",
      "| 3     | 0.87392 |  0.86791 |   11.7      \n",
      "| 4     | 0.89344 |  0.88543 |   15.8      \n",
      "| 5     | 0.90329 |  0.90155 |   19.7      \n",
      "| 6     | 0.90625 |  0.89969 |   23.7      \n",
      "| 7     | 0.91391 |  0.90841 |   27.6      \n",
      "| 8     | 0.91560 |  0.90988 |   31.6      \n",
      "| 9     | 0.91965 |  0.91123 |   35.7      \n",
      "| 10    | 0.92424 |  0.91482 |   39.6      \n",
      "| 11    | 0.92595 |  0.91697 |   43.4      \n",
      "| 12    | 0.92778 |  0.91915 |   47.4      \n",
      "| 13    | 0.93254 |  0.91875 |   51.3      \n",
      "| 14    | 0.93154 |  0.92074 |   55.1      \n",
      "| 15    | 0.93165 |  0.91992 |   59.0      \n",
      "| 16    | 0.92936 |  0.90685 |   63.1      \n",
      "| 17    | 0.93323 |  0.91774 |   67.1      \n",
      "| 18    | 0.93343 |  0.91861 |   71.1      \n",
      "| 19    | 0.93469 |  0.91900 |   75.2      \n",
      "Early stopping occured at epoch 19\n",
      "Training done in 75.213 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.60688 |  0.65446 |   3.5       \n",
      "| 2     | 0.80910 |  0.79469 |   7.0       \n",
      "| 3     | 0.84949 |  0.84828 |   10.5      \n",
      "| 4     | 0.86027 |  0.84345 |   14.2      \n",
      "| 5     | 0.86179 |  0.74718 |   17.8      \n",
      "| 6     | 0.87041 |  0.76427 |   21.3      \n",
      "| 7     | 0.87981 |  0.83169 |   24.8      \n",
      "| 8     | 0.88683 |  0.79045 |   28.4      \n",
      "Early stopping occured at epoch 8\n",
      "Training done in 28.371 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.67231 |  0.61306 |   3.6       \n",
      "| 2     | 0.80549 |  0.80441 |   7.0       \n",
      "| 3     | 0.84186 |  0.81475 |   10.6      \n",
      "| 4     | 0.84644 |  0.81787 |   14.1      \n",
      "| 5     | 0.85716 |  0.83528 |   17.6      \n",
      "| 6     | 0.84908 |  0.82842 |   21.2      \n",
      "| 7     | 0.86052 |  0.83452 |   24.8      \n",
      "| 8     | 0.86316 |  0.57218 |   28.0      \n",
      "| 9     | 0.86640 |  0.87305 |   31.6      \n",
      "| 10    | 0.87952 |  0.87842 |   35.2      \n",
      "| 11    | 0.88756 |  0.87990 |   38.7      \n",
      "| 12    | 0.88947 |  0.85198 |   42.3      \n",
      "| 13    | 0.89342 |  0.80537 |   45.8      \n",
      "| 14    | 0.86561 |  0.79148 |   49.4      \n",
      "| 15    | 0.88308 |  0.79231 |   52.9      \n",
      "| 16    | 0.89663 |  0.78531 |   56.4      \n",
      "Early stopping occured at epoch 16\n",
      "Training done in 56.425 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.49321 |  0.49426 |   2.2       \n",
      "| 2     | 0.52273 |  0.63172 |   4.4       \n",
      "| 3     | 0.59110 |  0.59450 |   6.6       \n",
      "| 4     | 0.57942 |  0.66346 |   8.8       \n",
      "| 5     | 0.65121 |  0.56663 |   11.1      \n",
      "| 6     | 0.61118 |  0.63908 |   13.2      \n",
      "| 7     | 0.67931 |  0.62478 |   15.3      \n",
      "| 8     | 0.73057 |  0.67006 |   17.5      \n",
      "| 9     | 0.72601 |  0.69892 |   19.8      \n",
      "| 10    | 0.75387 |  0.71605 |   22.0      \n",
      "| 11    | 0.77932 |  0.74693 |   24.3      \n",
      "| 12    | 0.77327 |  0.73288 |   26.5      \n",
      "| 13    | 0.79890 |  0.74462 |   28.7      \n",
      "| 14    | 0.81017 |  0.75342 |   31.1      \n",
      "| 15    | 0.81241 |  0.76366 |   33.5      \n",
      "| 16    | 0.82597 |  0.77834 |   35.7      \n",
      "| 17    | 0.83235 |  0.78874 |   38.0      \n",
      "| 18    | 0.83707 |  0.79666 |   40.4      \n",
      "| 19    | 0.83956 |  0.75305 |   42.7      \n",
      "| 20    | 0.84203 |  0.77225 |   44.9      \n",
      "| 21    | 0.84777 |  0.77461 |   47.1      \n",
      "| 22    | 0.84825 |  0.78918 |   49.5      \n",
      "| 23    | 0.85274 |  0.80364 |   51.7      \n",
      "| 24    | 0.85743 |  0.81984 |   53.9      \n",
      "| 25    | 0.85964 |  0.82101 |   56.2      \n",
      "| 26    | 0.86340 |  0.82325 |   58.4      \n",
      "| 27    | 0.86872 |  0.83668 |   60.7      \n",
      "| 28    | 0.87288 |  0.84084 |   63.1      \n",
      "| 29    | 0.87684 |  0.84192 |   65.4      \n",
      "| 30    | 0.87856 |  0.84108 |   67.6      \n",
      "| 31    | 0.87657 |  0.82972 |   69.8      \n",
      "| 32    | 0.87481 |  0.83823 |   72.1      \n",
      "| 33    | 0.87833 |  0.83148 |   74.3      \n",
      "| 34    | 0.88138 |  0.83904 |   76.3      \n",
      "Early stopping occured at epoch 34\n",
      "Training done in 76.287 seconds.\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.50743 |  0.55081 |   2.1       \n",
      "| 2     | 0.59636 |  0.58841 |   4.4       \n",
      "| 3     | 0.58684 |  0.59869 |   6.6       \n",
      "| 4     | 0.61847 |  0.62195 |   8.7       \n",
      "| 5     | 0.66361 |  0.54672 |   10.8      \n",
      "| 6     | 0.61922 |  0.62641 |   12.9      \n",
      "| 7     | 0.65459 |  0.60174 |   15.1      \n",
      "| 8     | 0.68806 |  0.63192 |   17.4      \n",
      "| 9     | 0.71652 |  0.67703 |   19.6      \n",
      "| 10    | 0.74444 |  0.68808 |   21.7      \n",
      "| 11    | 0.76006 |  0.70062 |   23.9      \n",
      "| 12    | 0.76927 |  0.72071 |   25.9      \n",
      "| 13    | 0.77935 |  0.73487 |   28.1      \n",
      "| 14    | 0.76211 |  0.74338 |   30.1      \n",
      "| 15    | 0.77364 |  0.73996 |   32.3      \n",
      "| 16    | 0.78406 |  0.75684 |   34.5      \n",
      "| 17    | 0.79304 |  0.75301 |   36.6      \n",
      "| 18    | 0.80667 |  0.76650 |   38.8      \n",
      "| 19    | 0.80834 |  0.75676 |   40.9      \n",
      "| 20    | 0.81601 |  0.75026 |   43.0      \n",
      "| 21    | 0.82745 |  0.80010 |   45.1      \n",
      "| 22    | 0.84184 |  0.78180 |   47.2      \n",
      "| 23    | 0.83370 |  0.78898 |   49.3      \n",
      "| 24    | 0.81880 |  0.81066 |   51.5      \n",
      "| 25    | 0.84167 |  0.80860 |   53.5      \n",
      "| 26    | 0.85340 |  0.81799 |   55.6      \n",
      "| 27    | 0.85648 |  0.83012 |   57.8      \n",
      "| 28    | 0.85652 |  0.82291 |   59.9      \n",
      "| 29    | 0.85416 |  0.83347 |   62.1      \n",
      "| 30    | 0.85730 |  0.80944 |   64.3      \n",
      "| 31    | 0.85892 |  0.81875 |   66.4      \n",
      "| 32    | 0.86208 |  0.81558 |   68.6      \n",
      "| 33    | 0.86742 |  0.85095 |   70.7      \n",
      "| 34    | 0.87530 |  0.85222 |   72.9      \n",
      "| 35    | 0.88425 |  0.84585 |   74.9      \n",
      "| 36    | 0.88613 |  0.84992 |   77.2      \n",
      "| 37    | 0.88684 |  0.85726 |   79.5      \n",
      "| 38    | 0.88705 |  0.85177 |   81.6      \n",
      "| 39    | 0.88315 |  0.86540 |   83.8      \n",
      "| 40    | 0.88799 |  0.85955 |   86.1      \n",
      "| 41    | 0.88962 |  0.86510 |   88.4      \n",
      "| 42    | 0.88933 |  0.80940 |   90.7      \n",
      "| 43    | 0.88894 |  0.79979 |   93.0      \n",
      "| 44    | 0.89280 |  0.79154 |   95.2      \n",
      "Early stopping occured at epoch 44\n",
      "Training done in 95.222 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.61388 |  0.63027 |   1.8       \n",
      "| 2     | 0.79789 |  0.64712 |   3.9       \n",
      "| 3     | 0.82465 |  0.71222 |   6.1       \n",
      "| 4     | 0.84143 |  0.77502 |   8.1       \n",
      "| 5     | 0.85158 |  0.82416 |   10.2      \n",
      "| 6     | 0.85660 |  0.77193 |   12.1      \n",
      "| 7     | 0.83654 |  0.80551 |   14.2      \n",
      "| 8     | 0.83717 |  0.83280 |   16.4      \n",
      "| 9     | 0.84815 |  0.83953 |   18.5      \n",
      "| 10    | 0.85446 |  0.84029 |   20.4      \n",
      "| 11    | 0.85997 |  0.83800 |   22.5      \n",
      "| 12    | 0.86852 |  0.85333 |   24.5      \n",
      "| 13    | 0.87776 |  0.85706 |   26.6      \n",
      "| 14    | 0.88138 |  0.85200 |   28.5      \n",
      "| 15    | 0.88382 |  0.87072 |   30.5      \n",
      "| 16    | 0.89192 |  0.87515 |   32.6      \n",
      "| 17    | 0.89549 |  0.88529 |   34.8      \n",
      "| 18    | 0.90207 |  0.88999 |   36.6      \n",
      "| 19    | 0.90715 |  0.89302 |   38.6      \n",
      "| 20    | 0.90734 |  0.89472 |   40.7      \n",
      "| 21    | 0.90915 |  0.89550 |   42.7      \n",
      "| 22    | 0.90549 |  0.88559 |   44.7      \n",
      "| 23    | 0.89540 |  0.88670 |   46.7      \n",
      "| 24    | 0.90068 |  0.89565 |   48.8      \n",
      "| 25    | 0.90429 |  0.89057 |   51.0      \n",
      "| 26    | 0.90616 |  0.89032 |   53.0      \n",
      "| 27    | 0.90853 |  0.89810 |   55.0      \n",
      "| 28    | 0.91351 |  0.89754 |   57.2      \n",
      "| 29    | 0.90912 |  0.89889 |   59.1      \n",
      "| 30    | 0.91016 |  0.90555 |   61.3      \n",
      "| 31    | 0.91291 |  0.90785 |   63.4      \n",
      "| 32    | 0.91528 |  0.90347 |   65.6      \n",
      "| 33    | 0.90910 |  0.89980 |   67.7      \n",
      "| 34    | 0.91311 |  0.90558 |   69.8      \n",
      "| 35    | 0.91203 |  0.89299 |   71.9      \n",
      "| 36    | 0.90329 |  0.88666 |   74.0      \n",
      "Early stopping occured at epoch 36\n",
      "Training done in 73.999 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.61000 |  0.68365 |   2.1       \n",
      "| 2     | 0.74710 |  0.73009 |   4.1       \n",
      "| 3     | 0.76563 |  0.78544 |   6.1       \n",
      "| 4     | 0.81745 |  0.80637 |   8.3       \n",
      "| 5     | 0.84197 |  0.84806 |   10.3      \n",
      "| 6     | 0.86077 |  0.85860 |   12.3      \n",
      "| 7     | 0.87166 |  0.86251 |   14.5      \n",
      "| 8     | 0.88260 |  0.87246 |   16.6      \n",
      "| 9     | 0.89050 |  0.87627 |   18.7      \n",
      "| 10    | 0.89544 |  0.87974 |   20.8      \n",
      "| 11    | 0.90051 |  0.87980 |   22.9      \n",
      "| 12    | 0.89792 |  0.87564 |   25.0      \n",
      "| 13    | 0.89612 |  0.88123 |   27.1      \n",
      "| 14    | 0.89723 |  0.88539 |   29.2      \n",
      "| 15    | 0.89745 |  0.88806 |   31.3      \n",
      "| 16    | 0.89982 |  0.88842 |   33.4      \n",
      "| 17    | 0.89954 |  0.88801 |   35.4      \n",
      "| 18    | 0.90034 |  0.89170 |   37.5      \n",
      "| 19    | 0.89814 |  0.88936 |   39.4      \n",
      "| 20    | 0.89552 |  0.89148 |   41.5      \n",
      "| 21    | 0.89451 |  0.89445 |   43.5      \n",
      "| 22    | 0.89723 |  0.89388 |   45.5      \n",
      "| 23    | 0.90060 |  0.89794 |   47.7      \n",
      "| 24    | 0.90339 |  0.89442 |   49.8      \n",
      "| 25    | 0.90219 |  0.90045 |   52.0      \n",
      "| 26    | 0.90337 |  0.89646 |   53.9      \n",
      "| 27    | 0.90043 |  0.89551 |   55.9      \n",
      "| 28    | 0.89790 |  0.88959 |   58.0      \n",
      "| 29    | 0.88900 |  0.89374 |   60.1      \n",
      "| 30    | 0.89088 |  0.89460 |   62.1      \n",
      "Early stopping occured at epoch 30\n",
      "Training done in 62.108 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54279 |  0.58663 |   2.8       \n",
      "| 2     | 0.61180 |  0.67780 |   5.7       \n",
      "| 3     | 0.60412 |  0.67268 |   8.5       \n",
      "| 4     | 0.58520 |  0.66871 |   11.5      \n",
      "| 5     | 0.65060 |  0.77340 |   14.5      \n",
      "| 6     | 0.78585 |  0.80138 |   17.5      \n",
      "| 7     | 0.81211 |  0.80731 |   20.6      \n",
      "| 8     | 0.81194 |  0.77666 |   23.4      \n",
      "| 9     | 0.82304 |  0.81473 |   26.4      \n",
      "| 10    | 0.84161 |  0.81684 |   29.2      \n",
      "| 11    | 0.83439 |  0.79933 |   32.0      \n",
      "| 12    | 0.82300 |  0.80288 |   34.7      \n",
      "| 13    | 0.82755 |  0.80015 |   37.4      \n",
      "| 14    | 0.83103 |  0.82594 |   40.2      \n",
      "| 15    | 0.84182 |  0.83317 |   42.9      \n",
      "| 16    | 0.84381 |  0.85154 |   45.7      \n",
      "| 17    | 0.85277 |  0.83480 |   48.3      \n",
      "| 18    | 0.85006 |  0.82735 |   51.2      \n",
      "| 19    | 0.85428 |  0.83948 |   54.0      \n",
      "| 20    | 0.85729 |  0.84475 |   56.8      \n",
      "| 21    | 0.86099 |  0.84593 |   59.6      \n",
      "Early stopping occured at epoch 21\n",
      "Training done in 59.590 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52999 |  0.47989 |   2.9       \n",
      "| 2     | 0.63930 |  0.58672 |   5.7       \n",
      "| 3     | 0.56381 |  0.59775 |   8.6       \n",
      "| 4     | 0.60827 |  0.65129 |   11.4      \n",
      "| 5     | 0.71563 |  0.67622 |   14.2      \n",
      "| 6     | 0.73309 |  0.72768 |   17.1      \n",
      "| 7     | 0.76363 |  0.69106 |   20.2      \n",
      "| 8     | 0.79343 |  0.74915 |   23.0      \n",
      "| 9     | 0.80213 |  0.73781 |   25.8      \n",
      "| 10    | 0.78984 |  0.79142 |   28.8      \n",
      "| 11    | 0.77919 |  0.78614 |   31.4      \n",
      "| 12    | 0.82142 |  0.80266 |   34.3      \n",
      "| 13    | 0.85377 |  0.84161 |   37.1      \n",
      "| 14    | 0.86388 |  0.84111 |   39.9      \n",
      "| 15    | 0.87068 |  0.83467 |   42.8      \n",
      "| 16    | 0.87233 |  0.84751 |   45.7      \n",
      "| 17    | 0.87166 |  0.83107 |   48.5      \n",
      "| 18    | 0.87474 |  0.84882 |   51.5      \n",
      "| 19    | 0.87879 |  0.84566 |   54.5      \n",
      "| 20    | 0.87914 |  0.85033 |   57.4      \n",
      "| 21    | 0.87526 |  0.86422 |   60.4      \n",
      "| 22    | 0.87780 |  0.87103 |   63.5      \n",
      "| 23    | 0.88239 |  0.87644 |   66.6      \n",
      "| 24    | 0.88510 |  0.87853 |   69.7      \n",
      "| 25    | 0.88810 |  0.87829 |   72.6      \n",
      "| 26    | 0.88505 |  0.88261 |   75.6      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 27    | 0.88945 |  0.87054 |   78.4      \n",
      "| 28    | 0.85649 |  0.84335 |   81.3      \n",
      "| 29    | 0.84610 |  0.84579 |   84.2      \n",
      "| 30    | 0.83537 |  0.82546 |   87.2      \n",
      "| 31    | 0.84641 |  0.81317 |   90.5      \n",
      "Early stopping occured at epoch 31\n",
      "Training done in 90.474 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.50162 |  0.54267 |   4.3       \n",
      "| 2     | 0.57221 |  0.64221 |   8.5       \n",
      "| 3     | 0.65137 |  0.62072 |   12.6      \n",
      "| 4     | 0.63472 |  0.60554 |   16.8      \n",
      "| 5     | 0.67128 |  0.64900 |   21.1      \n",
      "| 6     | 0.60385 |  0.62076 |   25.2      \n",
      "| 7     | 0.63891 |  0.56609 |   29.2      \n",
      "| 8     | 0.62824 |  0.67343 |   33.3      \n",
      "| 9     | 0.69975 |  0.63916 |   37.4      \n",
      "| 10    | 0.66688 |  0.62799 |   41.5      \n",
      "| 11    | 0.67724 |  0.65323 |   45.5      \n",
      "| 12    | 0.75587 |  0.70558 |   49.5      \n",
      "| 13    | 0.77015 |  0.66385 |   53.6      \n",
      "| 14    | 0.78436 |  0.70240 |   57.7      \n",
      "| 15    | 0.79818 |  0.75267 |   61.8      \n",
      "| 16    | 0.80072 |  0.74542 |   65.9      \n",
      "| 17    | 0.80504 |  0.77079 |   69.9      \n",
      "| 18    | 0.82910 |  0.60477 |   73.9      \n",
      "| 19    | 0.83610 |  0.75860 |   78.0      \n",
      "| 20    | 0.84433 |  0.63304 |   82.1      \n",
      "| 21    | 0.84250 |  0.79942 |   86.1      \n",
      "| 22    | 0.83553 |  0.80014 |   90.1      \n",
      "| 23    | 0.84922 |  0.80690 |   94.1      \n",
      "| 24    | 0.85553 |  0.81710 |   98.3      \n",
      "| 25    | 0.84707 |  0.82861 |   102.4     \n",
      "| 26    | 0.85785 |  0.83278 |   106.4     \n",
      "| 27    | 0.85943 |  0.75758 |   110.4     \n",
      "| 28    | 0.86250 |  0.76906 |   114.3     \n",
      "| 29    | 0.86663 |  0.78210 |   118.4     \n",
      "| 30    | 0.86848 |  0.77629 |   122.5     \n",
      "| 31    | 0.86452 |  0.81051 |   126.6     \n",
      "Early stopping occured at epoch 31\n",
      "Training done in 126.627 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.49852 |  0.47140 |   4.1       \n",
      "| 2     | 0.50436 |  0.59605 |   8.2       \n",
      "| 3     | 0.58619 |  0.60743 |   12.3      \n",
      "| 4     | 0.61455 |  0.61881 |   16.4      \n",
      "| 5     | 0.63426 |  0.64694 |   20.6      \n",
      "| 6     | 0.66717 |  0.64998 |   24.9      \n",
      "| 7     | 0.66272 |  0.68553 |   29.0      \n",
      "| 8     | 0.67372 |  0.66654 |   33.2      \n",
      "| 9     | 0.73756 |  0.67844 |   37.3      \n",
      "| 10    | 0.78900 |  0.62148 |   41.7      \n",
      "| 11    | 0.77393 |  0.75079 |   46.0      \n",
      "| 12    | 0.78541 |  0.75597 |   50.4      \n",
      "| 13    | 0.74723 |  0.77205 |   54.4      \n",
      "| 14    | 0.81048 |  0.77549 |   58.3      \n",
      "| 15    | 0.81720 |  0.79313 |   62.3      \n",
      "| 16    | 0.81926 |  0.76743 |   66.4      \n",
      "| 17    | 0.82355 |  0.78295 |   70.4      \n",
      "| 18    | 0.82787 |  0.79858 |   74.3      \n",
      "| 19    | 0.82601 |  0.79238 |   78.2      \n",
      "| 20    | 0.82880 |  0.80511 |   82.3      \n",
      "| 21    | 0.83902 |  0.81852 |   86.4      \n",
      "| 22    | 0.83327 |  0.80697 |   90.4      \n",
      "| 23    | 0.84684 |  0.80487 |   94.4      \n",
      "| 24    | 0.84381 |  0.79882 |   98.4      \n",
      "| 25    | 0.84576 |  0.80704 |   102.6     \n",
      "| 26    | 0.84011 |  0.80890 |   106.7     \n",
      "Early stopping occured at epoch 26\n",
      "Training done in 106.661 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54395 |  0.55131 |   2.5       \n",
      "| 2     | 0.58533 |  0.49288 |   5.0       \n",
      "| 3     | 0.68668 |  0.54904 |   7.4       \n",
      "| 4     | 0.72761 |  0.63052 |   9.9       \n",
      "| 5     | 0.72207 |  0.56069 |   12.3      \n",
      "| 6     | 0.75313 |  0.68472 |   14.8      \n",
      "| 7     | 0.78794 |  0.74215 |   17.1      \n",
      "| 8     | 0.80054 |  0.69965 |   19.5      \n",
      "| 9     | 0.78576 |  0.68407 |   21.9      \n",
      "| 10    | 0.78765 |  0.67849 |   24.4      \n",
      "| 11    | 0.81558 |  0.75667 |   26.9      \n",
      "| 12    | 0.83194 |  0.75414 |   29.3      \n",
      "| 13    | 0.84752 |  0.75602 |   31.6      \n",
      "| 14    | 0.83853 |  0.74882 |   33.9      \n",
      "| 15    | 0.85836 |  0.78113 |   36.3      \n",
      "| 16    | 0.86376 |  0.83664 |   38.7      \n",
      "| 17    | 0.86749 |  0.84696 |   41.2      \n",
      "| 18    | 0.86412 |  0.85621 |   43.7      \n",
      "| 19    | 0.87037 |  0.83712 |   46.0      \n",
      "| 20    | 0.87312 |  0.84381 |   48.3      \n",
      "| 21    | 0.87185 |  0.85316 |   50.6      \n",
      "| 22    | 0.88005 |  0.86381 |   53.0      \n",
      "| 23    | 0.88227 |  0.86013 |   55.6      \n",
      "| 24    | 0.88455 |  0.87325 |   58.1      \n",
      "| 25    | 0.88185 |  0.86912 |   60.6      \n",
      "| 26    | 0.88524 |  0.87539 |   63.1      \n",
      "| 27    | 0.87989 |  0.87574 |   65.6      \n",
      "| 28    | 0.88734 |  0.88173 |   68.2      \n",
      "| 29    | 0.88675 |  0.88193 |   70.5      \n",
      "| 30    | 0.88535 |  0.87471 |   72.8      \n",
      "| 31    | 0.88902 |  0.87771 |   75.0      \n",
      "| 32    | 0.88668 |  0.87737 |   77.1      \n",
      "| 33    | 0.88844 |  0.86946 |   79.4      \n",
      "| 34    | 0.88715 |  0.84680 |   81.6      \n",
      "Early stopping occured at epoch 34\n",
      "Training done in 81.574 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51279 |  0.62565 |   2.3       \n",
      "| 2     | 0.62677 |  0.45523 |   4.5       \n",
      "| 3     | 0.70858 |  0.55628 |   6.8       \n",
      "| 4     | 0.77463 |  0.57042 |   9.2       \n",
      "| 5     | 0.78771 |  0.56111 |   11.5      \n",
      "| 6     | 0.78026 |  0.57200 |   13.8      \n",
      "Early stopping occured at epoch 6\n",
      "Training done in 13.775 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.67865 |  0.76895 |   4.3       \n",
      "| 2     | 0.78828 |  0.78550 |   8.7       \n",
      "| 3     | 0.80451 |  0.74494 |   12.4      \n",
      "| 4     | 0.81595 |  0.80539 |   16.9      \n",
      "| 5     | 0.83243 |  0.82904 |   21.3      \n",
      "| 6     | 0.85200 |  0.84523 |   25.7      \n",
      "| 7     | 0.86479 |  0.85471 |   30.1      \n",
      "| 8     | 0.87457 |  0.86142 |   34.4      \n",
      "| 9     | 0.87619 |  0.86984 |   38.7      \n",
      "| 10    | 0.87711 |  0.86295 |   43.1      \n",
      "| 11    | 0.86980 |  0.85523 |   47.4      \n",
      "| 12    | 0.87625 |  0.86846 |   51.7      \n",
      "| 13    | 0.88374 |  0.87755 |   56.1      \n",
      "| 14    | 0.88646 |  0.88064 |   60.4      \n",
      "| 15    | 0.88889 |  0.87620 |   64.8      \n",
      "| 16    | 0.88156 |  0.87343 |   68.8      \n",
      "| 17    | 0.88135 |  0.88102 |   73.2      \n",
      "| 18    | 0.88655 |  0.87713 |   77.5      \n",
      "| 19    | 0.88759 |  0.83917 |   81.7      \n",
      "| 20    | 0.88732 |  0.87173 |   86.0      \n",
      "| 21    | 0.88897 |  0.85537 |   90.3      \n",
      "| 22    | 0.88045 |  0.86485 |   94.9      \n",
      "Early stopping occured at epoch 22\n",
      "Training done in 94.930 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.66182 |  0.76168 |   4.1       \n",
      "| 2     | 0.79385 |  0.79745 |   8.6       \n",
      "| 3     | 0.81358 |  0.78627 |   13.1      \n",
      "| 4     | 0.81922 |  0.81966 |   17.5      \n",
      "| 5     | 0.84294 |  0.82968 |   21.8      \n",
      "| 6     | 0.86295 |  0.85245 |   26.0      \n",
      "| 7     | 0.87267 |  0.85783 |   30.4      \n",
      "| 8     | 0.87555 |  0.87140 |   34.7      \n",
      "| 9     | 0.88018 |  0.87448 |   39.1      \n",
      "| 10    | 0.88263 |  0.87449 |   43.5      \n",
      "| 11    | 0.88224 |  0.87362 |   47.7      \n",
      "| 12    | 0.88218 |  0.87917 |   52.1      \n",
      "| 13    | 0.86886 |  0.84505 |   56.5      \n",
      "| 14    | 0.87792 |  0.86015 |   60.6      \n",
      "| 15    | 0.88411 |  0.88087 |   65.0      \n",
      "| 16    | 0.89093 |  0.88251 |   69.3      \n",
      "| 17    | 0.89553 |  0.89160 |   73.5      \n",
      "| 18    | 0.89747 |  0.88797 |   77.5      \n",
      "| 19    | 0.88104 |  0.86983 |   81.6      \n",
      "| 20    | 0.89031 |  0.88060 |   85.7      \n",
      "| 21    | 0.88880 |  0.88445 |   89.8      \n",
      "| 22    | 0.89546 |  0.88455 |   94.0      \n",
      "Early stopping occured at epoch 22\n",
      "Training done in 94.028 seconds.\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51159 |  0.62327 |   3.2       \n",
      "| 2     | 0.65645 |  0.62575 |   6.5       \n",
      "| 3     | 0.74746 |  0.65442 |   9.8       \n",
      "| 4     | 0.67079 |  0.72261 |   13.0      \n",
      "| 5     | 0.78918 |  0.73115 |   16.3      \n",
      "| 6     | 0.80290 |  0.80162 |   19.6      \n",
      "| 7     | 0.74483 |  0.75545 |   22.7      \n",
      "| 8     | 0.79748 |  0.76249 |   25.9      \n",
      "| 9     | 0.82144 |  0.76367 |   29.2      \n",
      "| 10    | 0.83481 |  0.76732 |   32.4      \n",
      "| 11    | 0.84336 |  0.81004 |   35.7      \n",
      "| 12    | 0.86164 |  0.81575 |   39.0      \n",
      "| 13    | 0.86451 |  0.82440 |   42.1      \n",
      "| 14    | 0.86238 |  0.83333 |   45.4      \n",
      "| 15    | 0.86418 |  0.82951 |   48.6      \n",
      "| 16    | 0.86403 |  0.82859 |   51.8      \n",
      "| 17    | 0.86523 |  0.83017 |   54.9      \n",
      "| 18    | 0.87034 |  0.83455 |   58.2      \n",
      "| 19    | 0.87014 |  0.83966 |   61.3      \n",
      "| 20    | 0.86830 |  0.80487 |   64.5      \n",
      "| 21    | 0.87213 |  0.83707 |   67.5      \n",
      "| 22    | 0.87451 |  0.85434 |   70.8      \n",
      "| 23    | 0.87648 |  0.85310 |   74.0      \n",
      "| 24    | 0.87550 |  0.87178 |   77.2      \n",
      "| 25    | 0.88018 |  0.87184 |   80.3      \n",
      "| 26    | 0.87719 |  0.87509 |   83.5      \n",
      "| 27    | 0.87645 |  0.86820 |   86.5      \n",
      "| 28    | 0.87956 |  0.87206 |   89.6      \n",
      "| 29    | 0.88019 |  0.87558 |   92.8      \n",
      "| 30    | 0.89325 |  0.87291 |   96.1      \n",
      "| 31    | 0.89700 |  0.87195 |   99.3      \n",
      "| 32    | 0.89824 |  0.85099 |   102.5     \n",
      "| 33    | 0.89911 |  0.84223 |   105.7     \n",
      "| 34    | 0.89925 |  0.83507 |   109.0     \n",
      "Early stopping occured at epoch 34\n",
      "Training done in 108.957 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51492 |  0.59816 |   3.5       \n",
      "| 2     | 0.64090 |  0.64139 |   6.8       \n",
      "| 3     | 0.70805 |  0.58863 |   10.2      \n",
      "| 4     | 0.63602 |  0.67421 |   13.4      \n",
      "| 5     | 0.71722 |  0.61654 |   16.8      \n",
      "| 6     | 0.75411 |  0.72674 |   20.5      \n",
      "| 7     | 0.79040 |  0.74110 |   23.7      \n",
      "| 8     | 0.73620 |  0.73774 |   27.0      \n",
      "| 9     | 0.78260 |  0.76687 |   30.2      \n",
      "| 10    | 0.77602 |  0.77327 |   33.5      \n",
      "| 11    | 0.76804 |  0.76946 |   36.6      \n",
      "| 12    | 0.81112 |  0.79892 |   39.9      \n",
      "| 13    | 0.83989 |  0.81270 |   43.1      \n",
      "| 14    | 0.84706 |  0.82175 |   46.4      \n",
      "| 15    | 0.85465 |  0.82450 |   49.6      \n",
      "| 16    | 0.85606 |  0.81851 |   52.7      \n",
      "| 17    | 0.85689 |  0.82436 |   55.8      \n",
      "| 18    | 0.85474 |  0.83357 |   59.0      \n",
      "| 19    | 0.85571 |  0.83452 |   62.3      \n",
      "| 20    | 0.85885 |  0.83516 |   65.4      \n",
      "| 21    | 0.86225 |  0.84620 |   68.6      \n",
      "| 22    | 0.86412 |  0.84156 |   71.8      \n",
      "| 23    | 0.86671 |  0.84068 |   74.9      \n",
      "| 24    | 0.86578 |  0.83981 |   78.1      \n",
      "| 25    | 0.87301 |  0.85214 |   81.3      \n",
      "| 26    | 0.87855 |  0.84186 |   84.5      \n",
      "| 27    | 0.87744 |  0.85682 |   87.7      \n",
      "| 28    | 0.87945 |  0.85813 |   91.0      \n",
      "| 29    | 0.87983 |  0.86606 |   94.2      \n",
      "| 30    | 0.88095 |  0.86591 |   97.4      \n",
      "| 31    | 0.88533 |  0.87147 |   100.6     \n",
      "| 32    | 0.88806 |  0.86787 |   103.9     \n",
      "| 33    | 0.88990 |  0.86705 |   107.1     \n",
      "| 34    | 0.89226 |  0.86508 |   110.2     \n",
      "| 35    | 0.89516 |  0.86493 |   113.3     \n",
      "| 36    | 0.89560 |  0.86199 |   116.5     \n",
      "Early stopping occured at epoch 36\n",
      "Training done in 116.507 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.63935 |  0.71016 |   2.0       \n",
      "| 2     | 0.82897 |  0.80202 |   3.9       \n",
      "| 3     | 0.85558 |  0.83642 |   5.6       \n",
      "| 4     | 0.86996 |  0.84157 |   7.4       \n",
      "| 5     | 0.87727 |  0.85595 |   9.0       \n",
      "| 6     | 0.88298 |  0.86394 |   10.8      \n",
      "| 7     | 0.88817 |  0.86041 |   12.5      \n",
      "| 8     | 0.89030 |  0.87190 |   14.4      \n",
      "| 9     | 0.89525 |  0.87341 |   16.2      \n",
      "| 10    | 0.89512 |  0.88215 |   17.9      \n",
      "| 11    | 0.89694 |  0.88646 |   19.7      \n",
      "| 12    | 0.90005 |  0.88794 |   21.5      \n",
      "| 13    | 0.90234 |  0.88910 |   23.2      \n",
      "| 14    | 0.90487 |  0.89352 |   25.0      \n",
      "| 15    | 0.90898 |  0.89180 |   26.7      \n",
      "| 16    | 0.90927 |  0.89340 |   28.5      \n",
      "| 17    | 0.90934 |  0.89299 |   30.2      \n",
      "| 18    | 0.91284 |  0.89907 |   32.0      \n",
      "| 19    | 0.91565 |  0.90048 |   33.8      \n",
      "| 20    | 0.91915 |  0.90122 |   35.5      \n",
      "| 21    | 0.92183 |  0.90330 |   37.2      \n",
      "| 22    | 0.92164 |  0.90412 |   39.0      \n",
      "| 23    | 0.92321 |  0.90471 |   40.8      \n",
      "| 24    | 0.92129 |  0.90419 |   42.6      \n",
      "| 25    | 0.92060 |  0.90418 |   44.3      \n",
      "| 26    | 0.92434 |  0.90370 |   46.1      \n",
      "| 27    | 0.92577 |  0.90506 |   47.9      \n",
      "| 28    | 0.92722 |  0.90783 |   49.7      \n",
      "| 29    | 0.92801 |  0.90693 |   51.5      \n",
      "| 30    | 0.93004 |  0.90675 |   53.3      \n",
      "| 31    | 0.93138 |  0.90670 |   55.0      \n",
      "| 32    | 0.93030 |  0.91010 |   56.7      \n",
      "| 33    | 0.93142 |  0.91154 |   58.5      \n",
      "| 34    | 0.93159 |  0.90419 |   60.2      \n",
      "| 35    | 0.93347 |  0.91128 |   62.0      \n",
      "| 36    | 0.92009 |  0.89320 |   63.8      \n",
      "| 37    | 0.91584 |  0.89270 |   65.7      \n",
      "| 38    | 0.91803 |  0.85892 |   67.4      \n",
      "Early stopping occured at epoch 38\n",
      "Training done in 67.373 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.64489 |  0.67457 |   1.7       \n",
      "| 2     | 0.82245 |  0.72103 |   3.4       \n",
      "| 3     | 0.84697 |  0.77668 |   5.2       \n",
      "| 4     | 0.86302 |  0.82019 |   6.9       \n",
      "| 5     | 0.87629 |  0.84965 |   8.7       \n",
      "| 6     | 0.89090 |  0.85819 |   10.4      \n",
      "| 7     | 0.89647 |  0.86070 |   12.0      \n",
      "| 8     | 0.89565 |  0.86419 |   13.7      \n",
      "| 9     | 0.90141 |  0.88095 |   15.5      \n",
      "| 10    | 0.90619 |  0.88521 |   17.4      \n",
      "| 11    | 0.90961 |  0.87919 |   19.2      \n",
      "| 12    | 0.91198 |  0.89497 |   21.0      \n",
      "| 13    | 0.91338 |  0.89596 |   22.8      \n",
      "| 14    | 0.91792 |  0.90599 |   24.7      \n",
      "| 15    | 0.91966 |  0.90235 |   26.4      \n",
      "| 16    | 0.92313 |  0.90027 |   28.0      \n",
      "| 17    | 0.92601 |  0.90820 |   29.7      \n",
      "| 18    | 0.92606 |  0.91086 |   31.5      \n",
      "| 19    | 0.92886 |  0.91298 |   33.3      \n",
      "| 20    | 0.93030 |  0.91507 |   35.0      \n",
      "| 21    | 0.93153 |  0.91282 |   36.7      \n",
      "| 22    | 0.93167 |  0.91180 |   38.4      \n",
      "| 23    | 0.93171 |  0.91409 |   40.1      \n",
      "| 24    | 0.93298 |  0.90709 |   41.8      \n",
      "| 25    | 0.93138 |  0.90276 |   43.6      \n",
      "Early stopping occured at epoch 25\n",
      "Training done in 43.582 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.65959 |  0.69570 |   3.6       \n",
      "| 2     | 0.77964 |  0.72113 |   7.3       \n",
      "| 3     | 0.81709 |  0.79864 |   10.6      \n",
      "| 4     | 0.84639 |  0.84808 |   14.2      \n",
      "| 5     | 0.86562 |  0.86060 |   17.7      \n",
      "| 6     | 0.87542 |  0.86690 |   21.2      \n",
      "| 7     | 0.88324 |  0.86964 |   24.7      \n",
      "| 8     | 0.88714 |  0.88015 |   28.1      \n",
      "| 9     | 0.89164 |  0.87897 |   31.5      \n",
      "| 10    | 0.89411 |  0.88742 |   35.2      \n",
      "| 11    | 0.90061 |  0.89356 |   38.7      \n",
      "| 12    | 0.90398 |  0.89572 |   42.3      \n",
      "| 13    | 0.90907 |  0.89250 |   45.9      \n",
      "| 14    | 0.90858 |  0.90173 |   49.6      \n",
      "| 15    | 0.91041 |  0.88553 |   53.2      \n",
      "| 16    | 0.90381 |  0.90089 |   57.0      \n",
      "| 17    | 0.91141 |  0.89941 |   60.5      \n",
      "| 18    | 0.91494 |  0.88570 |   63.8      \n",
      "| 19    | 0.91735 |  0.90059 |   67.3      \n",
      "Early stopping occured at epoch 19\n",
      "Training done in 67.327 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1     | 0.62061 |  0.71110 |   3.5       \n",
      "| 2     | 0.79214 |  0.72197 |   7.0       \n",
      "| 3     | 0.80923 |  0.78768 |   10.6      \n",
      "| 4     | 0.82929 |  0.80743 |   14.0      \n",
      "| 5     | 0.84267 |  0.77832 |   17.5      \n",
      "| 6     | 0.86428 |  0.85720 |   20.9      \n",
      "| 7     | 0.85491 |  0.83803 |   24.5      \n",
      "| 8     | 0.83050 |  0.84474 |   28.1      \n",
      "| 9     | 0.86626 |  0.86343 |   31.8      \n",
      "| 10    | 0.85367 |  0.86685 |   35.5      \n",
      "| 11    | 0.85937 |  0.74854 |   39.1      \n",
      "| 12    | 0.86596 |  0.79115 |   42.7      \n",
      "| 13    | 0.87523 |  0.85412 |   46.3      \n",
      "| 14    | 0.87725 |  0.78900 |   49.6      \n",
      "| 15    | 0.88445 |  0.76848 |   53.0      \n",
      "Early stopping occured at epoch 15\n",
      "Training done in 52.973 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52978 |  0.56464 |   4.5       \n",
      "| 2     | 0.61227 |  0.64745 |   8.9       \n",
      "| 3     | 0.67558 |  0.64465 |   13.2      \n",
      "| 4     | 0.70054 |  0.69835 |   17.6      \n",
      "| 5     | 0.74017 |  0.74306 |   22.3      \n",
      "| 6     | 0.77279 |  0.79016 |   26.8      \n",
      "| 7     | 0.80224 |  0.83132 |   31.2      \n",
      "| 8     | 0.81442 |  0.80216 |   35.6      \n",
      "| 9     | 0.82458 |  0.78061 |   40.1      \n",
      "| 10    | 0.81115 |  0.82795 |   44.5      \n",
      "| 11    | 0.81933 |  0.81439 |   49.0      \n",
      "| 12    | 0.82542 |  0.81406 |   53.5      \n",
      "Early stopping occured at epoch 12\n",
      "Training done in 53.453 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52196 |  0.56608 |   4.4       \n",
      "| 2     | 0.55865 |  0.62256 |   8.9       \n",
      "| 3     | 0.63847 |  0.61158 |   13.2      \n",
      "| 4     | 0.69050 |  0.68905 |   17.7      \n",
      "| 5     | 0.74009 |  0.80217 |   22.2      \n",
      "| 6     | 0.80741 |  0.82703 |   26.5      \n",
      "| 7     | 0.84596 |  0.78487 |   30.9      \n",
      "| 8     | 0.86059 |  0.83540 |   35.4      \n",
      "| 9     | 0.86988 |  0.86153 |   40.3      \n",
      "| 10    | 0.87554 |  0.84150 |   45.0      \n",
      "| 11    | 0.87700 |  0.85029 |   49.6      \n",
      "| 12    | 0.87200 |  0.84163 |   54.4      \n",
      "| 13    | 0.88017 |  0.86064 |   59.5      \n",
      "| 14    | 0.88022 |  0.86704 |   64.3      \n",
      "| 15    | 0.88692 |  0.87091 |   69.2      \n",
      "| 16    | 0.88577 |  0.87542 |   74.6      \n",
      "| 17    | 0.89052 |  0.87732 |   79.6      \n",
      "| 18    | 0.89282 |  0.87933 |   84.6      \n",
      "| 19    | 0.89469 |  0.87501 |   89.6      \n",
      "| 20    | 0.89503 |  0.88644 |   94.5      \n",
      "| 21    | 0.90013 |  0.88994 |   99.9      \n",
      "| 22    | 0.89952 |  0.89377 |   105.2     \n",
      "| 23    | 0.89417 |  0.81266 |   110.4     \n",
      "| 24    | 0.88139 |  0.83642 |   115.6     \n",
      "| 25    | 0.87733 |  0.78438 |   120.5     \n",
      "| 26    | 0.84995 |  0.69226 |   125.4     \n",
      "| 27    | 0.84907 |  0.75162 |   130.5     \n",
      "Early stopping occured at epoch 27\n",
      "Training done in 130.498 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52536 |  0.51304 |   3.0       \n",
      "| 2     | 0.68858 |  0.58722 |   6.3       \n",
      "| 3     | 0.77891 |  0.68135 |   9.5       \n",
      "| 4     | 0.80474 |  0.71810 |   12.6      \n",
      "| 5     | 0.80651 |  0.73442 |   15.7      \n",
      "| 6     | 0.78435 |  0.75884 |   18.9      \n",
      "| 7     | 0.81445 |  0.81274 |   22.0      \n",
      "| 8     | 0.82819 |  0.83078 |   25.0      \n",
      "| 9     | 0.84192 |  0.83598 |   27.9      \n",
      "| 10    | 0.83720 |  0.83290 |   31.1      \n",
      "| 11    | 0.84948 |  0.83489 |   34.1      \n",
      "| 12    | 0.85549 |  0.81610 |   37.2      \n",
      "| 13    | 0.85768 |  0.82813 |   40.2      \n",
      "| 14    | 0.86129 |  0.82706 |   43.4      \n",
      "Early stopping occured at epoch 14\n",
      "Training done in 43.421 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52219 |  0.48779 |   3.2       \n",
      "| 2     | 0.65511 |  0.55208 |   6.2       \n",
      "| 3     | 0.69255 |  0.61469 |   9.5       \n",
      "| 4     | 0.79735 |  0.70721 |   12.7      \n",
      "| 5     | 0.81773 |  0.71045 |   16.1      \n",
      "| 6     | 0.81541 |  0.70700 |   19.2      \n",
      "| 7     | 0.78229 |  0.70806 |   22.3      \n",
      "| 8     | 0.79491 |  0.75942 |   25.5      \n",
      "| 9     | 0.79764 |  0.73440 |   28.7      \n",
      "| 10    | 0.80247 |  0.74022 |   31.9      \n",
      "| 11    | 0.82336 |  0.74842 |   35.1      \n",
      "| 12    | 0.84192 |  0.76515 |   38.1      \n",
      "| 13    | 0.84057 |  0.80368 |   41.2      \n",
      "| 14    | 0.84116 |  0.82641 |   44.2      \n",
      "| 15    | 0.83540 |  0.80277 |   47.1      \n",
      "| 16    | 0.82823 |  0.81321 |   50.1      \n",
      "| 17    | 0.84108 |  0.80445 |   53.1      \n",
      "| 18    | 0.84938 |  0.79920 |   56.2      \n",
      "| 19    | 0.84127 |  0.81879 |   59.4      \n",
      "Early stopping occured at epoch 19\n",
      "Training done in 59.441 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.56533 |  0.57720 |   3.0       \n",
      "| 2     | 0.76093 |  0.76772 |   6.0       \n",
      "| 3     | 0.81097 |  0.78112 |   9.1       \n",
      "| 4     | 0.83032 |  0.80631 |   12.1      \n",
      "| 5     | 0.83902 |  0.80937 |   15.2      \n",
      "| 6     | 0.84939 |  0.81417 |   18.2      \n",
      "| 7     | 0.85141 |  0.82857 |   21.3      \n",
      "| 8     | 0.85303 |  0.83356 |   24.3      \n",
      "| 9     | 0.86832 |  0.85495 |   27.4      \n",
      "| 10    | 0.87794 |  0.86919 |   30.3      \n",
      "| 11    | 0.88779 |  0.87942 |   33.4      \n",
      "| 12    | 0.88263 |  0.87734 |   36.5      \n",
      "| 13    | 0.89384 |  0.89095 |   39.7      \n",
      "| 14    | 0.90716 |  0.89940 |   42.9      \n",
      "| 15    | 0.91227 |  0.90381 |   45.9      \n",
      "| 16    | 0.91520 |  0.90636 |   48.9      \n",
      "| 17    | 0.92058 |  0.90326 |   52.0      \n",
      "| 18    | 0.92239 |  0.90675 |   55.2      \n",
      "| 19    | 0.92431 |  0.90963 |   58.2      \n",
      "| 20    | 0.92584 |  0.89238 |   61.3      \n",
      "| 21    | 0.92867 |  0.89031 |   64.4      \n",
      "| 22    | 0.92879 |  0.90610 |   67.5      \n",
      "| 23    | 0.92939 |  0.89975 |   70.5      \n",
      "| 24    | 0.93146 |  0.91410 |   73.6      \n",
      "| 25    | 0.93163 |  0.91619 |   76.7      \n",
      "| 26    | 0.93171 |  0.91744 |   79.9      \n",
      "| 27    | 0.93272 |  0.91695 |   83.0      \n",
      "| 28    | 0.93112 |  0.91660 |   85.9      \n",
      "| 29    | 0.93335 |  0.91812 |   89.0      \n",
      "| 30    | 0.93326 |  0.91882 |   92.1      \n",
      "| 31    | 0.93078 |  0.91478 |   95.0      \n",
      "| 32    | 0.93415 |  0.91984 |   98.1      \n",
      "| 33    | 0.93642 |  0.91911 |   101.3     \n",
      "| 34    | 0.93691 |  0.91750 |   104.4     \n",
      "| 35    | 0.93720 |  0.91798 |   107.6     \n",
      "| 36    | 0.93873 |  0.91828 |   110.7     \n",
      "| 37    | 0.93959 |  0.91852 |   113.7     \n",
      "Early stopping occured at epoch 37\n",
      "Training done in 113.705 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.58682 |  0.65700 |   2.7       \n",
      "| 2     | 0.74781 |  0.69394 |   5.6       \n",
      "| 3     | 0.78873 |  0.80747 |   8.6       \n",
      "| 4     | 0.82536 |  0.80315 |   11.7      \n",
      "| 5     | 0.82248 |  0.83479 |   14.6      \n",
      "| 6     | 0.83024 |  0.81243 |   17.7      \n",
      "| 7     | 0.83353 |  0.83869 |   20.9      \n",
      "| 8     | 0.84885 |  0.82363 |   24.1      \n",
      "| 9     | 0.85362 |  0.84577 |   27.1      \n",
      "| 10    | 0.86217 |  0.85355 |   30.2      \n",
      "| 11    | 0.86746 |  0.84066 |   33.4      \n",
      "| 12    | 0.86748 |  0.84050 |   36.6      \n",
      "| 13    | 0.86994 |  0.85887 |   39.7      \n",
      "| 14    | 0.87144 |  0.86145 |   42.8      \n",
      "| 15    | 0.87660 |  0.86335 |   45.8      \n",
      "| 16    | 0.88062 |  0.86708 |   48.9      \n",
      "| 17    | 0.87977 |  0.87076 |   51.9      \n",
      "| 18    | 0.88064 |  0.87136 |   54.9      \n",
      "| 19    | 0.88864 |  0.87524 |   58.1      \n",
      "| 20    | 0.89503 |  0.88623 |   61.2      \n",
      "| 21    | 0.89735 |  0.88855 |   64.2      \n",
      "| 22    | 0.89961 |  0.88104 |   67.6      \n",
      "| 23    | 0.89874 |  0.87786 |   71.2      \n",
      "| 24    | 0.90277 |  0.88896 |   74.3      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 25    | 0.90830 |  0.89597 |   77.5      \n",
      "| 26    | 0.91116 |  0.89772 |   80.7      \n",
      "| 27    | 0.91026 |  0.88413 |   83.7      \n",
      "| 28    | 0.90692 |  0.87004 |   86.8      \n",
      "| 29    | 0.91219 |  0.81889 |   89.8      \n",
      "| 30    | 0.91580 |  0.87494 |   92.9      \n",
      "| 31    | 0.91406 |  0.89187 |   96.1      \n",
      "Early stopping occured at epoch 31\n",
      "Training done in 96.074 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.48887 |  0.50854 |   6.6       \n",
      "| 2     | 0.55554 |  0.54611 |   13.2      \n",
      "| 3     | 0.54929 |  0.65410 |   19.9      \n",
      "| 4     | 0.51741 |  0.56708 |   26.3      \n",
      "| 5     | 0.50233 |  0.49170 |   32.7      \n",
      "| 6     | 0.51842 |  0.53531 |   39.0      \n",
      "| 7     | 0.55205 |  0.57524 |   45.3      \n",
      "| 8     | 0.62794 |  0.62762 |   51.8      \n",
      "Early stopping occured at epoch 8\n",
      "Training done in 51.832 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.48181 |  0.51100 |   6.6       \n",
      "| 2     | 0.51356 |  0.42891 |   13.1      \n",
      "| 3     | 0.53418 |  0.60101 |   20.0      \n",
      "| 4     | 0.58547 |  0.64122 |   26.4      \n",
      "| 5     | 0.55204 |  0.64113 |   32.8      \n",
      "| 6     | 0.66751 |  0.63171 |   39.2      \n",
      "| 7     | 0.60355 |  0.52811 |   45.5      \n",
      "| 8     | 0.58301 |  0.54673 |   51.7      \n",
      "| 9     | 0.58832 |  0.60074 |   58.1      \n",
      "Early stopping occured at epoch 9\n",
      "Training done in 58.147 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51273 |  0.57612 |   4.1       \n",
      "| 2     | 0.54468 |  0.59335 |   8.4       \n",
      "| 3     | 0.61430 |  0.48749 |   12.5      \n",
      "| 4     | 0.60131 |  0.57299 |   16.7      \n",
      "| 5     | 0.59965 |  0.56765 |   21.0      \n",
      "| 6     | 0.58831 |  0.62777 |   25.2      \n",
      "| 7     | 0.60159 |  0.57461 |   29.4      \n",
      "| 8     | 0.61417 |  0.53214 |   33.7      \n",
      "| 9     | 0.62286 |  0.64893 |   37.8      \n",
      "| 10    | 0.68140 |  0.59094 |   42.0      \n",
      "| 11    | 0.72671 |  0.71585 |   46.2      \n",
      "| 12    | 0.75686 |  0.70731 |   50.2      \n",
      "| 13    | 0.78030 |  0.73302 |   54.4      \n",
      "| 14    | 0.80048 |  0.73045 |   58.5      \n",
      "| 15    | 0.81488 |  0.74567 |   62.6      \n",
      "| 16    | 0.82817 |  0.71935 |   66.8      \n",
      "| 17    | 0.83313 |  0.73945 |   70.9      \n",
      "| 18    | 0.83950 |  0.74044 |   75.0      \n",
      "| 19    | 0.84668 |  0.77625 |   79.2      \n",
      "| 20    | 0.85552 |  0.78463 |   83.4      \n",
      "| 21    | 0.86127 |  0.79220 |   87.6      \n",
      "| 22    | 0.86089 |  0.80246 |   91.8      \n",
      "| 23    | 0.86316 |  0.81447 |   96.1      \n",
      "| 24    | 0.86481 |  0.82723 |   100.4     \n",
      "| 25    | 0.86853 |  0.81062 |   104.5     \n",
      "| 26    | 0.87094 |  0.79929 |   108.8     \n",
      "| 27    | 0.87585 |  0.83454 |   112.9     \n",
      "| 28    | 0.87598 |  0.83702 |   117.3     \n",
      "| 29    | 0.87733 |  0.85207 |   121.3     \n",
      "| 30    | 0.87992 |  0.86447 |   125.4     \n",
      "| 31    | 0.88594 |  0.86680 |   129.3     \n",
      "| 32    | 0.88903 |  0.83090 |   133.4     \n",
      "| 33    | 0.89414 |  0.85615 |   137.4     \n",
      "| 34    | 0.89614 |  0.87070 |   141.5     \n",
      "| 35    | 0.89732 |  0.86576 |   145.6     \n",
      "| 36    | 0.89810 |  0.85480 |   149.7     \n",
      "| 37    | 0.89840 |  0.85622 |   153.8     \n",
      "| 38    | 0.89734 |  0.85704 |   157.9     \n",
      "| 39    | 0.89368 |  0.85257 |   162.1     \n",
      "Early stopping occured at epoch 39\n",
      "Training done in 162.058 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.49299 |  0.51732 |   4.1       \n",
      "| 2     | 0.53937 |  0.64832 |   8.2       \n",
      "| 3     | 0.61242 |  0.56325 |   12.3      \n",
      "| 4     | 0.58878 |  0.63266 |   16.5      \n",
      "| 5     | 0.69154 |  0.70032 |   20.7      \n",
      "| 6     | 0.73345 |  0.64994 |   24.8      \n",
      "| 7     | 0.73982 |  0.67667 |   28.9      \n",
      "| 8     | 0.67081 |  0.59623 |   33.2      \n",
      "| 9     | 0.65190 |  0.65703 |   37.2      \n",
      "| 10    | 0.75028 |  0.70248 |   41.3      \n",
      "| 11    | 0.73822 |  0.74904 |   45.5      \n",
      "| 12    | 0.77116 |  0.76780 |   49.7      \n",
      "| 13    | 0.79738 |  0.77837 |   53.8      \n",
      "| 14    | 0.79584 |  0.77992 |   57.9      \n",
      "| 15    | 0.82444 |  0.76372 |   62.0      \n",
      "| 16    | 0.82665 |  0.75818 |   66.1      \n",
      "| 17    | 0.82738 |  0.75141 |   70.2      \n",
      "| 18    | 0.83708 |  0.77057 |   74.4      \n",
      "| 19    | 0.83849 |  0.79918 |   78.7      \n",
      "| 20    | 0.83829 |  0.78964 |   83.1      \n",
      "| 21    | 0.83665 |  0.80378 |   87.4      \n",
      "| 22    | 0.83526 |  0.81009 |   91.8      \n",
      "| 23    | 0.83915 |  0.81939 |   96.0      \n",
      "| 24    | 0.84908 |  0.81872 |   100.4     \n",
      "| 25    | 0.85055 |  0.81142 |   104.7     \n",
      "| 26    | 0.85424 |  0.79946 |   109.1     \n",
      "| 27    | 0.85686 |  0.82715 |   113.8     \n",
      "| 28    | 0.85972 |  0.83130 |   118.3     \n",
      "| 29    | 0.86739 |  0.84042 |   122.5     \n",
      "| 30    | 0.87089 |  0.83587 |   126.7     \n",
      "| 31    | 0.87524 |  0.79107 |   131.0     \n",
      "| 32    | 0.87794 |  0.84897 |   135.3     \n",
      "| 33    | 0.88041 |  0.84942 |   139.5     \n",
      "| 34    | 0.87929 |  0.85547 |   143.6     \n",
      "| 35    | 0.88567 |  0.85368 |   147.8     \n",
      "| 36    | 0.88528 |  0.85606 |   152.0     \n",
      "| 37    | 0.88470 |  0.85935 |   156.4     \n",
      "| 38    | 0.88908 |  0.85876 |   160.5     \n",
      "| 39    | 0.88989 |  0.86596 |   164.7     \n",
      "| 40    | 0.89029 |  0.86526 |   168.8     \n",
      "| 41    | 0.89037 |  0.86335 |   172.9     \n",
      "| 42    | 0.89054 |  0.86312 |   176.9     \n",
      "| 43    | 0.89182 |  0.86930 |   181.0     \n",
      "| 44    | 0.89361 |  0.87046 |   185.1     \n",
      "| 45    | 0.89297 |  0.86803 |   189.1     \n",
      "| 46    | 0.89229 |  0.86802 |   193.2     \n",
      "| 47    | 0.89103 |  0.86676 |   197.3     \n",
      "| 48    | 0.88693 |  0.86323 |   201.5     \n",
      "| 49    | 0.88931 |  0.86453 |   205.6     \n",
      "Early stopping occured at epoch 49\n",
      "Training done in 205.618 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.53124 |  0.48215 |   3.8       \n",
      "| 2     | 0.64006 |  0.66371 |   7.7       \n",
      "| 3     | 0.73480 |  0.67345 |   11.7      \n",
      "| 4     | 0.77613 |  0.73987 |   15.7      \n",
      "| 5     | 0.79205 |  0.67290 |   19.7      \n",
      "| 6     | 0.79534 |  0.77557 |   23.8      \n",
      "| 7     | 0.79664 |  0.74841 |   28.0      \n",
      "| 8     | 0.79481 |  0.75605 |   31.9      \n",
      "| 9     | 0.80905 |  0.77520 |   35.9      \n",
      "| 10    | 0.80974 |  0.76869 |   39.8      \n",
      "| 11    | 0.81857 |  0.77175 |   43.7      \n",
      "Early stopping occured at epoch 11\n",
      "Training done in 43.705 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51427 |  0.54894 |   3.7       \n",
      "| 2     | 0.61343 |  0.48034 |   7.4       \n",
      "| 3     | 0.61694 |  0.44838 |   11.2      \n",
      "| 4     | 0.70718 |  0.62458 |   15.0      \n",
      "| 5     | 0.75488 |  0.66054 |   18.9      \n",
      "| 6     | 0.78919 |  0.63295 |   22.6      \n",
      "| 7     | 0.79834 |  0.65854 |   26.5      \n",
      "| 8     | 0.80782 |  0.62891 |   30.2      \n",
      "| 9     | 0.81700 |  0.59245 |   33.9      \n",
      "| 10    | 0.82558 |  0.62610 |   37.7      \n",
      "Early stopping occured at epoch 10\n",
      "Training done in 37.692 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51533 |  0.50729 |   4.2       \n",
      "| 2     | 0.60817 |  0.60662 |   8.5       \n",
      "| 3     | 0.57854 |  0.54118 |   12.4      \n",
      "| 4     | 0.60288 |  0.54025 |   16.5      \n",
      "| 5     | 0.65879 |  0.62089 |   20.6      \n",
      "| 6     | 0.74058 |  0.74148 |   24.8      \n",
      "| 7     | 0.77925 |  0.79885 |   28.9      \n",
      "| 8     | 0.80087 |  0.76907 |   33.0      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 9     | 0.81416 |  0.75858 |   37.1      \n",
      "| 10    | 0.82334 |  0.77664 |   41.2      \n",
      "| 11    | 0.82765 |  0.78467 |   45.1      \n",
      "| 12    | 0.82757 |  0.77018 |   49.2      \n",
      "Early stopping occured at epoch 12\n",
      "Training done in 49.158 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51394 |  0.48985 |   4.3       \n",
      "| 2     | 0.53664 |  0.63032 |   8.3       \n",
      "| 3     | 0.66628 |  0.58079 |   12.1      \n",
      "| 4     | 0.69847 |  0.67224 |   16.1      \n",
      "| 5     | 0.72186 |  0.66335 |   20.3      \n",
      "| 6     | 0.75140 |  0.62585 |   24.4      \n",
      "| 7     | 0.76224 |  0.68772 |   28.7      \n",
      "| 8     | 0.77909 |  0.70892 |   32.8      \n",
      "| 9     | 0.78482 |  0.75092 |   36.9      \n",
      "| 10    | 0.78541 |  0.76770 |   41.0      \n",
      "| 11    | 0.78952 |  0.75831 |   45.3      \n",
      "| 12    | 0.81211 |  0.78614 |   49.5      \n",
      "| 13    | 0.82037 |  0.79126 |   53.5      \n",
      "| 14    | 0.82784 |  0.80753 |   57.7      \n",
      "| 15    | 0.82039 |  0.82083 |   62.0      \n",
      "| 16    | 0.83485 |  0.81564 |   66.2      \n",
      "| 17    | 0.84715 |  0.82981 |   70.2      \n",
      "| 18    | 0.85050 |  0.84422 |   74.4      \n",
      "| 19    | 0.86063 |  0.84292 |   78.6      \n",
      "| 20    | 0.86508 |  0.84790 |   82.7      \n",
      "| 21    | 0.86868 |  0.85250 |   86.9      \n",
      "| 22    | 0.86919 |  0.86193 |   91.0      \n",
      "| 23    | 0.87338 |  0.85246 |   95.1      \n",
      "| 24    | 0.87942 |  0.84941 |   99.2      \n",
      "| 25    | 0.88117 |  0.87616 |   103.2     \n",
      "| 26    | 0.88462 |  0.87477 |   107.2     \n",
      "| 27    | 0.88838 |  0.87315 |   111.3     \n",
      "| 28    | 0.88768 |  0.86799 |   115.2     \n",
      "| 29    | 0.88801 |  0.88366 |   119.1     \n",
      "| 30    | 0.88817 |  0.88026 |   122.8     \n",
      "| 31    | 0.88695 |  0.88161 |   127.0     \n",
      "| 32    | 0.88923 |  0.87913 |   131.0     \n",
      "| 33    | 0.89171 |  0.86619 |   135.0     \n",
      "| 34    | 0.89222 |  0.88315 |   139.0     \n",
      "Early stopping occured at epoch 34\n",
      "Training done in 138.995 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.59023 |  0.61779 |   2.7       \n",
      "| 2     | 0.79598 |  0.65605 |   5.7       \n",
      "| 3     | 0.81502 |  0.71948 |   8.6       \n",
      "| 4     | 0.83150 |  0.72421 |   11.4      \n",
      "| 5     | 0.84313 |  0.73311 |   14.1      \n",
      "| 6     | 0.85497 |  0.73166 |   16.9      \n",
      "| 7     | 0.87596 |  0.80401 |   19.6      \n",
      "| 8     | 0.87913 |  0.84370 |   22.5      \n",
      "| 9     | 0.88825 |  0.84783 |   25.5      \n",
      "| 10    | 0.89343 |  0.87465 |   28.3      \n",
      "| 11    | 0.89114 |  0.87429 |   31.2      \n",
      "| 12    | 0.89442 |  0.88788 |   34.1      \n",
      "| 13    | 0.90065 |  0.88958 |   37.2      \n",
      "| 14    | 0.90515 |  0.89461 |   40.0      \n",
      "| 15    | 0.90707 |  0.89676 |   42.9      \n",
      "| 16    | 0.91134 |  0.89694 |   45.8      \n",
      "| 17    | 0.91044 |  0.90050 |   48.6      \n",
      "| 18    | 0.91471 |  0.90339 |   51.6      \n",
      "| 19    | 0.91815 |  0.90078 |   54.4      \n",
      "| 20    | 0.92007 |  0.90278 |   57.3      \n",
      "| 21    | 0.92291 |  0.90432 |   60.2      \n",
      "| 22    | 0.92466 |  0.90863 |   63.2      \n",
      "| 23    | 0.92527 |  0.90683 |   65.9      \n",
      "| 24    | 0.92708 |  0.91080 |   68.7      \n",
      "| 25    | 0.92900 |  0.90983 |   71.4      \n",
      "| 26    | 0.92938 |  0.91255 |   74.1      \n",
      "| 27    | 0.93026 |  0.91376 |   77.0      \n",
      "| 28    | 0.93222 |  0.91076 |   79.7      \n",
      "| 29    | 0.93230 |  0.91405 |   82.5      \n",
      "| 30    | 0.93237 |  0.91263 |   85.2      \n",
      "| 31    | 0.93236 |  0.91536 |   88.0      \n",
      "| 32    | 0.93271 |  0.91310 |   90.9      \n",
      "| 33    | 0.93380 |  0.90945 |   93.8      \n",
      "| 34    | 0.93096 |  0.90974 |   96.5      \n",
      "| 35    | 0.93312 |  0.91187 |   99.2      \n",
      "| 36    | 0.93514 |  0.91190 |   102.1     \n",
      "Early stopping occured at epoch 36\n",
      "Training done in 102.106 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.56005 |  0.53059 |   3.0       \n",
      "| 2     | 0.79072 |  0.69736 |   5.8       \n",
      "| 3     | 0.82622 |  0.79114 |   8.6       \n",
      "| 4     | 0.84213 |  0.80031 |   11.4      \n",
      "| 5     | 0.84956 |  0.82897 |   14.1      \n",
      "| 6     | 0.85455 |  0.82853 |   16.9      \n",
      "| 7     | 0.86025 |  0.83924 |   19.8      \n",
      "| 8     | 0.86246 |  0.83590 |   22.6      \n",
      "| 9     | 0.86915 |  0.84009 |   25.5      \n",
      "| 10    | 0.87354 |  0.86436 |   28.4      \n",
      "| 11    | 0.87955 |  0.87361 |   31.3      \n",
      "| 12    | 0.88469 |  0.87808 |   34.3      \n",
      "| 13    | 0.89309 |  0.88341 |   37.2      \n",
      "| 14    | 0.89311 |  0.84180 |   40.1      \n",
      "| 15    | 0.86908 |  0.86247 |   42.9      \n",
      "| 16    | 0.88198 |  0.83561 |   45.6      \n",
      "| 17    | 0.87623 |  0.82436 |   48.4      \n",
      "| 18    | 0.87986 |  0.84054 |   51.2      \n",
      "Early stopping occured at epoch 18\n",
      "Training done in 51.196 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.66632 |  0.70456 |   2.2       \n",
      "| 2     | 0.80442 |  0.75974 |   4.5       \n",
      "| 3     | 0.83138 |  0.83267 |   6.7       \n",
      "| 4     | 0.85493 |  0.84142 |   9.0       \n",
      "| 5     | 0.86284 |  0.85185 |   11.2      \n",
      "| 6     | 0.86871 |  0.85291 |   13.4      \n",
      "| 7     | 0.87798 |  0.86176 |   15.7      \n",
      "| 8     | 0.88162 |  0.86584 |   17.9      \n",
      "| 9     | 0.88375 |  0.87740 |   20.0      \n",
      "| 10    | 0.88578 |  0.87605 |   22.3      \n",
      "| 11    | 0.88932 |  0.88116 |   24.6      \n",
      "| 12    | 0.89254 |  0.88560 |   26.8      \n",
      "| 13    | 0.89524 |  0.88588 |   29.1      \n",
      "| 14    | 0.89662 |  0.89020 |   31.5      \n",
      "| 15    | 0.89770 |  0.87724 |   33.8      \n",
      "| 16    | 0.90039 |  0.89303 |   36.1      \n",
      "| 17    | 0.90483 |  0.89836 |   38.4      \n",
      "| 18    | 0.90564 |  0.89749 |   40.7      \n",
      "| 19    | 0.90673 |  0.89072 |   43.0      \n",
      "| 20    | 0.90521 |  0.89872 |   45.2      \n",
      "| 21    | 0.91125 |  0.90071 |   47.5      \n",
      "| 22    | 0.91228 |  0.89527 |   50.0      \n",
      "| 23    | 0.90973 |  0.89989 |   52.4      \n",
      "| 24    | 0.91613 |  0.82819 |   54.8      \n",
      "| 25    | 0.92082 |  0.90923 |   57.0      \n",
      "| 26    | 0.92247 |  0.91198 |   59.5      \n",
      "| 27    | 0.92511 |  0.91060 |   61.5      \n",
      "| 28    | 0.92580 |  0.91673 |   63.7      \n",
      "| 29    | 0.92889 |  0.91583 |   65.9      \n",
      "| 30    | 0.92773 |  0.91642 |   68.1      \n",
      "| 31    | 0.93053 |  0.91635 |   70.3      \n",
      "| 32    | 0.93045 |  0.91452 |   72.5      \n",
      "| 33    | 0.92939 |  0.91833 |   74.8      \n",
      "| 34    | 0.93117 |  0.91984 |   77.0      \n",
      "| 35    | 0.93184 |  0.91654 |   79.1      \n",
      "| 36    | 0.93115 |  0.92112 |   81.5      \n",
      "| 37    | 0.93504 |  0.91989 |   83.7      \n",
      "| 38    | 0.93439 |  0.91972 |   85.9      \n",
      "| 39    | 0.93580 |  0.91972 |   88.2      \n",
      "| 40    | 0.93566 |  0.92052 |   90.5      \n",
      "| 41    | 0.93623 |  0.91740 |   92.8      \n",
      "Early stopping occured at epoch 41\n",
      "Training done in 92.750 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.66571 |  0.72849 |   2.2       \n",
      "| 2     | 0.79739 |  0.76422 |   4.4       \n",
      "| 3     | 0.83446 |  0.79669 |   6.8       \n",
      "| 4     | 0.85355 |  0.82121 |   9.1       \n",
      "| 5     | 0.86110 |  0.83632 |   11.3      \n",
      "| 6     | 0.86158 |  0.84287 |   13.4      \n",
      "| 7     | 0.87213 |  0.84873 |   15.7      \n",
      "| 8     | 0.87844 |  0.84924 |   17.7      \n",
      "| 9     | 0.88383 |  0.86134 |   20.0      \n",
      "| 10    | 0.88683 |  0.86487 |   22.2      \n",
      "| 11    | 0.88590 |  0.87430 |   24.5      \n",
      "| 12    | 0.88840 |  0.87775 |   26.7      \n",
      "| 13    | 0.89031 |  0.86629 |   29.0      \n",
      "| 14    | 0.89468 |  0.87830 |   31.4      \n",
      "| 15    | 0.89834 |  0.88538 |   33.9      \n",
      "| 16    | 0.90065 |  0.88346 |   36.2      \n",
      "| 17    | 0.90182 |  0.88445 |   38.7      \n",
      "| 18    | 0.90416 |  0.88894 |   41.0      \n",
      "| 19    | 0.90302 |  0.89765 |   43.3      \n",
      "| 20    | 0.90356 |  0.90084 |   45.7      \n",
      "| 21    | 0.90906 |  0.90488 |   48.1      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 22    | 0.91317 |  0.90451 |   50.5      \n",
      "| 23    | 0.91799 |  0.90943 |   52.9      \n",
      "| 24    | 0.92266 |  0.90967 |   55.1      \n",
      "| 25    | 0.92458 |  0.91487 |   57.4      \n",
      "| 26    | 0.92579 |  0.91554 |   59.6      \n",
      "| 27    | 0.92782 |  0.91755 |   61.9      \n",
      "| 28    | 0.92969 |  0.91748 |   64.3      \n",
      "| 29    | 0.93074 |  0.91986 |   66.6      \n",
      "| 30    | 0.93316 |  0.91412 |   69.0      \n",
      "| 31    | 0.93215 |  0.92115 |   71.4      \n",
      "| 32    | 0.93171 |  0.91975 |   73.8      \n",
      "| 33    | 0.93293 |  0.92075 |   76.0      \n",
      "| 34    | 0.93154 |  0.91463 |   78.3      \n",
      "| 35    | 0.93235 |  0.91105 |   80.6      \n",
      "| 36    | 0.93126 |  0.92046 |   82.8      \n",
      "Early stopping occured at epoch 36\n",
      "Training done in 82.815 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54591 |  0.58670 |   3.2       \n",
      "| 2     | 0.70779 |  0.76077 |   6.4       \n",
      "| 3     | 0.74328 |  0.63414 |   9.6       \n",
      "| 4     | 0.67619 |  0.63613 |   12.8      \n",
      "| 5     | 0.68860 |  0.70828 |   16.0      \n",
      "| 6     | 0.71385 |  0.76561 |   19.3      \n",
      "| 7     | 0.76566 |  0.73933 |   22.4      \n",
      "| 8     | 0.78475 |  0.76688 |   25.6      \n",
      "| 9     | 0.80656 |  0.78986 |   28.9      \n",
      "| 10    | 0.82109 |  0.79227 |   32.0      \n",
      "| 11    | 0.82930 |  0.79911 |   35.2      \n",
      "| 12    | 0.83486 |  0.79892 |   38.2      \n",
      "| 13    | 0.83938 |  0.81251 |   41.1      \n",
      "| 14    | 0.84407 |  0.82987 |   44.2      \n",
      "| 15    | 0.85142 |  0.82642 |   47.3      \n",
      "| 16    | 0.85391 |  0.81638 |   50.5      \n",
      "| 17    | 0.85258 |  0.82159 |   53.5      \n",
      "| 18    | 0.85040 |  0.82004 |   56.6      \n",
      "| 19    | 0.85527 |  0.83186 |   59.8      \n",
      "| 20    | 0.85527 |  0.82629 |   62.9      \n",
      "| 21    | 0.86087 |  0.81955 |   65.9      \n",
      "| 22    | 0.86203 |  0.82257 |   68.9      \n",
      "| 23    | 0.86127 |  0.81983 |   72.0      \n",
      "| 24    | 0.85956 |  0.82733 |   75.0      \n",
      "Early stopping occured at epoch 24\n",
      "Training done in 75.002 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.55062 |  0.70814 |   3.0       \n",
      "| 2     | 0.74161 |  0.64179 |   6.0       \n",
      "| 3     | 0.66673 |  0.65440 |   9.0       \n",
      "| 4     | 0.72183 |  0.76048 |   12.1      \n",
      "| 5     | 0.78166 |  0.76049 |   15.0      \n",
      "| 6     | 0.78768 |  0.75093 |   18.1      \n",
      "| 7     | 0.79029 |  0.78057 |   21.2      \n",
      "| 8     | 0.80936 |  0.79639 |   24.2      \n",
      "| 9     | 0.81293 |  0.80917 |   27.1      \n",
      "| 10    | 0.81741 |  0.81351 |   30.1      \n",
      "| 11    | 0.82969 |  0.81228 |   33.1      \n",
      "| 12    | 0.84304 |  0.82651 |   36.2      \n",
      "| 13    | 0.84747 |  0.81089 |   39.2      \n",
      "| 14    | 0.84819 |  0.80621 |   42.3      \n",
      "| 15    | 0.85157 |  0.80922 |   45.3      \n",
      "| 16    | 0.86087 |  0.81831 |   48.3      \n",
      "| 17    | 0.86519 |  0.81781 |   51.4      \n",
      "Early stopping occured at epoch 17\n",
      "Training done in 51.391 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.70014 |  0.79651 |   4.6       \n",
      "| 2     | 0.83260 |  0.82893 |   9.2       \n",
      "| 3     | 0.85310 |  0.83276 |   14.5      \n",
      "| 4     | 0.86287 |  0.84811 |   19.2      \n",
      "| 5     | 0.87733 |  0.87146 |   23.7      \n",
      "| 6     | 0.88661 |  0.88375 |   28.2      \n",
      "| 7     | 0.89582 |  0.88170 |   32.9      \n",
      "| 8     | 0.90308 |  0.88791 |   37.5      \n",
      "| 9     | 0.90713 |  0.89320 |   42.2      \n",
      "| 10    | 0.90988 |  0.90120 |   46.7      \n",
      "| 11    | 0.91024 |  0.89570 |   51.2      \n",
      "| 12    | 0.90666 |  0.90147 |   55.8      \n",
      "| 13    | 0.90929 |  0.90544 |   60.2      \n",
      "| 14    | 0.90204 |  0.88231 |   64.8      \n",
      "| 15    | 0.88926 |  0.88286 |   69.1      \n",
      "| 16    | 0.89821 |  0.87122 |   73.6      \n",
      "| 17    | 0.90254 |  0.89599 |   78.2      \n",
      "| 18    | 0.89993 |  0.89819 |   82.9      \n",
      "Early stopping occured at epoch 18\n",
      "Training done in 82.922 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.67272 |  0.77574 |   4.4       \n",
      "| 2     | 0.81389 |  0.81816 |   9.0       \n",
      "| 3     | 0.83674 |  0.83646 |   13.3      \n",
      "| 4     | 0.85375 |  0.82889 |   17.6      \n",
      "| 5     | 0.86670 |  0.84816 |   22.1      \n",
      "| 6     | 0.87276 |  0.84869 |   26.6      \n",
      "| 7     | 0.86798 |  0.85679 |   31.1      \n",
      "| 8     | 0.87698 |  0.86300 |   35.6      \n",
      "| 9     | 0.87991 |  0.85500 |   40.1      \n",
      "| 10    | 0.86591 |  0.85079 |   44.5      \n",
      "| 11    | 0.87882 |  0.88348 |   49.1      \n",
      "| 12    | 0.88522 |  0.88126 |   53.7      \n",
      "| 13    | 0.88435 |  0.86502 |   58.1      \n",
      "| 14    | 0.88373 |  0.88799 |   62.8      \n",
      "| 15    | 0.88991 |  0.85691 |   67.2      \n",
      "| 16    | 0.89150 |  0.72670 |   71.8      \n",
      "| 17    | 0.89749 |  0.88434 |   76.4      \n",
      "| 18    | 0.90012 |  0.88727 |   80.9      \n",
      "| 19    | 0.89951 |  0.89525 |   85.5      \n",
      "| 20    | 0.89935 |  0.89113 |   90.0      \n",
      "| 21    | 0.90008 |  0.89751 |   94.3      \n",
      "| 22    | 0.90187 |  0.89410 |   98.8      \n",
      "| 23    | 0.89960 |  0.89395 |   103.1     \n",
      "| 24    | 0.89698 |  0.89228 |   107.6     \n",
      "| 25    | 0.89788 |  0.89451 |   112.1     \n",
      "| 26    | 0.89637 |  0.88864 |   116.6     \n",
      "Early stopping occured at epoch 26\n",
      "Training done in 116.561 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.61536 |  0.77097 |   3.2       \n",
      "| 2     | 0.79334 |  0.72614 |   6.4       \n",
      "| 3     | 0.83341 |  0.81161 |   10.0      \n",
      "| 4     | 0.84121 |  0.83898 |   13.4      \n",
      "| 5     | 0.86742 |  0.84734 |   16.7      \n",
      "| 6     | 0.88647 |  0.86369 |   20.0      \n",
      "| 7     | 0.88562 |  0.86445 |   23.3      \n",
      "| 8     | 0.88611 |  0.87488 |   26.7      \n",
      "| 9     | 0.89104 |  0.88226 |   30.0      \n",
      "| 10    | 0.89415 |  0.88590 |   33.3      \n",
      "| 11    | 0.90143 |  0.88475 |   36.8      \n",
      "| 12    | 0.90730 |  0.89422 |   40.0      \n",
      "| 13    | 0.91207 |  0.90312 |   43.3      \n",
      "| 14    | 0.91810 |  0.90016 |   46.7      \n",
      "| 15    | 0.91639 |  0.90685 |   50.1      \n",
      "| 16    | 0.92130 |  0.91020 |   53.4      \n",
      "| 17    | 0.92347 |  0.91265 |   56.8      \n",
      "| 18    | 0.92521 |  0.91332 |   60.2      \n",
      "| 19    | 0.92449 |  0.91091 |   63.4      \n",
      "| 20    | 0.92617 |  0.90561 |   66.5      \n",
      "| 21    | 0.92818 |  0.91416 |   69.8      \n",
      "| 22    | 0.92766 |  0.91437 |   73.1      \n",
      "| 23    | 0.92627 |  0.91409 |   76.2      \n",
      "| 24    | 0.92754 |  0.91581 |   79.6      \n",
      "| 25    | 0.93175 |  0.91695 |   82.8      \n",
      "| 26    | 0.93388 |  0.91752 |   86.1      \n",
      "| 27    | 0.93306 |  0.91752 |   89.6      \n",
      "| 28    | 0.93441 |  0.91965 |   93.1      \n",
      "| 29    | 0.93605 |  0.91861 |   96.5      \n",
      "| 30    | 0.93654 |  0.91739 |   99.7      \n",
      "| 31    | 0.93790 |  0.91581 |   103.0     \n",
      "| 32    | 0.93987 |  0.91651 |   106.1     \n",
      "| 33    | 0.94142 |  0.91570 |   109.3     \n",
      "Early stopping occured at epoch 33\n",
      "Training done in 109.317 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.66194 |  0.78345 |   3.1       \n",
      "| 2     | 0.79718 |  0.81059 |   6.5       \n",
      "| 3     | 0.84945 |  0.84056 |   9.7       \n",
      "| 4     | 0.85823 |  0.82418 |   13.0      \n",
      "| 5     | 0.87247 |  0.83065 |   16.2      \n",
      "| 6     | 0.86872 |  0.82423 |   19.5      \n",
      "| 7     | 0.87692 |  0.87636 |   22.8      \n",
      "| 8     | 0.89633 |  0.88894 |   26.0      \n",
      "| 9     | 0.89311 |  0.89222 |   29.2      \n",
      "| 10    | 0.89976 |  0.88591 |   32.5      \n",
      "| 11    | 0.90098 |  0.89183 |   35.6      \n",
      "| 12    | 0.90150 |  0.89720 |   38.8      \n",
      "| 13    | 0.91080 |  0.89993 |   42.0      \n",
      "| 14    | 0.91500 |  0.90532 |   45.1      \n",
      "| 15    | 0.91584 |  0.90379 |   48.5      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 16    | 0.92083 |  0.90998 |   51.8      \n",
      "| 17    | 0.91480 |  0.88807 |   55.2      \n",
      "| 18    | 0.90331 |  0.89491 |   58.5      \n",
      "| 19    | 0.90899 |  0.89782 |   61.8      \n",
      "| 20    | 0.90860 |  0.89929 |   65.0      \n",
      "| 21    | 0.91511 |  0.90004 |   68.3      \n",
      "Early stopping occured at epoch 21\n",
      "Training done in 68.310 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.60122 |  0.59128 |   3.6       \n",
      "| 2     | 0.78214 |  0.66724 |   7.6       \n",
      "| 3     | 0.80067 |  0.73537 |   11.7      \n",
      "| 4     | 0.78328 |  0.79192 |   16.0      \n",
      "| 5     | 0.83495 |  0.82418 |   20.0      \n",
      "| 6     | 0.85497 |  0.77943 |   23.9      \n",
      "| 7     | 0.85738 |  0.83312 |   27.9      \n",
      "| 8     | 0.86202 |  0.84496 |   31.8      \n",
      "| 9     | 0.86834 |  0.86298 |   35.6      \n",
      "| 10    | 0.87997 |  0.86759 |   39.4      \n",
      "| 11    | 0.88631 |  0.87380 |   43.4      \n",
      "| 12    | 0.88077 |  0.87480 |   47.2      \n",
      "| 13    | 0.88699 |  0.87768 |   50.9      \n",
      "| 14    | 0.88951 |  0.87370 |   54.8      \n",
      "| 15    | 0.89471 |  0.88215 |   58.5      \n",
      "| 16    | 0.89611 |  0.88288 |   62.4      \n",
      "| 17    | 0.89709 |  0.89023 |   66.2      \n",
      "| 18    | 0.89500 |  0.85019 |   70.1      \n",
      "| 19    | 0.89619 |  0.87444 |   74.1      \n",
      "| 20    | 0.89859 |  0.88663 |   78.4      \n",
      "| 21    | 0.89852 |  0.89197 |   82.5      \n",
      "| 22    | 0.89339 |  0.88619 |   86.3      \n",
      "| 23    | 0.89523 |  0.88716 |   90.4      \n",
      "| 24    | 0.89358 |  0.88849 |   94.6      \n",
      "| 25    | 0.89583 |  0.87732 |   98.7      \n",
      "| 26    | 0.89676 |  0.89452 |   102.6     \n",
      "| 27    | 0.90030 |  0.88735 |   106.8     \n",
      "| 28    | 0.90272 |  0.89906 |   111.1     \n",
      "| 29    | 0.89709 |  0.89406 |   114.9     \n",
      "| 30    | 0.90089 |  0.89633 |   118.9     \n",
      "| 31    | 0.90422 |  0.89791 |   122.8     \n",
      "| 32    | 0.90723 |  0.90519 |   126.5     \n",
      "| 33    | 0.90646 |  0.90467 |   130.5     \n",
      "| 34    | 0.90495 |  0.90364 |   134.5     \n",
      "| 35    | 0.90854 |  0.90396 |   138.4     \n",
      "| 36    | 0.90774 |  0.90494 |   142.3     \n",
      "| 37    | 0.91052 |  0.90674 |   146.0     \n",
      "| 38    | 0.91089 |  0.90479 |   150.1     \n",
      "| 39    | 0.91068 |  0.90459 |   154.2     \n",
      "| 40    | 0.91385 |  0.90674 |   158.4     \n",
      "| 41    | 0.91054 |  0.90663 |   162.7     \n",
      "| 42    | 0.91396 |  0.90588 |   166.8     \n",
      "Early stopping occured at epoch 42\n",
      "Training done in 166.849 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.63356 |  0.67849 |   3.9       \n",
      "| 2     | 0.75317 |  0.79409 |   8.1       \n",
      "| 3     | 0.82399 |  0.81736 |   12.2      \n",
      "| 4     | 0.85182 |  0.84526 |   15.9      \n",
      "| 5     | 0.85458 |  0.84443 |   20.0      \n",
      "| 6     | 0.86209 |  0.85007 |   24.0      \n",
      "| 7     | 0.86320 |  0.85419 |   28.2      \n",
      "| 8     | 0.87327 |  0.84771 |   32.1      \n",
      "| 9     | 0.87765 |  0.83675 |   35.9      \n",
      "| 10    | 0.88378 |  0.87616 |   39.6      \n",
      "| 11    | 0.88058 |  0.87914 |   43.6      \n",
      "| 12    | 0.88653 |  0.88225 |   47.8      \n",
      "| 13    | 0.89240 |  0.88668 |   52.1      \n",
      "| 14    | 0.88888 |  0.89131 |   56.1      \n",
      "| 15    | 0.89828 |  0.89396 |   59.9      \n",
      "| 16    | 0.90300 |  0.88941 |   63.7      \n",
      "| 17    | 0.90304 |  0.89934 |   67.7      \n",
      "| 18    | 0.90493 |  0.89980 |   71.7      \n",
      "| 19    | 0.90595 |  0.90282 |   75.6      \n",
      "| 20    | 0.90733 |  0.90622 |   79.4      \n",
      "| 21    | 0.90951 |  0.90391 |   83.0      \n",
      "| 22    | 0.90876 |  0.90919 |   87.0      \n",
      "| 23    | 0.90997 |  0.90735 |   90.7      \n",
      "| 24    | 0.91185 |  0.89717 |   94.7      \n",
      "| 25    | 0.91419 |  0.89921 |   98.6      \n",
      "| 26    | 0.91298 |  0.87112 |   102.6     \n",
      "| 27    | 0.91296 |  0.87839 |   106.5     \n",
      "Early stopping occured at epoch 27\n",
      "Training done in 106.498 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52695 |  0.51056 |   3.7       \n",
      "| 2     | 0.59541 |  0.58148 |   7.5       \n",
      "| 3     | 0.68032 |  0.57692 |   11.3      \n",
      "| 4     | 0.68894 |  0.48432 |   15.0      \n",
      "| 5     | 0.71176 |  0.54483 |   18.8      \n",
      "| 6     | 0.78068 |  0.57572 |   22.5      \n",
      "| 7     | 0.81119 |  0.55921 |   26.6      \n",
      "Early stopping occured at epoch 7\n",
      "Training done in 26.624 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52652 |  0.57643 |   3.8       \n",
      "| 2     | 0.60885 |  0.54331 |   7.8       \n",
      "| 3     | 0.68562 |  0.66458 |   11.8      \n",
      "| 4     | 0.72957 |  0.66074 |   15.7      \n",
      "| 5     | 0.67396 |  0.63566 |   19.6      \n",
      "| 6     | 0.69106 |  0.64612 |   23.7      \n",
      "| 7     | 0.71421 |  0.65888 |   27.7      \n",
      "| 8     | 0.77556 |  0.65238 |   31.6      \n",
      "Early stopping occured at epoch 8\n",
      "Training done in 31.576 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57120 |  0.72651 |   5.3       \n",
      "| 2     | 0.81557 |  0.81021 |   10.5      \n",
      "| 3     | 0.85766 |  0.84967 |   15.8      \n",
      "| 4     | 0.87561 |  0.86645 |   21.1      \n",
      "| 5     | 0.88324 |  0.87569 |   26.3      \n",
      "| 6     | 0.88506 |  0.88605 |   31.8      \n",
      "| 7     | 0.89302 |  0.88633 |   37.3      \n",
      "| 8     | 0.89615 |  0.88726 |   42.5      \n",
      "| 9     | 0.89482 |  0.88638 |   47.7      \n",
      "| 10    | 0.89452 |  0.88498 |   52.9      \n",
      "| 11    | 0.89734 |  0.89406 |   58.0      \n",
      "| 12    | 0.90292 |  0.89933 |   63.0      \n",
      "| 13    | 0.90544 |  0.90013 |   67.9      \n",
      "| 14    | 0.90717 |  0.90289 |   73.2      \n",
      "| 15    | 0.90863 |  0.90413 |   78.4      \n",
      "| 16    | 0.91035 |  0.90370 |   83.4      \n",
      "| 17    | 0.91285 |  0.90590 |   88.5      \n",
      "| 18    | 0.91513 |  0.90837 |   93.8      \n",
      "| 19    | 0.91775 |  0.90851 |   98.9      \n",
      "| 20    | 0.92030 |  0.90560 |   104.2     \n",
      "| 21    | 0.92134 |  0.90618 |   109.5     \n",
      "| 22    | 0.92270 |  0.90885 |   114.6     \n",
      "| 23    | 0.92447 |  0.91169 |   119.9     \n",
      "| 24    | 0.92365 |  0.90997 |   125.2     \n",
      "| 25    | 0.92488 |  0.91429 |   130.5     \n",
      "| 26    | 0.92790 |  0.91615 |   135.7     \n",
      "| 27    | 0.92866 |  0.91249 |   140.8     \n",
      "| 28    | 0.93015 |  0.91480 |   145.2     \n",
      "| 29    | 0.93078 |  0.90246 |   150.3     \n",
      "| 30    | 0.93096 |  0.91364 |   155.5     \n",
      "| 31    | 0.93382 |  0.91380 |   160.4     \n",
      "Early stopping occured at epoch 31\n",
      "Training done in 160.379 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57527 |  0.74396 |   5.1       \n",
      "| 2     | 0.80205 |  0.80456 |   10.1      \n",
      "| 3     | 0.83405 |  0.82957 |   15.1      \n",
      "| 4     | 0.82964 |  0.80436 |   20.3      \n",
      "| 5     | 0.82388 |  0.81864 |   25.4      \n",
      "| 6     | 0.84915 |  0.85477 |   30.4      \n",
      "| 7     | 0.86608 |  0.87045 |   35.5      \n",
      "| 8     | 0.87816 |  0.88057 |   40.5      \n",
      "| 9     | 0.88329 |  0.88385 |   45.3      \n",
      "| 10    | 0.88965 |  0.88347 |   50.3      \n",
      "| 11    | 0.88675 |  0.88964 |   55.6      \n",
      "| 12    | 0.88770 |  0.89267 |   60.6      \n",
      "| 13    | 0.89324 |  0.89079 |   65.6      \n",
      "| 14    | 0.89844 |  0.89755 |   70.7      \n",
      "| 15    | 0.90281 |  0.89926 |   75.8      \n",
      "| 16    | 0.90461 |  0.90293 |   80.9      \n",
      "| 17    | 0.90656 |  0.90150 |   86.0      \n",
      "| 18    | 0.90546 |  0.90510 |   91.2      \n",
      "| 19    | 0.90771 |  0.90549 |   96.5      \n",
      "| 20    | 0.91190 |  0.90594 |   101.9     \n",
      "| 21    | 0.91242 |  0.90747 |   107.1     \n",
      "| 22    | 0.91160 |  0.91110 |   112.2     \n",
      "| 23    | 0.91422 |  0.91231 |   117.4     \n",
      "| 24    | 0.91393 |  0.90909 |   122.7     \n",
      "| 25    | 0.90999 |  0.90733 |   127.7     \n",
      "| 26    | 0.91363 |  0.83655 |   132.9     \n",
      "| 27    | 0.91647 |  0.89629 |   138.0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 28    | 0.91676 |  0.91224 |   143.2     \n",
      "Early stopping occured at epoch 28\n",
      "Training done in 143.179 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.56692 |  0.60620 |   3.2       \n",
      "| 2     | 0.71073 |  0.73782 |   6.4       \n",
      "| 3     | 0.79445 |  0.79707 |   9.5       \n",
      "| 4     | 0.83450 |  0.82763 |   12.7      \n",
      "| 5     | 0.84784 |  0.82063 |   15.7      \n",
      "| 6     | 0.84665 |  0.83793 |   18.9      \n",
      "| 7     | 0.86074 |  0.84636 |   22.2      \n",
      "| 8     | 0.86970 |  0.84789 |   25.3      \n",
      "| 9     | 0.87891 |  0.86157 |   28.5      \n",
      "| 10    | 0.88024 |  0.86216 |   31.7      \n",
      "| 11    | 0.88189 |  0.86225 |   35.0      \n",
      "| 12    | 0.88364 |  0.86718 |   38.3      \n",
      "| 13    | 0.87928 |  0.86956 |   41.4      \n",
      "| 14    | 0.88188 |  0.86804 |   44.6      \n",
      "| 15    | 0.88553 |  0.87475 |   47.7      \n",
      "| 16    | 0.88811 |  0.85532 |   51.0      \n",
      "| 17    | 0.88419 |  0.87613 |   54.1      \n",
      "| 18    | 0.89178 |  0.88031 |   57.2      \n",
      "| 19    | 0.89170 |  0.88437 |   60.5      \n",
      "| 20    | 0.89621 |  0.88745 |   63.7      \n",
      "| 21    | 0.89786 |  0.88465 |   66.9      \n",
      "| 22    | 0.89927 |  0.88997 |   70.2      \n",
      "| 23    | 0.90111 |  0.89309 |   73.4      \n",
      "| 24    | 0.90282 |  0.89642 |   76.7      \n",
      "| 25    | 0.90596 |  0.89460 |   79.9      \n",
      "| 26    | 0.90387 |  0.89779 |   83.1      \n",
      "| 27    | 0.90571 |  0.87814 |   86.3      \n",
      "| 28    | 0.90556 |  0.89902 |   89.5      \n",
      "| 29    | 0.90593 |  0.89608 |   92.6      \n",
      "| 30    | 0.90559 |  0.90066 |   95.5      \n",
      "| 31    | 0.90853 |  0.90294 |   98.8      \n",
      "| 32    | 0.90961 |  0.90307 |   101.9     \n",
      "| 33    | 0.91092 |  0.90442 |   105.0     \n",
      "| 34    | 0.91118 |  0.90518 |   108.1     \n",
      "| 35    | 0.91117 |  0.90353 |   111.2     \n",
      "| 36    | 0.91235 |  0.90404 |   114.3     \n",
      "| 37    | 0.91254 |  0.90287 |   117.4     \n",
      "| 38    | 0.91077 |  0.90385 |   120.3     \n",
      "| 39    | 0.91170 |  0.90566 |   123.6     \n",
      "| 40    | 0.91340 |  0.90179 |   126.8     \n",
      "| 41    | 0.91411 |  0.90657 |   130.1     \n",
      "| 42    | 0.91475 |  0.90364 |   133.5     \n",
      "| 43    | 0.91333 |  0.90134 |   136.8     \n",
      "| 44    | 0.90901 |  0.89975 |   140.1     \n",
      "| 45    | 0.90994 |  0.90017 |   143.4     \n",
      "| 46    | 0.91024 |  0.90238 |   146.7     \n",
      "Early stopping occured at epoch 46\n",
      "Training done in 146.678 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.56322 |  0.55949 |   3.0       \n",
      "| 2     | 0.69592 |  0.67690 |   6.1       \n",
      "| 3     | 0.78581 |  0.75687 |   9.2       \n",
      "| 4     | 0.81441 |  0.75231 |   12.4      \n",
      "| 5     | 0.82926 |  0.81087 |   15.7      \n",
      "| 6     | 0.82318 |  0.83242 |   18.9      \n",
      "| 7     | 0.83584 |  0.82158 |   22.0      \n",
      "| 8     | 0.84808 |  0.79765 |   25.1      \n",
      "| 9     | 0.85236 |  0.82145 |   28.1      \n",
      "| 10    | 0.85699 |  0.84661 |   31.3      \n",
      "| 11    | 0.85814 |  0.84203 |   34.3      \n",
      "| 12    | 0.86623 |  0.84285 |   37.3      \n",
      "| 13    | 0.86538 |  0.84918 |   40.5      \n",
      "| 14    | 0.86886 |  0.85148 |   43.7      \n",
      "| 15    | 0.87255 |  0.86447 |   47.0      \n",
      "| 16    | 0.87466 |  0.86724 |   50.5      \n",
      "| 17    | 0.87071 |  0.86050 |   53.7      \n",
      "| 18    | 0.87685 |  0.86817 |   57.0      \n",
      "| 19    | 0.88169 |  0.86793 |   60.0      \n",
      "| 20    | 0.88000 |  0.85628 |   63.2      \n",
      "| 21    | 0.87833 |  0.86824 |   66.3      \n",
      "| 22    | 0.88010 |  0.87238 |   69.7      \n",
      "| 23    | 0.88398 |  0.86356 |   72.9      \n",
      "| 24    | 0.88273 |  0.86366 |   75.9      \n",
      "| 25    | 0.88518 |  0.86106 |   79.0      \n",
      "| 26    | 0.88483 |  0.86929 |   82.0      \n",
      "| 27    | 0.89064 |  0.86432 |   85.1      \n",
      "Early stopping occured at epoch 27\n",
      "Training done in 85.094 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.55195 |  0.58948 |   3.0       \n",
      "| 2     | 0.74277 |  0.73384 |   6.3       \n",
      "| 3     | 0.81554 |  0.76755 |   9.6       \n",
      "| 4     | 0.83769 |  0.78478 |   12.6      \n",
      "| 5     | 0.85007 |  0.76336 |   15.8      \n",
      "| 6     | 0.86191 |  0.81490 |   18.9      \n",
      "| 7     | 0.86397 |  0.81765 |   21.9      \n",
      "| 8     | 0.87765 |  0.83233 |   25.3      \n",
      "| 9     | 0.88375 |  0.83522 |   28.5      \n",
      "| 10    | 0.88990 |  0.85163 |   31.7      \n",
      "| 11    | 0.89601 |  0.87083 |   35.0      \n",
      "| 12    | 0.89992 |  0.88520 |   38.4      \n",
      "| 13    | 0.90496 |  0.88564 |   41.8      \n",
      "| 14    | 0.90921 |  0.89076 |   44.9      \n",
      "| 15    | 0.91322 |  0.89350 |   48.2      \n",
      "| 16    | 0.91300 |  0.89848 |   51.3      \n",
      "| 17    | 0.91731 |  0.90018 |   54.4      \n",
      "| 18    | 0.91832 |  0.90345 |   57.4      \n",
      "| 19    | 0.92054 |  0.90556 |   60.4      \n",
      "| 20    | 0.91842 |  0.90374 |   63.3      \n",
      "| 21    | 0.92061 |  0.90671 |   66.3      \n",
      "| 22    | 0.92276 |  0.90651 |   69.2      \n",
      "| 23    | 0.92334 |  0.91096 |   72.3      \n",
      "| 24    | 0.92361 |  0.90810 |   75.3      \n",
      "| 25    | 0.92569 |  0.91081 |   78.4      \n",
      "| 26    | 0.92709 |  0.91427 |   81.4      \n",
      "| 27    | 0.92705 |  0.91298 |   84.4      \n",
      "| 28    | 0.92861 |  0.91320 |   87.5      \n",
      "| 29    | 0.93007 |  0.91468 |   90.6      \n",
      "| 30    | 0.93202 |  0.91443 |   93.6      \n",
      "| 31    | 0.93403 |  0.91714 |   96.5      \n",
      "| 32    | 0.93306 |  0.91467 |   99.3      \n",
      "| 33    | 0.93473 |  0.91483 |   102.3     \n",
      "| 34    | 0.93502 |  0.91717 |   105.4     \n",
      "| 35    | 0.93623 |  0.91734 |   108.3     \n",
      "| 36    | 0.93825 |  0.91832 |   111.4     \n",
      "| 37    | 0.93657 |  0.91700 |   114.4     \n",
      "| 38    | 0.93895 |  0.91780 |   117.5     \n",
      "| 39    | 0.93758 |  0.91529 |   120.7     \n",
      "| 40    | 0.93997 |  0.91792 |   123.8     \n",
      "| 41    | 0.94133 |  0.91945 |   126.8     \n",
      "| 42    | 0.94164 |  0.92123 |   129.7     \n",
      "| 43    | 0.94166 |  0.92039 |   132.6     \n",
      "| 44    | 0.94082 |  0.91730 |   135.6     \n",
      "| 45    | 0.94138 |  0.91624 |   138.5     \n",
      "| 46    | 0.93965 |  0.91710 |   141.3     \n",
      "| 47    | 0.94118 |  0.91642 |   144.2     \n",
      "Early stopping occured at epoch 47\n",
      "Training done in 144.232 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51737 |  0.58415 |   2.9       \n",
      "| 2     | 0.72063 |  0.69760 |   5.7       \n",
      "| 3     | 0.78703 |  0.74938 |   8.7       \n",
      "| 4     | 0.81485 |  0.76584 |   11.9      \n",
      "| 5     | 0.83551 |  0.77771 |   15.1      \n",
      "| 6     | 0.84897 |  0.81491 |   18.1      \n",
      "| 7     | 0.86055 |  0.80994 |   21.1      \n",
      "| 8     | 0.86177 |  0.81103 |   24.2      \n",
      "| 9     | 0.87260 |  0.82813 |   27.3      \n",
      "| 10    | 0.88225 |  0.85755 |   30.3      \n",
      "| 11    | 0.88931 |  0.84828 |   33.2      \n",
      "| 12    | 0.88607 |  0.83464 |   36.2      \n",
      "| 13    | 0.87985 |  0.83243 |   39.3      \n",
      "| 14    | 0.88635 |  0.82723 |   42.3      \n",
      "| 15    | 0.87927 |  0.85568 |   45.2      \n",
      "Early stopping occured at epoch 15\n",
      "Training done in 45.187 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54490 |  0.57818 |   5.9       \n",
      "| 2     | 0.61665 |  0.55373 |   11.7      \n",
      "| 3     | 0.66088 |  0.64469 |   17.5      \n",
      "| 4     | 0.71786 |  0.59299 |   23.2      \n",
      "| 5     | 0.71931 |  0.72687 |   29.2      \n",
      "| 6     | 0.75310 |  0.75951 |   35.3      \n",
      "| 7     | 0.79454 |  0.79823 |   41.0      \n",
      "| 8     | 0.82896 |  0.81136 |   47.0      \n",
      "| 9     | 0.84520 |  0.84064 |   52.8      \n",
      "| 10    | 0.86354 |  0.84360 |   58.8      \n",
      "| 11    | 0.87162 |  0.84635 |   64.6      \n",
      "| 12    | 0.87944 |  0.85166 |   70.8      \n",
      "| 13    | 0.88545 |  0.85755 |   76.7      \n",
      "| 14    | 0.89019 |  0.86106 |   82.5      \n",
      "| 15    | 0.89249 |  0.87784 |   88.5      \n",
      "| 16    | 0.89541 |  0.87803 |   94.1      \n",
      "| 17    | 0.89795 |  0.88739 |   100.1     \n",
      "| 18    | 0.90033 |  0.89259 |   106.0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 19    | 0.89963 |  0.89077 |   111.8     \n",
      "| 20    | 0.90390 |  0.89362 |   117.8     \n",
      "| 21    | 0.90655 |  0.89473 |   123.8     \n",
      "| 22    | 0.90754 |  0.89701 |   129.8     \n",
      "| 23    | 0.90628 |  0.89065 |   135.7     \n",
      "| 24    | 0.90764 |  0.88641 |   141.5     \n",
      "| 25    | 0.90885 |  0.89507 |   147.5     \n",
      "| 26    | 0.90916 |  0.87931 |   153.2     \n",
      "| 27    | 0.91063 |  0.87342 |   159.0     \n",
      "Early stopping occured at epoch 27\n",
      "Training done in 159.043 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54339 |  0.58916 |   5.8       \n",
      "| 2     | 0.68805 |  0.61557 |   11.6      \n",
      "| 3     | 0.65379 |  0.60513 |   17.5      \n",
      "| 4     | 0.73456 |  0.69310 |   23.2      \n",
      "| 5     | 0.76397 |  0.77017 |   28.9      \n",
      "| 6     | 0.78009 |  0.79027 |   34.8      \n",
      "| 7     | 0.81034 |  0.79533 |   40.6      \n",
      "| 8     | 0.83373 |  0.77972 |   46.4      \n",
      "| 9     | 0.84968 |  0.80642 |   52.1      \n",
      "| 10    | 0.85284 |  0.84808 |   57.8      \n",
      "| 11    | 0.85516 |  0.78489 |   63.6      \n",
      "| 12    | 0.84982 |  0.81943 |   69.3      \n",
      "| 13    | 0.86577 |  0.83414 |   75.1      \n",
      "| 14    | 0.86049 |  0.82793 |   80.9      \n",
      "| 15    | 0.86510 |  0.84881 |   86.7      \n",
      "| 16    | 0.87137 |  0.84719 |   92.5      \n",
      "| 17    | 0.86609 |  0.86659 |   98.2      \n",
      "| 18    | 0.87066 |  0.86441 |   104.0     \n",
      "| 19    | 0.87681 |  0.86214 |   110.0     \n",
      "| 20    | 0.88319 |  0.87382 |   115.8     \n",
      "| 21    | 0.88922 |  0.88255 |   121.7     \n",
      "| 22    | 0.89142 |  0.88539 |   127.8     \n",
      "| 23    | 0.89433 |  0.88736 |   133.8     \n",
      "| 24    | 0.89637 |  0.87229 |   139.7     \n",
      "| 25    | 0.89709 |  0.88388 |   145.9     \n",
      "| 26    | 0.89878 |  0.88755 |   152.1     \n",
      "| 27    | 0.89792 |  0.89618 |   157.6     \n",
      "| 28    | 0.89704 |  0.89223 |   163.4     \n",
      "| 29    | 0.89605 |  0.89079 |   169.1     \n",
      "| 30    | 0.89857 |  0.89576 |   175.0     \n",
      "| 31    | 0.90000 |  0.89691 |   180.7     \n",
      "| 32    | 0.90217 |  0.89520 |   186.5     \n",
      "| 33    | 0.90352 |  0.89782 |   192.4     \n",
      "| 34    | 0.90166 |  0.88992 |   198.0     \n",
      "| 35    | 0.90065 |  0.89378 |   203.9     \n",
      "| 36    | 0.89782 |  0.89818 |   209.8     \n",
      "| 37    | 0.90116 |  0.90119 |   215.7     \n",
      "| 38    | 0.90470 |  0.90303 |   221.6     \n",
      "| 39    | 0.90573 |  0.90382 |   227.2     \n",
      "| 40    | 0.90786 |  0.90553 |   232.9     \n",
      "| 41    | 0.90822 |  0.90507 |   238.7     \n",
      "| 42    | 0.90825 |  0.90582 |   244.5     \n",
      "| 43    | 0.90980 |  0.90537 |   250.2     \n",
      "| 44    | 0.91077 |  0.90718 |   256.1     \n",
      "| 45    | 0.91194 |  0.90761 |   262.0     \n",
      "| 46    | 0.91231 |  0.91052 |   268.0     \n",
      "| 47    | 0.91207 |  0.90877 |   273.9     \n",
      "| 48    | 0.91301 |  0.90898 |   279.6     \n",
      "| 49    | 0.91317 |  0.90906 |   285.4     \n",
      "| 50    | 0.91535 |  0.90750 |   291.4     \n",
      "| 51    | 0.91443 |  0.90852 |   297.3     \n",
      "Early stopping occured at epoch 51\n",
      "Training done in 297.340 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51274 |  0.52251 |   3.3       \n",
      "| 2     | 0.58426 |  0.51946 |   6.5       \n",
      "| 3     | 0.63557 |  0.57400 |   9.7       \n",
      "| 4     | 0.68077 |  0.53342 |   12.8      \n",
      "| 5     | 0.69636 |  0.65046 |   16.1      \n",
      "| 6     | 0.69145 |  0.64883 |   19.3      \n",
      "| 7     | 0.68057 |  0.70286 |   22.4      \n",
      "| 8     | 0.75222 |  0.70956 |   25.6      \n",
      "| 9     | 0.78979 |  0.74666 |   28.6      \n",
      "| 10    | 0.80765 |  0.72563 |   31.6      \n",
      "| 11    | 0.81068 |  0.74311 |   34.8      \n",
      "| 12    | 0.81358 |  0.70572 |   37.9      \n",
      "| 13    | 0.81669 |  0.68980 |   41.1      \n",
      "| 14    | 0.82180 |  0.73250 |   44.2      \n",
      "Early stopping occured at epoch 14\n",
      "Training done in 44.183 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52629 |  0.51827 |   3.1       \n",
      "| 2     | 0.55902 |  0.60908 |   6.5       \n",
      "| 3     | 0.62477 |  0.62490 |   9.6       \n",
      "| 4     | 0.69382 |  0.58954 |   12.7      \n",
      "| 5     | 0.75925 |  0.67695 |   15.8      \n",
      "| 6     | 0.77238 |  0.73263 |   18.9      \n",
      "| 7     | 0.79307 |  0.72193 |   21.9      \n",
      "| 8     | 0.81136 |  0.76468 |   25.1      \n",
      "| 9     | 0.81574 |  0.77996 |   28.2      \n",
      "| 10    | 0.82829 |  0.78043 |   31.4      \n",
      "| 11    | 0.84048 |  0.76560 |   34.5      \n",
      "| 12    | 0.85222 |  0.79815 |   37.8      \n",
      "| 13    | 0.86214 |  0.80654 |   41.1      \n",
      "| 14    | 0.86186 |  0.79329 |   44.5      \n",
      "| 15    | 0.85830 |  0.80030 |   47.8      \n",
      "| 16    | 0.86364 |  0.78765 |   51.3      \n",
      "| 17    | 0.85734 |  0.78341 |   54.8      \n",
      "| 18    | 0.85101 |  0.80846 |   58.2      \n",
      "| 19    | 0.84724 |  0.79838 |   61.5      \n",
      "| 20    | 0.84617 |  0.81224 |   64.7      \n",
      "| 21    | 0.86874 |  0.81723 |   68.1      \n",
      "| 22    | 0.87048 |  0.79222 |   71.3      \n",
      "| 23    | 0.87565 |  0.81621 |   74.4      \n",
      "| 24    | 0.87979 |  0.81330 |   77.6      \n",
      "| 25    | 0.88037 |  0.80294 |   80.8      \n",
      "| 26    | 0.88061 |  0.81516 |   83.9      \n",
      "Early stopping occured at epoch 26\n",
      "Training done in 83.949 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.60551 |  0.71481 |   3.8       \n",
      "| 2     | 0.80512 |  0.75551 |   7.8       \n",
      "| 3     | 0.82775 |  0.75265 |   11.8      \n",
      "| 4     | 0.83037 |  0.73782 |   15.7      \n",
      "| 5     | 0.84595 |  0.77959 |   19.5      \n",
      "| 6     | 0.85667 |  0.77509 |   23.2      \n",
      "| 7     | 0.85840 |  0.81449 |   27.2      \n",
      "| 8     | 0.86110 |  0.80938 |   31.1      \n",
      "| 9     | 0.86231 |  0.82409 |   35.1      \n",
      "| 10    | 0.87854 |  0.84009 |   39.0      \n",
      "| 11    | 0.88148 |  0.85592 |   42.8      \n",
      "| 12    | 0.88149 |  0.86443 |   46.6      \n",
      "| 13    | 0.88694 |  0.86460 |   50.3      \n",
      "| 14    | 0.88775 |  0.86876 |   54.2      \n",
      "| 15    | 0.89111 |  0.86845 |   58.0      \n",
      "| 16    | 0.89226 |  0.87441 |   61.8      \n",
      "| 17    | 0.88805 |  0.87804 |   65.6      \n",
      "| 18    | 0.89460 |  0.87847 |   69.3      \n",
      "| 19    | 0.89626 |  0.88020 |   73.0      \n",
      "| 20    | 0.89958 |  0.87903 |   76.9      \n",
      "| 21    | 0.89792 |  0.87565 |   80.7      \n",
      "| 22    | 0.90000 |  0.88462 |   84.7      \n",
      "| 23    | 0.90168 |  0.87510 |   88.4      \n",
      "| 24    | 0.89947 |  0.88502 |   92.3      \n",
      "| 25    | 0.90151 |  0.88832 |   96.2      \n",
      "| 26    | 0.89706 |  0.88945 |   100.2     \n",
      "| 27    | 0.89948 |  0.89120 |   104.0     \n",
      "| 28    | 0.89984 |  0.89262 |   108.1     \n",
      "| 29    | 0.90101 |  0.89319 |   112.4     \n",
      "| 30    | 0.89652 |  0.88934 |   116.4     \n",
      "| 31    | 0.89840 |  0.89169 |   120.3     \n",
      "| 32    | 0.90000 |  0.89249 |   124.1     \n",
      "| 33    | 0.89947 |  0.89357 |   127.7     \n",
      "| 34    | 0.90320 |  0.89445 |   131.4     \n",
      "| 35    | 0.90325 |  0.89264 |   135.1     \n",
      "| 36    | 0.90294 |  0.89510 |   138.6     \n",
      "| 37    | 0.90345 |  0.89476 |   142.3     \n",
      "| 38    | 0.90322 |  0.89596 |   146.2     \n",
      "| 39    | 0.90397 |  0.89401 |   149.9     \n",
      "| 40    | 0.90507 |  0.89811 |   153.8     \n",
      "| 41    | 0.90554 |  0.89093 |   157.5     \n",
      "| 42    | 0.90426 |  0.89632 |   161.2     \n",
      "| 43    | 0.90617 |  0.89661 |   165.0     \n",
      "| 44    | 0.90577 |  0.89698 |   168.9     \n",
      "| 45    | 0.90437 |  0.89774 |   172.7     \n",
      "Early stopping occured at epoch 45\n",
      "Training done in 172.705 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.63028 |  0.69708 |   3.9       \n",
      "| 2     | 0.79476 |  0.74081 |   7.7       \n",
      "| 3     | 0.79141 |  0.77989 |   11.6      \n",
      "| 4     | 0.79982 |  0.80366 |   15.4      \n",
      "| 5     | 0.81211 |  0.80016 |   19.3      \n",
      "| 6     | 0.81499 |  0.80657 |   23.3      \n",
      "| 7     | 0.82934 |  0.82809 |   27.0      \n",
      "| 8     | 0.84760 |  0.83940 |   30.8      \n",
      "| 9     | 0.85395 |  0.84435 |   34.8      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 10    | 0.85296 |  0.84546 |   38.7      \n",
      "| 11    | 0.86028 |  0.84282 |   42.7      \n",
      "| 12    | 0.85657 |  0.85433 |   46.6      \n",
      "| 13    | 0.86051 |  0.85772 |   50.4      \n",
      "| 14    | 0.86641 |  0.84886 |   54.5      \n",
      "| 15    | 0.86574 |  0.86093 |   58.4      \n",
      "| 16    | 0.87007 |  0.86612 |   62.4      \n",
      "| 17    | 0.87518 |  0.86872 |   66.3      \n",
      "| 18    | 0.88151 |  0.86617 |   70.2      \n",
      "| 19    | 0.88342 |  0.87240 |   74.1      \n",
      "| 20    | 0.88562 |  0.86991 |   77.7      \n",
      "| 21    | 0.88607 |  0.87788 |   81.5      \n",
      "| 22    | 0.88919 |  0.88011 |   85.5      \n",
      "| 23    | 0.89095 |  0.86514 |   89.3      \n",
      "| 24    | 0.88185 |  0.88102 |   93.1      \n",
      "| 25    | 0.89198 |  0.88181 |   96.9      \n",
      "| 26    | 0.88597 |  0.86861 |   100.8     \n",
      "| 27    | 0.88570 |  0.87293 |   104.7     \n",
      "| 28    | 0.88550 |  0.87280 |   108.5     \n",
      "| 29    | 0.88652 |  0.87873 |   112.4     \n",
      "| 30    | 0.89219 |  0.88419 |   116.1     \n",
      "| 31    | 0.89346 |  0.88435 |   120.2     \n",
      "| 32    | 0.89660 |  0.88787 |   124.1     \n",
      "| 33    | 0.89969 |  0.89053 |   127.9     \n",
      "| 34    | 0.89869 |  0.88945 |   131.7     \n",
      "| 35    | 0.90112 |  0.89332 |   135.5     \n",
      "| 36    | 0.90204 |  0.89294 |   139.3     \n",
      "| 37    | 0.90232 |  0.89138 |   143.0     \n",
      "| 38    | 0.90320 |  0.89242 |   146.7     \n",
      "| 39    | 0.90183 |  0.89324 |   150.5     \n",
      "| 40    | 0.90103 |  0.89049 |   154.2     \n",
      "Early stopping occured at epoch 40\n",
      "Training done in 154.250 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.53687 |  0.50756 |   1.9       \n",
      "| 2     | 0.72031 |  0.52198 |   3.9       \n",
      "| 3     | 0.76470 |  0.69210 |   5.7       \n",
      "| 4     | 0.78434 |  0.58926 |   7.7       \n",
      "| 5     | 0.80349 |  0.73455 |   9.7       \n",
      "| 6     | 0.81890 |  0.73871 |   11.7      \n",
      "| 7     | 0.82355 |  0.76658 |   13.7      \n",
      "| 8     | 0.83071 |  0.77494 |   15.6      \n",
      "| 9     | 0.83779 |  0.79530 |   17.6      \n",
      "| 10    | 0.84276 |  0.79101 |   19.5      \n",
      "| 11    | 0.84703 |  0.78105 |   21.4      \n",
      "| 12    | 0.85111 |  0.78892 |   23.3      \n",
      "| 13    | 0.85603 |  0.78931 |   25.3      \n",
      "| 14    | 0.85854 |  0.80512 |   27.1      \n",
      "| 15    | 0.85976 |  0.80398 |   29.1      \n",
      "| 16    | 0.86384 |  0.80026 |   30.9      \n",
      "| 17    | 0.86326 |  0.80676 |   32.8      \n",
      "| 18    | 0.86091 |  0.80775 |   34.7      \n",
      "| 19    | 0.86036 |  0.80586 |   36.5      \n",
      "| 20    | 0.86481 |  0.81921 |   38.3      \n",
      "| 21    | 0.86516 |  0.81803 |   40.3      \n",
      "| 22    | 0.86826 |  0.79802 |   42.3      \n",
      "| 23    | 0.86481 |  0.77288 |   44.1      \n",
      "| 24    | 0.86141 |  0.79384 |   46.1      \n",
      "| 25    | 0.86604 |  0.78349 |   48.1      \n",
      "Early stopping occured at epoch 25\n",
      "Training done in 48.065 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54549 |  0.59489 |   2.0       \n",
      "| 2     | 0.73737 |  0.66990 |   4.0       \n",
      "| 3     | 0.78363 |  0.72216 |   6.1       \n",
      "| 4     | 0.79818 |  0.77854 |   8.1       \n",
      "| 5     | 0.80493 |  0.76629 |   10.1      \n",
      "| 6     | 0.81389 |  0.79979 |   12.1      \n",
      "| 7     | 0.81495 |  0.81169 |   14.2      \n",
      "| 8     | 0.81718 |  0.80753 |   16.2      \n",
      "| 9     | 0.81771 |  0.79405 |   18.2      \n",
      "| 10    | 0.82298 |  0.81105 |   20.3      \n",
      "| 11    | 0.82678 |  0.81142 |   22.3      \n",
      "| 12    | 0.82805 |  0.81918 |   24.4      \n",
      "| 13    | 0.82999 |  0.81687 |   26.4      \n",
      "| 14    | 0.83218 |  0.81450 |   28.4      \n",
      "| 15    | 0.83268 |  0.82197 |   30.4      \n",
      "| 16    | 0.84027 |  0.82745 |   32.6      \n",
      "| 17    | 0.84022 |  0.83122 |   34.7      \n",
      "| 18    | 0.84067 |  0.83244 |   36.9      \n",
      "| 19    | 0.84381 |  0.83273 |   38.9      \n",
      "| 20    | 0.84326 |  0.83633 |   41.0      \n",
      "| 21    | 0.84721 |  0.82825 |   43.0      \n",
      "| 22    | 0.84598 |  0.82716 |   45.2      \n",
      "| 23    | 0.84720 |  0.83896 |   47.4      \n",
      "| 24    | 0.85649 |  0.84353 |   49.5      \n",
      "| 25    | 0.86210 |  0.84583 |   51.6      \n",
      "| 26    | 0.86522 |  0.85189 |   53.7      \n",
      "| 27    | 0.86852 |  0.84985 |   55.8      \n",
      "| 28    | 0.86906 |  0.86348 |   57.8      \n",
      "| 29    | 0.86741 |  0.85159 |   59.9      \n",
      "| 30    | 0.87173 |  0.84372 |   62.0      \n",
      "| 31    | 0.87704 |  0.85883 |   64.1      \n",
      "| 32    | 0.88065 |  0.85205 |   66.3      \n",
      "| 33    | 0.88415 |  0.82031 |   68.3      \n",
      "Early stopping occured at epoch 33\n",
      "Training done in 68.314 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.55546 |  0.47142 |   6.1       \n",
      "| 2     | 0.73107 |  0.71030 |   11.9      \n",
      "| 3     | 0.81778 |  0.70688 |   17.6      \n",
      "| 4     | 0.83917 |  0.72332 |   23.5      \n",
      "| 5     | 0.84565 |  0.81116 |   29.3      \n",
      "| 6     | 0.85267 |  0.74511 |   35.1      \n",
      "| 7     | 0.85816 |  0.83475 |   41.0      \n",
      "| 8     | 0.86201 |  0.50110 |   46.8      \n",
      "| 9     | 0.86197 |  0.80338 |   52.5      \n",
      "| 10    | 0.87733 |  0.86561 |   58.5      \n",
      "| 11    | 0.88432 |  0.83540 |   64.3      \n",
      "| 12    | 0.88558 |  0.85675 |   70.0      \n",
      "| 13    | 0.88605 |  0.86307 |   75.8      \n",
      "| 14    | 0.88982 |  0.88777 |   81.8      \n",
      "| 15    | 0.89579 |  0.88876 |   87.8      \n",
      "| 16    | 0.89495 |  0.87641 |   93.6      \n",
      "| 17    | 0.88681 |  0.84458 |   99.4      \n",
      "| 18    | 0.87436 |  0.85289 |   105.5     \n",
      "| 19    | 0.87644 |  0.86707 |   111.4     \n",
      "| 20    | 0.88930 |  0.88313 |   117.1     \n",
      "Early stopping occured at epoch 20\n",
      "Training done in 117.125 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57250 |  0.62232 |   6.0       \n",
      "| 2     | 0.74370 |  0.68145 |   11.9      \n",
      "| 3     | 0.78977 |  0.59843 |   17.8      \n",
      "| 4     | 0.81686 |  0.78747 |   23.5      \n",
      "| 5     | 0.85082 |  0.80527 |   29.5      \n",
      "| 6     | 0.86042 |  0.83636 |   35.0      \n",
      "| 7     | 0.85988 |  0.84230 |   40.7      \n",
      "| 8     | 0.86906 |  0.86331 |   46.6      \n",
      "| 9     | 0.87280 |  0.80303 |   52.5      \n",
      "| 10    | 0.87508 |  0.85096 |   58.2      \n",
      "| 11    | 0.89212 |  0.88950 |   64.1      \n",
      "| 12    | 0.90107 |  0.88837 |   69.9      \n",
      "| 13    | 0.90169 |  0.88478 |   75.8      \n",
      "| 14    | 0.90224 |  0.90106 |   81.6      \n",
      "| 15    | 0.89837 |  0.86163 |   87.3      \n",
      "| 16    | 0.89517 |  0.88418 |   93.4      \n",
      "| 17    | 0.89685 |  0.89356 |   99.4      \n",
      "| 18    | 0.89976 |  0.88743 |   105.3     \n",
      "| 19    | 0.90430 |  0.89880 |   111.0     \n",
      "Early stopping occured at epoch 19\n",
      "Training done in 111.021 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51398 |  0.55459 |   4.3       \n",
      "| 2     | 0.56592 |  0.62377 |   8.6       \n",
      "| 3     | 0.58851 |  0.64993 |   13.0      \n",
      "| 4     | 0.68861 |  0.57023 |   17.3      \n",
      "| 5     | 0.63810 |  0.60901 |   21.7      \n",
      "| 6     | 0.60177 |  0.68192 |   26.0      \n",
      "| 7     | 0.64095 |  0.70259 |   30.2      \n",
      "| 8     | 0.70044 |  0.72042 |   34.3      \n",
      "| 9     | 0.65603 |  0.66470 |   38.8      \n",
      "| 10    | 0.73300 |  0.56128 |   43.0      \n",
      "| 11    | 0.64633 |  0.56614 |   47.2      \n",
      "| 12    | 0.63416 |  0.66776 |   51.3      \n",
      "| 13    | 0.66078 |  0.63330 |   55.6      \n",
      "Early stopping occured at epoch 13\n",
      "Training done in 55.619 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.50625 |  0.54813 |   4.3       \n",
      "| 2     | 0.56723 |  0.52959 |   8.9       \n",
      "| 3     | 0.58198 |  0.60568 |   13.4      \n",
      "| 4     | 0.61507 |  0.57797 |   17.9      \n",
      "| 5     | 0.63515 |  0.61407 |   22.2      \n",
      "| 6     | 0.63069 |  0.60913 |   26.7      \n",
      "| 7     | 0.56736 |  0.62458 |   31.0      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 8     | 0.63079 |  0.64051 |   35.5      \n",
      "| 9     | 0.66136 |  0.66726 |   39.9      \n",
      "| 10    | 0.65647 |  0.66658 |   44.1      \n",
      "| 11    | 0.64163 |  0.65188 |   48.3      \n",
      "| 12    | 0.67677 |  0.67846 |   52.3      \n",
      "| 13    | 0.65338 |  0.64543 |   56.4      \n",
      "| 14    | 0.64638 |  0.66143 |   60.5      \n",
      "| 15    | 0.69343 |  0.74892 |   64.6      \n",
      "| 16    | 0.72300 |  0.76025 |   68.7      \n",
      "| 17    | 0.74457 |  0.78100 |   72.8      \n",
      "| 18    | 0.76258 |  0.74320 |   76.9      \n",
      "| 19    | 0.77028 |  0.75204 |   81.1      \n",
      "| 20    | 0.78034 |  0.80517 |   85.4      \n",
      "| 21    | 0.79262 |  0.82132 |   89.5      \n",
      "| 22    | 0.80268 |  0.81184 |   93.7      \n",
      "| 23    | 0.81585 |  0.81656 |   97.9      \n",
      "| 24    | 0.83560 |  0.82385 |   102.2     \n",
      "| 25    | 0.84931 |  0.83086 |   106.7     \n",
      "| 26    | 0.85675 |  0.82948 |   110.9     \n",
      "| 27    | 0.85942 |  0.85104 |   115.1     \n",
      "| 28    | 0.86781 |  0.84515 |   119.3     \n",
      "| 29    | 0.86885 |  0.85247 |   123.5     \n",
      "| 30    | 0.87252 |  0.85527 |   127.8     \n",
      "| 31    | 0.87537 |  0.85991 |   132.1     \n",
      "| 32    | 0.88311 |  0.86435 |   136.4     \n",
      "| 33    | 0.89005 |  0.87336 |   140.5     \n",
      "| 34    | 0.88873 |  0.85921 |   144.5     \n",
      "| 35    | 0.85804 |  0.86415 |   148.6     \n",
      "| 36    | 0.86686 |  0.86107 |   152.5     \n",
      "| 37    | 0.86942 |  0.86485 |   156.6     \n",
      "| 38    | 0.86729 |  0.84481 |   161.0     \n",
      "Early stopping occured at epoch 38\n",
      "Training done in 160.999 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51785 |  0.61775 |   3.8       \n",
      "| 2     | 0.72834 |  0.75096 |   7.7       \n",
      "| 3     | 0.77063 |  0.73408 |   11.9      \n",
      "| 4     | 0.81447 |  0.79175 |   15.7      \n",
      "| 5     | 0.82164 |  0.77843 |   19.6      \n",
      "| 6     | 0.81256 |  0.79712 |   23.7      \n",
      "| 7     | 0.82534 |  0.82148 |   27.4      \n",
      "| 8     | 0.83201 |  0.81354 |   31.0      \n",
      "| 9     | 0.83316 |  0.82663 |   34.7      \n",
      "| 10    | 0.84170 |  0.82494 |   38.3      \n",
      "| 11    | 0.85548 |  0.84140 |   42.3      \n",
      "| 12    | 0.85884 |  0.85825 |   46.0      \n",
      "| 13    | 0.87428 |  0.85995 |   49.8      \n",
      "| 14    | 0.87985 |  0.86475 |   53.7      \n",
      "| 15    | 0.88556 |  0.87439 |   57.5      \n",
      "| 16    | 0.88992 |  0.86761 |   61.1      \n",
      "| 17    | 0.89231 |  0.88002 |   64.9      \n",
      "| 18    | 0.89552 |  0.88659 |   68.7      \n",
      "| 19    | 0.89717 |  0.88991 |   72.4      \n",
      "| 20    | 0.89814 |  0.88634 |   76.1      \n",
      "| 21    | 0.89887 |  0.88159 |   79.8      \n",
      "| 22    | 0.89581 |  0.87268 |   83.5      \n",
      "| 23    | 0.89791 |  0.88553 |   87.2      \n",
      "| 24    | 0.89744 |  0.87618 |   91.2      \n",
      "Early stopping occured at epoch 24\n",
      "Training done in 91.176 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.53724 |  0.61635 |   3.8       \n",
      "| 2     | 0.73506 |  0.76005 |   7.4       \n",
      "| 3     | 0.80905 |  0.75333 |   11.3      \n",
      "| 4     | 0.83083 |  0.80637 |   15.3      \n",
      "| 5     | 0.84034 |  0.81932 |   19.3      \n",
      "| 6     | 0.84738 |  0.79883 |   23.1      \n",
      "| 7     | 0.85812 |  0.77079 |   26.9      \n",
      "| 8     | 0.86429 |  0.84988 |   30.5      \n",
      "| 9     | 0.87154 |  0.85100 |   34.4      \n",
      "| 10    | 0.87212 |  0.83698 |   38.2      \n",
      "| 11    | 0.86952 |  0.85668 |   42.0      \n",
      "| 12    | 0.86979 |  0.83254 |   45.7      \n",
      "| 13    | 0.87709 |  0.82833 |   49.5      \n",
      "| 14    | 0.88482 |  0.86816 |   53.4      \n",
      "| 15    | 0.88850 |  0.85511 |   57.2      \n",
      "| 16    | 0.88326 |  0.87251 |   60.9      \n",
      "| 17    | 0.88152 |  0.88339 |   65.0      \n",
      "| 18    | 0.88464 |  0.84875 |   68.9      \n",
      "| 19    | 0.88605 |  0.87061 |   72.6      \n",
      "| 20    | 0.88848 |  0.85513 |   76.5      \n",
      "| 21    | 0.89086 |  0.85802 |   80.4      \n",
      "| 22    | 0.89149 |  0.85553 |   84.0      \n",
      "Early stopping occured at epoch 22\n",
      "Training done in 84.001 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57143 |  0.56018 |   3.1       \n",
      "| 2     | 0.73820 |  0.64037 |   6.6       \n",
      "| 3     | 0.78318 |  0.71451 |   10.1      \n",
      "| 4     | 0.78823 |  0.66010 |   13.2      \n",
      "| 5     | 0.80198 |  0.61083 |   16.4      \n",
      "| 6     | 0.80922 |  0.75941 |   19.8      \n",
      "| 7     | 0.82528 |  0.80867 |   23.1      \n",
      "| 8     | 0.83608 |  0.80953 |   26.6      \n",
      "| 9     | 0.84938 |  0.82952 |   30.0      \n",
      "| 10    | 0.86647 |  0.85134 |   33.3      \n",
      "| 11    | 0.87294 |  0.86142 |   36.5      \n",
      "| 12    | 0.87984 |  0.86851 |   39.5      \n",
      "| 13    | 0.88415 |  0.87735 |   42.4      \n",
      "| 14    | 0.88697 |  0.87793 |   45.1      \n",
      "| 15    | 0.89040 |  0.88156 |   48.1      \n",
      "| 16    | 0.89438 |  0.88996 |   51.0      \n",
      "| 17    | 0.89889 |  0.89016 |   53.9      \n",
      "| 18    | 0.89957 |  0.89544 |   56.8      \n",
      "| 19    | 0.90420 |  0.90026 |   59.8      \n",
      "| 20    | 0.90794 |  0.89900 |   62.7      \n",
      "| 21    | 0.91124 |  0.89884 |   65.8      \n",
      "| 22    | 0.91416 |  0.90309 |   69.0      \n",
      "| 23    | 0.91647 |  0.89724 |   72.1      \n",
      "| 24    | 0.91988 |  0.90740 |   75.2      \n",
      "| 25    | 0.92068 |  0.91128 |   78.3      \n",
      "| 26    | 0.91938 |  0.91618 |   81.2      \n",
      "| 27    | 0.92289 |  0.91705 |   84.1      \n",
      "| 28    | 0.92675 |  0.91538 |   87.1      \n",
      "| 29    | 0.92513 |  0.79775 |   90.1      \n",
      "| 30    | 0.92466 |  0.78706 |   93.0      \n",
      "| 31    | 0.92790 |  0.90334 |   96.0      \n",
      "| 32    | 0.92995 |  0.90825 |   98.8      \n",
      "Early stopping occured at epoch 32\n",
      "Training done in 98.840 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.55712 |  0.55157 |   3.0       \n",
      "| 2     | 0.66105 |  0.62139 |   5.9       \n",
      "| 3     | 0.76332 |  0.68307 |   8.9       \n",
      "| 4     | 0.78038 |  0.75912 |   12.2      \n",
      "| 5     | 0.81173 |  0.80305 |   15.6      \n",
      "| 6     | 0.83491 |  0.83402 |   18.6      \n",
      "| 7     | 0.85651 |  0.83161 |   21.4      \n",
      "| 8     | 0.86240 |  0.82724 |   24.5      \n",
      "| 9     | 0.87603 |  0.86780 |   27.6      \n",
      "| 10    | 0.88282 |  0.84856 |   30.6      \n",
      "| 11    | 0.86171 |  0.80388 |   33.6      \n",
      "| 12    | 0.87082 |  0.83235 |   36.7      \n",
      "| 13    | 0.86854 |  0.85943 |   39.7      \n",
      "| 14    | 0.87197 |  0.70024 |   42.7      \n",
      "Early stopping occured at epoch 14\n",
      "Training done in 42.664 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.56553 |  0.70204 |   7.3       \n",
      "| 2     | 0.73917 |  0.78879 |   14.8      \n",
      "| 3     | 0.81349 |  0.82172 |   22.0      \n",
      "| 4     | 0.85889 |  0.82295 |   29.4      \n",
      "| 5     | 0.87527 |  0.77084 |   36.5      \n",
      "| 6     | 0.86740 |  0.77207 |   43.7      \n",
      "| 7     | 0.85379 |  0.71192 |   50.6      \n",
      "| 8     | 0.86755 |  0.86654 |   57.8      \n",
      "| 9     | 0.87919 |  0.85729 |   64.8      \n",
      "| 10    | 0.88604 |  0.86994 |   72.0      \n",
      "| 11    | 0.88642 |  0.88466 |   79.0      \n",
      "| 12    | 0.88728 |  0.86747 |   86.4      \n",
      "| 13    | 0.89372 |  0.88049 |   93.8      \n",
      "| 14    | 0.89823 |  0.83372 |   101.3     \n",
      "| 15    | 0.89935 |  0.73509 |   108.5     \n",
      "| 16    | 0.89826 |  0.76638 |   115.8     \n",
      "Early stopping occured at epoch 16\n",
      "Training done in 115.841 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57599 |  0.65359 |   7.6       \n",
      "| 2     | 0.70103 |  0.76444 |   15.1      \n",
      "| 3     | 0.78919 |  0.75568 |   22.7      \n",
      "| 4     | 0.80161 |  0.73147 |   29.9      \n",
      "| 5     | 0.84496 |  0.81747 |   37.0      \n",
      "| 6     | 0.85260 |  0.82698 |   44.6      \n",
      "| 7     | 0.85527 |  0.80674 |   52.2      \n",
      "| 8     | 0.85233 |  0.81150 |   60.0      \n",
      "| 9     | 0.85908 |  0.83543 |   67.2      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 10    | 0.87015 |  0.87175 |   74.8      \n",
      "| 11    | 0.87698 |  0.87819 |   82.4      \n",
      "| 12    | 0.88541 |  0.87844 |   89.6      \n",
      "| 13    | 0.88871 |  0.88754 |   96.8      \n",
      "| 14    | 0.89384 |  0.82006 |   104.3     \n",
      "| 15    | 0.88755 |  0.73966 |   111.8     \n",
      "| 16    | 0.89129 |  0.84487 |   119.3     \n",
      "| 17    | 0.89516 |  0.77504 |   126.6     \n",
      "| 18    | 0.89161 |  0.88225 |   133.9     \n",
      "Early stopping occured at epoch 18\n",
      "Training done in 133.916 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.52284 |  0.47709 |   4.0       \n",
      "| 2     | 0.58581 |  0.53985 |   8.1       \n",
      "| 3     | 0.71934 |  0.64273 |   12.0      \n",
      "| 4     | 0.79978 |  0.74624 |   15.9      \n",
      "| 5     | 0.82849 |  0.80325 |   19.8      \n",
      "| 6     | 0.83785 |  0.76095 |   23.8      \n",
      "| 7     | 0.82180 |  0.81821 |   27.8      \n",
      "| 8     | 0.85064 |  0.83769 |   31.8      \n",
      "| 9     | 0.86330 |  0.73016 |   35.8      \n",
      "| 10    | 0.86634 |  0.85425 |   39.8      \n",
      "| 11    | 0.88550 |  0.86606 |   43.8      \n",
      "| 12    | 0.89256 |  0.85925 |   47.6      \n",
      "| 13    | 0.88990 |  0.86673 |   51.5      \n",
      "| 14    | 0.88038 |  0.87027 |   55.5      \n",
      "| 15    | 0.88821 |  0.87678 |   59.4      \n",
      "| 16    | 0.89026 |  0.87618 |   63.3      \n",
      "| 17    | 0.89200 |  0.84661 |   67.5      \n",
      "| 18    | 0.89293 |  0.86865 |   71.5      \n",
      "| 19    | 0.89286 |  0.88155 |   75.6      \n",
      "| 20    | 0.89809 |  0.89080 |   79.4      \n",
      "| 21    | 0.89870 |  0.88893 |   83.3      \n",
      "| 22    | 0.89463 |  0.87994 |   87.2      \n",
      "| 23    | 0.88912 |  0.88107 |   91.1      \n",
      "| 24    | 0.89091 |  0.85789 |   95.0      \n",
      "| 25    | 0.88786 |  0.86904 |   98.8      \n",
      "Early stopping occured at epoch 25\n",
      "Training done in 98.816 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51117 |  0.54223 |   3.8       \n",
      "| 2     | 0.57982 |  0.52076 |   7.7       \n",
      "| 3     | 0.70384 |  0.56780 |   11.6      \n",
      "| 4     | 0.78441 |  0.73362 |   15.5      \n",
      "| 5     | 0.81970 |  0.73398 |   19.4      \n",
      "| 6     | 0.83974 |  0.82155 |   23.2      \n",
      "| 7     | 0.84878 |  0.82602 |   26.9      \n",
      "| 8     | 0.84940 |  0.80935 |   30.8      \n",
      "| 9     | 0.87048 |  0.81968 |   34.9      \n",
      "| 10    | 0.86109 |  0.79770 |   38.7      \n",
      "| 11    | 0.87010 |  0.85857 |   42.7      \n",
      "| 12    | 0.88267 |  0.86021 |   46.8      \n",
      "| 13    | 0.89318 |  0.86854 |   50.8      \n",
      "| 14    | 0.89238 |  0.87775 |   55.1      \n",
      "| 15    | 0.88297 |  0.80745 |   58.8      \n",
      "| 16    | 0.88331 |  0.87360 |   62.9      \n",
      "| 17    | 0.88673 |  0.87810 |   67.0      \n",
      "| 18    | 0.88909 |  0.86718 |   71.0      \n",
      "| 19    | 0.89012 |  0.88885 |   74.9      \n",
      "| 20    | 0.89273 |  0.87781 |   78.5      \n",
      "| 21    | 0.89758 |  0.88181 |   82.3      \n",
      "| 22    | 0.90549 |  0.86786 |   86.3      \n",
      "| 23    | 0.90756 |  0.81375 |   90.1      \n",
      "| 24    | 0.90388 |  0.81934 |   93.9      \n",
      "Early stopping occured at epoch 24\n",
      "Training done in 93.883 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.53153 |  0.52056 |   4.5       \n",
      "| 2     | 0.76195 |  0.70331 |   8.8       \n",
      "| 3     | 0.81781 |  0.79426 |   13.6      \n",
      "| 4     | 0.83982 |  0.80655 |   18.1      \n",
      "| 5     | 0.85703 |  0.66194 |   22.9      \n",
      "| 6     | 0.86277 |  0.85091 |   27.6      \n",
      "| 7     | 0.87808 |  0.82463 |   32.3      \n",
      "| 8     | 0.87625 |  0.78982 |   36.9      \n",
      "| 9     | 0.88943 |  0.87962 |   41.6      \n",
      "| 10    | 0.89399 |  0.88771 |   46.7      \n",
      "| 11    | 0.90099 |  0.88454 |   51.7      \n",
      "| 12    | 0.90187 |  0.88914 |   56.7      \n",
      "| 13    | 0.90334 |  0.88911 |   61.5      \n",
      "| 14    | 0.90486 |  0.89739 |   66.6      \n",
      "| 15    | 0.90635 |  0.89469 |   71.5      \n",
      "| 16    | 0.89741 |  0.87359 |   76.6      \n",
      "| 17    | 0.89226 |  0.88778 |   81.6      \n",
      "| 18    | 0.89355 |  0.88792 |   86.6      \n",
      "| 19    | 0.89486 |  0.88288 |   91.9      \n",
      "Early stopping occured at epoch 19\n",
      "Training done in 91.888 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.53053 |  0.56049 |   5.0       \n",
      "| 2     | 0.76078 |  0.53897 |   9.9       \n",
      "| 3     | 0.83593 |  0.78601 |   14.9      \n",
      "| 4     | 0.86091 |  0.80734 |   19.8      \n",
      "| 5     | 0.88427 |  0.87021 |   24.8      \n",
      "| 6     | 0.89901 |  0.88381 |   29.8      \n",
      "| 7     | 0.90069 |  0.89403 |   34.9      \n",
      "| 8     | 0.89972 |  0.89586 |   39.7      \n",
      "| 9     | 0.90498 |  0.89810 |   44.7      \n",
      "| 10    | 0.90488 |  0.90234 |   49.7      \n",
      "| 11    | 0.90513 |  0.90038 |   54.6      \n",
      "| 12    | 0.90760 |  0.89302 |   59.4      \n",
      "| 13    | 0.90227 |  0.77571 |   64.3      \n",
      "| 14    | 0.89451 |  0.47350 |   69.1      \n",
      "| 15    | 0.90261 |  0.53137 |   74.0      \n",
      "Early stopping occured at epoch 15\n",
      "Training done in 74.015 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57674 |  0.40842 |   3.5       \n",
      "| 2     | 0.76226 |  0.71554 |   7.2       \n",
      "| 3     | 0.80474 |  0.60863 |   10.7      \n",
      "| 4     | 0.81652 |  0.77996 |   14.3      \n",
      "| 5     | 0.83430 |  0.80511 |   17.9      \n",
      "| 6     | 0.84472 |  0.82061 |   21.3      \n",
      "| 7     | 0.85814 |  0.82722 |   24.8      \n",
      "| 8     | 0.85800 |  0.84255 |   28.2      \n",
      "| 9     | 0.87587 |  0.85471 |   31.7      \n",
      "| 10    | 0.88321 |  0.85415 |   35.1      \n",
      "| 11    | 0.88832 |  0.87301 |   38.7      \n",
      "| 12    | 0.89011 |  0.82597 |   42.1      \n",
      "| 13    | 0.88214 |  0.86119 |   45.5      \n",
      "| 14    | 0.86262 |  0.86990 |   49.0      \n",
      "| 15    | 0.88711 |  0.87479 |   52.5      \n",
      "| 16    | 0.89436 |  0.88165 |   56.0      \n",
      "| 17    | 0.89347 |  0.88523 |   59.3      \n",
      "| 18    | 0.89404 |  0.88001 |   62.8      \n",
      "| 19    | 0.88772 |  0.86102 |   66.1      \n",
      "| 20    | 0.89027 |  0.88367 |   69.6      \n",
      "| 21    | 0.89187 |  0.87942 |   73.1      \n",
      "| 22    | 0.88759 |  0.88109 |   76.6      \n",
      "Early stopping occured at epoch 22\n",
      "Training done in 76.646 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.55533 |  0.60250 |   3.3       \n",
      "| 2     | 0.62062 |  0.65799 |   6.8       \n",
      "| 3     | 0.67223 |  0.74925 |   10.3      \n",
      "| 4     | 0.80838 |  0.80751 |   13.7      \n",
      "| 5     | 0.84376 |  0.81480 |   17.5      \n",
      "| 6     | 0.85461 |  0.81336 |   21.2      \n",
      "| 7     | 0.85513 |  0.80766 |   24.7      \n",
      "| 8     | 0.85893 |  0.83693 |   28.0      \n",
      "| 9     | 0.86891 |  0.84593 |   31.3      \n",
      "| 10    | 0.87332 |  0.85530 |   34.8      \n",
      "| 11    | 0.86770 |  0.81147 |   38.2      \n",
      "| 12    | 0.87066 |  0.85055 |   41.8      \n",
      "| 13    | 0.88030 |  0.83102 |   45.2      \n",
      "| 14    | 0.87397 |  0.86891 |   48.7      \n",
      "| 15    | 0.88173 |  0.85858 |   52.0      \n",
      "| 16    | 0.86748 |  0.81190 |   55.3      \n",
      "| 17    | 0.87067 |  0.86166 |   58.8      \n",
      "| 18    | 0.88055 |  0.85567 |   62.1      \n",
      "| 19    | 0.88450 |  0.87062 |   65.6      \n",
      "| 20    | 0.87985 |  0.88143 |   69.0      \n",
      "| 21    | 0.88881 |  0.88119 |   72.3      \n",
      "| 22    | 0.89527 |  0.89195 |   75.7      \n",
      "| 23    | 0.90232 |  0.89088 |   79.2      \n",
      "| 24    | 0.90367 |  0.90046 |   82.7      \n",
      "| 25    | 0.90595 |  0.89610 |   86.2      \n",
      "| 26    | 0.90692 |  0.90104 |   89.7      \n",
      "| 27    | 0.90749 |  0.89872 |   93.2      \n",
      "| 28    | 0.90812 |  0.90381 |   96.6      \n",
      "| 29    | 0.90988 |  0.90520 |   100.0     \n",
      "| 30    | 0.91173 |  0.90504 |   103.3     \n",
      "| 31    | 0.91260 |  0.90497 |   106.7     \n",
      "| 32    | 0.91278 |  0.90112 |   110.0     \n",
      "| 33    | 0.91314 |  0.90236 |   113.3     \n",
      "| 34    | 0.91310 |  0.90395 |   116.7     \n",
      "Early stopping occured at epoch 34\n",
      "Training done in 116.660 seconds.\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.63810 |  0.72577 |   4.3       \n",
      "| 2     | 0.77203 |  0.77510 |   8.6       \n",
      "| 3     | 0.81460 |  0.79318 |   13.1      \n",
      "| 4     | 0.83460 |  0.82037 |   17.5      \n",
      "| 5     | 0.84303 |  0.82953 |   21.8      \n",
      "| 6     | 0.85462 |  0.85564 |   26.2      \n",
      "| 7     | 0.86054 |  0.85916 |   30.6      \n",
      "| 8     | 0.86689 |  0.85020 |   35.0      \n",
      "| 9     | 0.86848 |  0.84210 |   39.3      \n",
      "| 10    | 0.86747 |  0.84292 |   43.6      \n",
      "| 11    | 0.86973 |  0.84565 |   48.0      \n",
      "| 12    | 0.87568 |  0.84720 |   52.3      \n",
      "Early stopping occured at epoch 12\n",
      "Training done in 52.262 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.57518 |  0.65104 |   4.3       \n",
      "| 2     | 0.77591 |  0.74490 |   8.7       \n",
      "| 3     | 0.83688 |  0.82560 |   13.2      \n",
      "| 4     | 0.86393 |  0.86085 |   17.6      \n",
      "| 5     | 0.88416 |  0.86128 |   22.0      \n",
      "| 6     | 0.88331 |  0.85228 |   26.3      \n",
      "| 7     | 0.87758 |  0.85437 |   30.6      \n",
      "| 8     | 0.87395 |  0.86785 |   35.0      \n",
      "| 9     | 0.88160 |  0.87559 |   39.4      \n",
      "| 10    | 0.89096 |  0.87988 |   43.8      \n",
      "| 11    | 0.90198 |  0.89327 |   48.1      \n",
      "| 12    | 0.90594 |  0.87933 |   52.3      \n",
      "| 13    | 0.89800 |  0.88135 |   56.7      \n",
      "| 14    | 0.90209 |  0.80194 |   61.0      \n",
      "| 15    | 0.90611 |  0.77350 |   65.4      \n",
      "| 16    | 0.90764 |  0.81305 |   69.7      \n",
      "Early stopping occured at epoch 16\n",
      "Training done in 69.683 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.50763 |  0.57954 |   2.7       \n",
      "| 2     | 0.60929 |  0.58175 |   5.3       \n",
      "| 3     | 0.69108 |  0.68863 |   7.9       \n",
      "| 4     | 0.73212 |  0.69939 |   10.5      \n",
      "| 5     | 0.75898 |  0.71367 |   13.1      \n",
      "| 6     | 0.80263 |  0.74827 |   15.6      \n",
      "| 7     | 0.81512 |  0.75110 |   18.0      \n",
      "| 8     | 0.82567 |  0.74132 |   20.5      \n",
      "| 9     | 0.83002 |  0.75444 |   23.1      \n",
      "| 10    | 0.83830 |  0.78724 |   25.7      \n",
      "| 11    | 0.82439 |  0.74721 |   28.3      \n",
      "| 12    | 0.83953 |  0.83187 |   30.8      \n",
      "| 13    | 0.84911 |  0.83108 |   33.3      \n",
      "| 14    | 0.84944 |  0.82791 |   35.9      \n",
      "| 15    | 0.85665 |  0.83966 |   38.4      \n",
      "| 16    | 0.86274 |  0.83925 |   41.0      \n",
      "| 17    | 0.86368 |  0.84424 |   43.6      \n",
      "| 18    | 0.86290 |  0.85076 |   46.2      \n",
      "| 19    | 0.86490 |  0.84866 |   48.7      \n",
      "| 20    | 0.87309 |  0.85917 |   51.2      \n",
      "| 21    | 0.87314 |  0.85415 |   53.8      \n",
      "| 22    | 0.86882 |  0.85268 |   56.3      \n",
      "| 23    | 0.87342 |  0.85767 |   58.8      \n",
      "| 24    | 0.87640 |  0.85316 |   61.3      \n",
      "| 25    | 0.87927 |  0.86053 |   63.8      \n",
      "| 26    | 0.88184 |  0.86173 |   66.4      \n",
      "| 27    | 0.88212 |  0.86434 |   68.9      \n",
      "| 28    | 0.88229 |  0.86588 |   71.5      \n",
      "| 29    | 0.87995 |  0.86007 |   74.1      \n",
      "| 30    | 0.88021 |  0.86588 |   76.7      \n",
      "| 31    | 0.88096 |  0.86208 |   79.3      \n",
      "| 32    | 0.88059 |  0.87298 |   81.9      \n",
      "| 33    | 0.88224 |  0.87027 |   84.4      \n",
      "| 34    | 0.88194 |  0.86681 |   86.9      \n",
      "| 35    | 0.88487 |  0.87427 |   89.5      \n",
      "| 36    | 0.88514 |  0.86682 |   92.1      \n",
      "| 37    | 0.88712 |  0.83334 |   94.6      \n",
      "| 38    | 0.88647 |  0.79905 |   97.2      \n",
      "| 39    | 0.88754 |  0.85027 |   99.8      \n",
      "| 40    | 0.89103 |  0.85951 |   102.3     \n",
      "Early stopping occured at epoch 40\n",
      "Training done in 102.316 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51918 |  0.59838 |   2.5       \n",
      "| 2     | 0.66092 |  0.68685 |   5.0       \n",
      "| 3     | 0.75558 |  0.72344 |   7.6       \n",
      "| 4     | 0.75718 |  0.77547 |   9.9       \n",
      "| 5     | 0.77122 |  0.77403 |   12.4      \n",
      "| 6     | 0.79509 |  0.71107 |   15.0      \n",
      "| 7     | 0.80132 |  0.76884 |   17.5      \n",
      "| 8     | 0.82198 |  0.81435 |   20.2      \n",
      "| 9     | 0.82314 |  0.74828 |   22.8      \n",
      "| 10    | 0.81317 |  0.82123 |   25.2      \n",
      "| 11    | 0.82765 |  0.72233 |   27.8      \n",
      "| 12    | 0.83676 |  0.72810 |   30.4      \n",
      "| 13    | 0.84474 |  0.83125 |   33.0      \n",
      "| 14    | 0.85617 |  0.84519 |   35.6      \n",
      "| 15    | 0.86362 |  0.85255 |   38.2      \n",
      "| 16    | 0.86848 |  0.85344 |   40.8      \n",
      "| 17    | 0.87550 |  0.73370 |   43.4      \n",
      "| 18    | 0.87870 |  0.85901 |   46.1      \n",
      "| 19    | 0.88220 |  0.86181 |   48.7      \n",
      "| 20    | 0.88362 |  0.77288 |   51.1      \n",
      "| 21    | 0.88496 |  0.78839 |   53.6      \n",
      "| 22    | 0.88801 |  0.87379 |   56.3      \n",
      "| 23    | 0.88997 |  0.87660 |   58.9      \n",
      "| 24    | 0.89329 |  0.87527 |   61.6      \n",
      "| 25    | 0.89368 |  0.88131 |   64.3      \n",
      "| 26    | 0.89337 |  0.86660 |   66.8      \n",
      "| 27    | 0.88189 |  0.85715 |   69.2      \n",
      "| 28    | 0.88424 |  0.86471 |   71.8      \n",
      "| 29    | 0.89074 |  0.88595 |   74.4      \n",
      "| 30    | 0.89451 |  0.88702 |   77.0      \n",
      "| 31    | 0.89599 |  0.88571 |   79.6      \n",
      "| 32    | 0.89400 |  0.88584 |   82.1      \n",
      "| 33    | 0.89725 |  0.88709 |   84.7      \n",
      "| 34    | 0.89750 |  0.88478 |   87.3      \n",
      "| 35    | 0.89617 |  0.88956 |   89.9      \n",
      "| 36    | 0.89690 |  0.88915 |   92.7      \n",
      "| 37    | 0.89555 |  0.88841 |   95.5      \n",
      "| 38    | 0.89356 |  0.88533 |   98.1      \n",
      "| 39    | 0.89010 |  0.88504 |   100.9     \n",
      "| 40    | 0.88945 |  0.88632 |   103.6     \n",
      "Early stopping occured at epoch 40\n",
      "Training done in 103.609 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.49244 |  0.49208 |   7.7       \n",
      "| 2     | 0.48391 |  0.48307 |   15.3      \n",
      "| 3     | 0.51373 |  0.54057 |   22.9      \n",
      "| 4     | 0.45458 |  0.43519 |   30.4      \n",
      "| 5     | 0.49290 |  0.51053 |   37.9      \n",
      "| 6     | 0.56289 |  0.62067 |   45.6      \n",
      "| 7     | 0.63558 |  0.69321 |   53.4      \n",
      "| 8     | 0.67145 |  0.76523 |   61.1      \n",
      "| 9     | 0.75494 |  0.74327 |   68.8      \n",
      "| 10    | 0.76972 |  0.73501 |   76.3      \n",
      "| 11    | 0.75885 |  0.70821 |   83.9      \n",
      "| 12    | 0.75903 |  0.74474 |   91.6      \n",
      "| 13    | 0.80802 |  0.79157 |   99.3      \n",
      "| 14    | 0.82742 |  0.73431 |   107.0     \n",
      "| 15    | 0.84381 |  0.81157 |   114.8     \n",
      "| 16    | 0.85210 |  0.81096 |   122.2     \n",
      "| 17    | 0.85944 |  0.76929 |   129.8     \n",
      "| 18    | 0.86210 |  0.81210 |   137.2     \n",
      "| 19    | 0.87071 |  0.82016 |   144.8     \n",
      "| 20    | 0.85799 |  0.78860 |   152.4     \n",
      "| 21    | 0.87620 |  0.74761 |   159.7     \n",
      "| 22    | 0.87712 |  0.70378 |   167.2     \n",
      "| 23    | 0.88307 |  0.79712 |   174.8     \n",
      "| 24    | 0.88989 |  0.77536 |   182.4     \n",
      "Early stopping occured at epoch 24\n",
      "Training done in 182.373 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.48805 |  0.48738 |   7.6       \n",
      "| 2     | 0.47145 |  0.53003 |   15.1      \n",
      "| 3     | 0.49148 |  0.49586 |   22.6      \n",
      "| 4     | 0.45264 |  0.47409 |   30.0      \n",
      "| 5     | 0.53906 |  0.49470 |   37.5      \n",
      "| 6     | 0.53785 |  0.58531 |   45.0      \n",
      "| 7     | 0.58689 |  0.59386 |   52.7      \n",
      "| 8     | 0.63528 |  0.62823 |   60.3      \n",
      "| 9     | 0.62949 |  0.60900 |   67.8      \n",
      "| 10    | 0.67671 |  0.67950 |   75.4      \n",
      "| 11    | 0.67306 |  0.65537 |   83.0      \n",
      "| 12    | 0.64309 |  0.68304 |   90.6      \n",
      "| 13    | 0.65085 |  0.63186 |   98.0      \n",
      "| 14    | 0.72155 |  0.71209 |   105.5     \n",
      "| 15    | 0.73210 |  0.70421 |   113.2     \n",
      "| 16    | 0.69990 |  0.67870 |   120.6     \n",
      "| 17    | 0.71294 |  0.71597 |   128.2     \n",
      "| 18    | 0.69567 |  0.63273 |   135.9     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 19    | 0.69020 |  0.68458 |   143.5     \n",
      "| 20    | 0.68431 |  0.69482 |   151.1     \n",
      "| 21    | 0.69958 |  0.72266 |   159.0     \n",
      "| 22    | 0.74757 |  0.66235 |   166.6     \n",
      "| 23    | 0.77825 |  0.69194 |   174.5     \n",
      "| 24    | 0.78845 |  0.76788 |   182.3     \n",
      "| 25    | 0.78582 |  0.77680 |   189.9     \n",
      "| 26    | 0.78666 |  0.78686 |   197.5     \n",
      "| 27    | 0.79643 |  0.77659 |   205.0     \n",
      "| 28    | 0.79581 |  0.75425 |   212.6     \n",
      "| 29    | 0.79506 |  0.77307 |   220.1     \n",
      "| 30    | 0.80430 |  0.75889 |   227.8     \n",
      "| 31    | 0.79815 |  0.73479 |   235.3     \n",
      "Early stopping occured at epoch 31\n",
      "Training done in 235.323 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.59558 |  0.59512 |   2.6       \n",
      "| 2     | 0.75475 |  0.71833 |   5.3       \n",
      "| 3     | 0.78543 |  0.76881 |   8.0       \n",
      "| 4     | 0.81918 |  0.79024 |   10.6      \n",
      "| 5     | 0.82949 |  0.79320 |   13.3      \n",
      "| 6     | 0.83444 |  0.78191 |   16.0      \n",
      "| 7     | 0.83633 |  0.78451 |   18.6      \n",
      "| 8     | 0.84183 |  0.82149 |   21.1      \n",
      "| 9     | 0.84060 |  0.82963 |   23.7      \n",
      "| 10    | 0.84575 |  0.83347 |   26.3      \n",
      "| 11    | 0.85684 |  0.81959 |   29.0      \n",
      "| 12    | 0.86392 |  0.83904 |   31.6      \n",
      "| 13    | 0.86623 |  0.84973 |   34.4      \n",
      "| 14    | 0.87043 |  0.85088 |   37.0      \n",
      "| 15    | 0.87132 |  0.84646 |   39.7      \n",
      "| 16    | 0.87265 |  0.85593 |   42.5      \n",
      "| 17    | 0.87483 |  0.86152 |   45.1      \n",
      "| 18    | 0.87564 |  0.86315 |   47.8      \n",
      "| 19    | 0.87581 |  0.86792 |   50.3      \n",
      "| 20    | 0.87753 |  0.86996 |   53.2      \n",
      "| 21    | 0.88102 |  0.86932 |   56.0      \n",
      "| 22    | 0.88181 |  0.87432 |   58.9      \n",
      "| 23    | 0.88319 |  0.87632 |   61.9      \n",
      "| 24    | 0.88348 |  0.87360 |   64.8      \n",
      "| 25    | 0.88448 |  0.87487 |   67.6      \n",
      "| 26    | 0.88734 |  0.87582 |   70.5      \n",
      "| 27    | 0.88733 |  0.87029 |   73.3      \n",
      "| 28    | 0.88884 |  0.87884 |   76.2      \n",
      "| 29    | 0.88587 |  0.87090 |   78.8      \n",
      "| 30    | 0.88914 |  0.87799 |   81.4      \n",
      "| 31    | 0.89005 |  0.87669 |   84.1      \n",
      "| 32    | 0.88768 |  0.88016 |   86.8      \n",
      "| 33    | 0.88934 |  0.87832 |   89.3      \n",
      "| 34    | 0.88915 |  0.87477 |   92.0      \n",
      "| 35    | 0.88620 |  0.86700 |   94.6      \n",
      "| 36    | 0.88636 |  0.87973 |   97.2      \n",
      "| 37    | 0.88520 |  0.87865 |   99.7      \n",
      "Early stopping occured at epoch 37\n",
      "Training done in 99.666 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.59478 |  0.68437 |   2.5       \n",
      "| 2     | 0.76717 |  0.75639 |   5.1       \n",
      "| 3     | 0.78260 |  0.77097 |   7.7       \n",
      "| 4     | 0.80551 |  0.76779 |   10.2      \n",
      "| 5     | 0.82146 |  0.79853 |   12.8      \n",
      "| 6     | 0.83015 |  0.81206 |   15.5      \n",
      "| 7     | 0.83921 |  0.80235 |   18.1      \n",
      "| 8     | 0.84170 |  0.82204 |   20.7      \n",
      "| 9     | 0.84976 |  0.82578 |   23.4      \n",
      "| 10    | 0.86479 |  0.84005 |   26.1      \n",
      "| 11    | 0.86540 |  0.81354 |   28.7      \n",
      "| 12    | 0.87262 |  0.84272 |   31.5      \n",
      "| 13    | 0.87978 |  0.85223 |   34.3      \n",
      "| 14    | 0.88531 |  0.85527 |   37.0      \n",
      "| 15    | 0.88530 |  0.86617 |   39.6      \n",
      "| 16    | 0.88193 |  0.87075 |   42.2      \n",
      "| 17    | 0.89308 |  0.87673 |   44.9      \n",
      "| 18    | 0.89630 |  0.87495 |   47.6      \n",
      "| 19    | 0.89992 |  0.88551 |   50.3      \n",
      "| 20    | 0.89856 |  0.88901 |   52.8      \n",
      "| 21    | 0.90130 |  0.88680 |   55.4      \n",
      "| 22    | 0.89810 |  0.89165 |   58.0      \n",
      "| 23    | 0.90324 |  0.89643 |   60.5      \n",
      "| 24    | 0.90433 |  0.90018 |   63.0      \n",
      "| 25    | 0.90457 |  0.89706 |   65.5      \n",
      "| 26    | 0.90729 |  0.89887 |   68.1      \n",
      "| 27    | 0.90679 |  0.89853 |   70.6      \n",
      "| 28    | 0.90700 |  0.90214 |   73.0      \n",
      "| 29    | 0.90683 |  0.90165 |   75.6      \n",
      "| 30    | 0.90777 |  0.90341 |   78.1      \n",
      "| 31    | 0.91002 |  0.90267 |   80.7      \n",
      "| 32    | 0.91027 |  0.90387 |   83.2      \n",
      "| 33    | 0.91114 |  0.90166 |   86.1      \n",
      "| 34    | 0.90926 |  0.89635 |   88.8      \n",
      "| 35    | 0.90387 |  0.89083 |   91.5      \n",
      "| 36    | 0.90267 |  0.89088 |   94.3      \n",
      "| 37    | 0.90211 |  0.88371 |   97.1      \n",
      "Early stopping occured at epoch 37\n",
      "Training done in 97.101 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.63996 |  0.76702 |   4.0       \n",
      "| 2     | 0.80514 |  0.79695 |   7.5       \n",
      "| 3     | 0.82368 |  0.80887 |   11.2      \n",
      "| 4     | 0.83522 |  0.83222 |   15.0      \n",
      "| 5     | 0.84396 |  0.83023 |   18.8      \n",
      "| 6     | 0.84320 |  0.84217 |   22.5      \n",
      "| 7     | 0.85676 |  0.85679 |   26.4      \n",
      "| 8     | 0.86555 |  0.86786 |   30.2      \n",
      "| 9     | 0.87503 |  0.88061 |   34.1      \n",
      "| 10    | 0.88253 |  0.88282 |   37.8      \n",
      "| 11    | 0.88950 |  0.88663 |   41.5      \n",
      "| 12    | 0.89139 |  0.88234 |   45.5      \n",
      "| 13    | 0.89062 |  0.88280 |   49.4      \n",
      "| 14    | 0.88892 |  0.88214 |   52.9      \n",
      "| 15    | 0.89038 |  0.85386 |   56.6      \n",
      "| 16    | 0.89046 |  0.88617 |   60.2      \n",
      "Early stopping occured at epoch 16\n",
      "Training done in 60.246 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.64353 |  0.75368 |   3.5       \n",
      "| 2     | 0.80317 |  0.70424 |   7.1       \n",
      "| 3     | 0.82151 |  0.73609 |   10.7      \n",
      "| 4     | 0.83423 |  0.57713 |   14.4      \n",
      "| 5     | 0.84201 |  0.70877 |   18.1      \n",
      "| 6     | 0.85631 |  0.82342 |   21.8      \n",
      "| 7     | 0.86235 |  0.81722 |   25.5      \n",
      "| 8     | 0.87612 |  0.82589 |   29.3      \n",
      "| 9     | 0.88316 |  0.82291 |   32.9      \n",
      "| 10    | 0.88924 |  0.86961 |   36.6      \n",
      "| 11    | 0.88846 |  0.84905 |   40.3      \n",
      "| 12    | 0.89136 |  0.86632 |   44.1      \n",
      "| 13    | 0.89498 |  0.88125 |   47.7      \n",
      "| 14    | 0.89444 |  0.88743 |   51.3      \n",
      "| 15    | 0.89811 |  0.88548 |   54.9      \n",
      "| 16    | 0.90298 |  0.89457 |   58.6      \n",
      "| 17    | 0.90342 |  0.89656 |   62.3      \n",
      "| 18    | 0.90848 |  0.90059 |   66.0      \n",
      "| 19    | 0.91062 |  0.89981 |   69.7      \n",
      "| 20    | 0.91164 |  0.90711 |   73.5      \n",
      "| 21    | 0.91479 |  0.90861 |   77.3      \n",
      "| 22    | 0.91347 |  0.91110 |   81.0      \n",
      "| 23    | 0.91518 |  0.89945 |   84.6      \n",
      "| 24    | 0.91834 |  0.89562 |   88.1      \n",
      "| 25    | 0.92210 |  0.91222 |   91.9      \n",
      "| 26    | 0.92119 |  0.91094 |   95.3      \n",
      "| 27    | 0.91918 |  0.91212 |   99.0      \n",
      "| 28    | 0.92004 |  0.91286 |   102.9     \n",
      "| 29    | 0.92164 |  0.91343 |   107.1     \n",
      "| 30    | 0.92050 |  0.91438 |   111.0     \n",
      "| 31    | 0.92086 |  0.91158 |   114.9     \n",
      "| 32    | 0.91696 |  0.88748 |   118.8     \n",
      "| 33    | 0.86638 |  0.87981 |   122.5     \n",
      "| 34    | 0.87869 |  0.84500 |   126.3     \n",
      "| 35    | 0.89280 |  0.85187 |   130.0     \n",
      "Early stopping occured at epoch 35\n",
      "Training done in 130.004 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.55238 |  0.57025 |   4.6       \n",
      "| 2     | 0.62174 |  0.58048 |   9.4       \n",
      "| 3     | 0.72028 |  0.74140 |   14.2      \n",
      "| 4     | 0.77466 |  0.71487 |   18.6      \n",
      "| 5     | 0.79690 |  0.76333 |   23.1      \n",
      "| 6     | 0.82959 |  0.79465 |   27.8      \n",
      "| 7     | 0.84429 |  0.83992 |   32.4      \n",
      "| 8     | 0.85041 |  0.84268 |   37.0      \n",
      "| 9     | 0.86087 |  0.84588 |   41.5      \n",
      "| 10    | 0.86663 |  0.85014 |   46.2      \n",
      "| 11    | 0.86573 |  0.86057 |   50.7      \n",
      "| 12    | 0.87138 |  0.86988 |   55.5      \n",
      "| 13    | 0.87692 |  0.87599 |   60.1      \n",
      "| 14    | 0.87835 |  0.87946 |   64.7      \n",
      "| 15    | 0.87913 |  0.87206 |   69.4      \n",
      "| 16    | 0.87731 |  0.87517 |   74.0      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 17    | 0.86695 |  0.86447 |   78.5      \n",
      "| 18    | 0.86398 |  0.87478 |   82.7      \n",
      "| 19    | 0.86999 |  0.87827 |   87.2      \n",
      "Early stopping occured at epoch 19\n",
      "Training done in 87.248 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.51886 |  0.53701 |   4.5       \n",
      "| 2     | 0.64675 |  0.72676 |   9.4       \n",
      "| 3     | 0.70375 |  0.73001 |   14.1      \n",
      "| 4     | 0.79624 |  0.77407 |   18.8      \n",
      "| 5     | 0.81020 |  0.80167 |   23.4      \n",
      "| 6     | 0.82137 |  0.79049 |   28.1      \n",
      "| 7     | 0.84116 |  0.81864 |   32.7      \n",
      "| 8     | 0.84493 |  0.75366 |   37.5      \n",
      "| 9     | 0.84819 |  0.77089 |   42.1      \n",
      "| 10    | 0.84113 |  0.76348 |   46.6      \n",
      "| 11    | 0.84846 |  0.83902 |   51.3      \n",
      "| 12    | 0.86230 |  0.82603 |   55.9      \n",
      "| 13    | 0.86245 |  0.85967 |   60.6      \n",
      "| 14    | 0.86812 |  0.86089 |   65.3      \n",
      "| 15    | 0.87919 |  0.87793 |   69.6      \n",
      "| 16    | 0.87853 |  0.87562 |   74.4      \n",
      "| 17    | 0.88407 |  0.88181 |   79.1      \n",
      "| 18    | 0.88724 |  0.87843 |   83.6      \n",
      "| 19    | 0.89056 |  0.88521 |   88.1      \n",
      "| 20    | 0.89169 |  0.88412 |   92.8      \n",
      "| 21    | 0.88964 |  0.88476 |   97.5      \n",
      "| 22    | 0.88871 |  0.88108 |   102.2     \n",
      "| 23    | 0.89127 |  0.87204 |   106.9     \n",
      "| 24    | 0.88871 |  0.85856 |   111.5     \n",
      "Early stopping occured at epoch 24\n",
      "Training done in 111.516 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54159 |  0.54612 |   3.0       \n",
      "| 2     | 0.66834 |  0.73609 |   6.0       \n",
      "| 3     | 0.75770 |  0.76349 |   9.1       \n",
      "| 4     | 0.80735 |  0.79366 |   11.9      \n",
      "| 5     | 0.82144 |  0.82215 |   14.8      \n",
      "| 6     | 0.82702 |  0.79136 |   17.6      \n",
      "| 7     | 0.84556 |  0.79874 |   20.5      \n",
      "| 8     | 0.84968 |  0.79896 |   23.3      \n",
      "| 9     | 0.85043 |  0.81927 |   26.1      \n",
      "| 10    | 0.85382 |  0.80434 |   29.1      \n",
      "Early stopping occured at epoch 10\n",
      "Training done in 29.056 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.55787 |  0.59988 |   3.0       \n",
      "| 2     | 0.65580 |  0.73689 |   6.3       \n",
      "| 3     | 0.77974 |  0.74772 |   9.3       \n",
      "| 4     | 0.81013 |  0.79572 |   12.3      \n",
      "| 5     | 0.82057 |  0.76169 |   15.4      \n",
      "| 6     | 0.82900 |  0.80204 |   18.3      \n",
      "| 7     | 0.84641 |  0.81230 |   21.4      \n",
      "| 8     | 0.85627 |  0.82448 |   24.4      \n",
      "| 9     | 0.86268 |  0.84515 |   27.2      \n",
      "| 10    | 0.86596 |  0.85530 |   30.2      \n",
      "| 11    | 0.87094 |  0.83404 |   33.1      \n",
      "| 12    | 0.85638 |  0.83013 |   36.1      \n",
      "| 13    | 0.84093 |  0.83609 |   39.1      \n",
      "| 14    | 0.85140 |  0.80616 |   42.0      \n",
      "| 15    | 0.85171 |  0.84374 |   45.0      \n",
      "Early stopping occured at epoch 15\n",
      "Training done in 45.034 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.56022 |  0.57506 |   3.1       \n",
      "| 2     | 0.75609 |  0.70766 |   6.0       \n",
      "| 3     | 0.80406 |  0.71546 |   9.0       \n",
      "| 4     | 0.79886 |  0.74467 |   12.1      \n",
      "| 5     | 0.82083 |  0.74259 |   15.2      \n",
      "| 6     | 0.84058 |  0.82768 |   18.2      \n",
      "| 7     | 0.86436 |  0.82865 |   21.3      \n",
      "| 8     | 0.86982 |  0.84654 |   24.5      \n",
      "| 9     | 0.87825 |  0.84459 |   27.4      \n",
      "| 10    | 0.88296 |  0.84303 |   30.4      \n",
      "| 11    | 0.88450 |  0.85304 |   33.5      \n",
      "| 12    | 0.88810 |  0.84340 |   36.5      \n",
      "| 13    | 0.88147 |  0.86482 |   39.6      \n",
      "| 14    | 0.88841 |  0.86721 |   42.8      \n",
      "| 15    | 0.88880 |  0.87225 |   45.8      \n",
      "| 16    | 0.88788 |  0.87335 |   49.0      \n",
      "| 17    | 0.88717 |  0.86995 |   52.1      \n",
      "| 18    | 0.88467 |  0.87773 |   55.3      \n",
      "| 19    | 0.88897 |  0.88077 |   58.5      \n",
      "| 20    | 0.89275 |  0.85433 |   61.6      \n",
      "| 21    | 0.89569 |  0.88078 |   64.7      \n",
      "| 22    | 0.89694 |  0.88765 |   67.7      \n",
      "| 23    | 0.89850 |  0.88722 |   70.5      \n",
      "| 24    | 0.89936 |  0.88980 |   73.6      \n",
      "| 25    | 0.90345 |  0.89166 |   76.8      \n",
      "| 26    | 0.90139 |  0.89306 |   79.9      \n",
      "| 27    | 0.90005 |  0.86528 |   82.9      \n",
      "| 28    | 0.89936 |  0.85797 |   85.9      \n",
      "| 29    | 0.90295 |  0.88446 |   88.9      \n",
      "| 30    | 0.90476 |  0.89689 |   91.8      \n",
      "| 31    | 0.90704 |  0.89533 |   94.8      \n",
      "| 32    | 0.90489 |  0.89722 |   97.8      \n",
      "| 33    | 0.90505 |  0.89437 |   100.8     \n",
      "| 34    | 0.89866 |  0.88517 |   103.8     \n",
      "| 35    | 0.89808 |  0.88211 |   106.9     \n",
      "| 36    | 0.90389 |  0.89641 |   109.8     \n",
      "| 37    | 0.90505 |  0.89732 |   112.8     \n",
      "| 38    | 0.90345 |  0.90086 |   115.8     \n",
      "| 39    | 0.90434 |  0.90132 |   118.6     \n",
      "| 40    | 0.90738 |  0.89963 |   121.7     \n",
      "| 41    | 0.90805 |  0.90036 |   124.6     \n",
      "| 42    | 0.90899 |  0.90262 |   127.7     \n",
      "| 43    | 0.90904 |  0.90161 |   130.7     \n",
      "| 44    | 0.90858 |  0.90156 |   133.8     \n",
      "| 45    | 0.91025 |  0.89582 |   136.7     \n",
      "| 46    | 0.90719 |  0.90183 |   139.7     \n",
      "| 47    | 0.91047 |  0.90235 |   142.7     \n",
      "Early stopping occured at epoch 47\n",
      "Training done in 142.710 seconds.\n",
      "---------------------------------------\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 5 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.54855 |  0.58076 |   3.0       \n",
      "| 2     | 0.61168 |  0.56894 |   5.9       \n",
      "| 3     | 0.67772 |  0.66760 |   8.9       \n",
      "| 4     | 0.80418 |  0.80256 |   11.9      \n",
      "| 5     | 0.81633 |  0.72589 |   14.9      \n",
      "| 6     | 0.83935 |  0.79226 |   17.8      \n",
      "| 7     | 0.86254 |  0.72734 |   20.8      \n",
      "| 8     | 0.86324 |  0.77055 |   23.9      \n",
      "| 9     | 0.86149 |  0.74361 |   26.9      \n",
      "Early stopping occured at epoch 9\n",
      "Training done in 26.895 seconds.\n",
      "---------------------------------------\n",
      "CPU times: user 2h 13min 21s, sys: 57min 59s, total: 3h 11min 20s\n",
      "Wall time: 3h 10min 11s\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"%%time\\nparams_results = []\\n\\nfor params in ParameterSampler(grid, n_iter=n_iter, random_state=0):\\n    params[\\\"n_d\\\"] = params[\\\"n_a\\\"]\\n    preds_params = np.zeros(shape=y_test.shape)\\n    results_outer = []\\n    for train_valid_index, test_index in StratifiedShuffleSplit(\\n        n_splits=outer_split, test_size=outer_test, random_state=0\\n    ).split(X, y):\\n        results_inner = []\\n        preds_outer = np.zeros(shape=y_test.shape)\\n        preds_split = np.zeros(shape=y[test_index].shape)\\n        for train_index, valid_index in StratifiedShuffleSplit(\\n            n_splits=inner_split, test_size=inner_test, random_state=0\\n        ).split(X[train_valid_index], y[train_valid_index]):\\n            X_train = X[train_valid_index][train_index]\\n            y_train = y[train_valid_index][train_index]\\n            X_valid = X[train_valid_index][valid_index]\\n            y_valid = y[train_valid_index][valid_index]\\n            \\n            \\n            model_params = {}\\n            fit_params = {}\\n            for elt, value in params.items():\\n                if elt in MODEL_PARAMS_KEYS:\\n                    model_params[elt] = value\\n                elif elt in [\\\"emb_generator\\\", \\\"max_emd_dims\\\"]:\\n                    model_params[\\\"cat_emb_dim\\\"] = params[\\\"emb_generator\\\"](cat_dims, params[\\\"max_emd_dims\\\"])\\n                else:\\n                    fit_params[elt] = value\\n\\n            clf = TabNetClassifier(cat_idxs=cat_idxs, cat_dims=cat_dims, **model_params)\\n            history = clf.fit(\\n                X_train, y_train, X_valid=X_valid, y_valid=y_valid, **fit_params\\n            )\\n            preds_inner = clf.predict_proba(X_test)[:, 1]\\n            preds_split += clf.predict_proba(X[test_index])[:, 1]\\n\\n            preds_outer += preds_inner\\n            auc_inner_test = roc_auc_score(y_score=preds_inner, y_true=y_test)\\n\\n            # preds_inner = clf.predict_proba(X_valid)[:, 1]\\n            # auc_inner_valid = roc_auc_score(y_score=preds_inner, y_true=y_valid)\\n\\n            res_inner = {\\n                #\\\"history\\\": history,\\n                #'auc_valid': auc_inner_valid,\\n                \\\"auc_test\\\": auc_inner_test,\\n            }\\n            results_inner.append(res_inner)\\n            del preds_inner, clf\\n        preds_outer = preds_outer / len(results_inner)\\n        preds_split = preds_split / len(results_inner)\\n        preds_params += preds_outer\\n        res_outer = {\\n            \\\"inner_results\\\": results_inner,\\n            \\\"auc_test\\\": roc_auc_score(y_score=preds_outer, y_true=y_test),\\n            \\\"auc_split\\\": roc_auc_score(y_score=preds_split, y_true=y[test_index]),\\n        }\\n        results_outer.append(res_outer)\\n        del preds_outer\\n    preds_params = preds_params / len(results_outer)\\n    res_params = {\\n        \\\"params\\\": params,\\n        \\\"outer_results\\\": results_outer,\\n        \\\"auc_test\\\": roc_auc_score(y_score=preds_params, y_true=y_test),\\n    }\\n    params_results.append(res_params)\\n    del preds_params\";\n",
       "                var nbb_formatted_code = \"%%time\\nparams_results = []\\n\\nfor params in ParameterSampler(grid, n_iter=n_iter, random_state=0):\\n    params[\\\"n_d\\\"] = params[\\\"n_a\\\"]\\n    preds_params = np.zeros(shape=y_test.shape)\\n    results_outer = []\\n    for train_valid_index, test_index in StratifiedShuffleSplit(\\n        n_splits=outer_split, test_size=outer_test, random_state=0\\n    ).split(X, y):\\n        results_inner = []\\n        preds_outer = np.zeros(shape=y_test.shape)\\n        preds_split = np.zeros(shape=y[test_index].shape)\\n        for train_index, valid_index in StratifiedShuffleSplit(\\n            n_splits=inner_split, test_size=inner_test, random_state=0\\n        ).split(X[train_valid_index], y[train_valid_index]):\\n            X_train = X[train_valid_index][train_index]\\n            y_train = y[train_valid_index][train_index]\\n            X_valid = X[train_valid_index][valid_index]\\n            y_valid = y[train_valid_index][valid_index]\\n            \\n            \\n            model_params = {}\\n            fit_params = {}\\n            for elt, value in params.items():\\n                if elt in MODEL_PARAMS_KEYS:\\n                    model_params[elt] = value\\n                elif elt in [\\\"emb_generator\\\", \\\"max_emd_dims\\\"]:\\n                    model_params[\\\"cat_emb_dim\\\"] = params[\\\"emb_generator\\\"](cat_dims, params[\\\"max_emd_dims\\\"])\\n                else:\\n                    fit_params[elt] = value\\n\\n            clf = TabNetClassifier(cat_idxs=cat_idxs, cat_dims=cat_dims, **model_params)\\n            history = clf.fit(\\n                X_train, y_train, X_valid=X_valid, y_valid=y_valid, **fit_params\\n            )\\n            preds_inner = clf.predict_proba(X_test)[:, 1]\\n            preds_split += clf.predict_proba(X[test_index])[:, 1]\\n\\n            preds_outer += preds_inner\\n            auc_inner_test = roc_auc_score(y_score=preds_inner, y_true=y_test)\\n\\n            # preds_inner = clf.predict_proba(X_valid)[:, 1]\\n            # auc_inner_valid = roc_auc_score(y_score=preds_inner, y_true=y_valid)\\n\\n            res_inner = {\\n                #\\\"history\\\": history,\\n                #'auc_valid': auc_inner_valid,\\n                \\\"auc_test\\\": auc_inner_test,\\n            }\\n            results_inner.append(res_inner)\\n            del preds_inner, clf\\n        preds_outer = preds_outer / len(results_inner)\\n        preds_split = preds_split / len(results_inner)\\n        preds_params += preds_outer\\n        res_outer = {\\n            \\\"inner_results\\\": results_inner,\\n            \\\"auc_test\\\": roc_auc_score(y_score=preds_outer, y_true=y_test),\\n            \\\"auc_split\\\": roc_auc_score(y_score=preds_split, y_true=y[test_index]),\\n        }\\n        results_outer.append(res_outer)\\n        del preds_outer\\n    preds_params = preds_params / len(results_outer)\\n    res_params = {\\n        \\\"params\\\": params,\\n        \\\"outer_results\\\": results_outer,\\n        \\\"auc_test\\\": roc_auc_score(y_score=preds_params, y_true=y_test),\\n    }\\n    params_results.append(res_params)\\n    del preds_params\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "params_results = []\n",
    "\n",
    "for params in ParameterSampler(grid, n_iter=n_iter, random_state=0):\n",
    "    params[\"n_d\"] = params[\"n_a\"]\n",
    "    preds_params = np.zeros(shape=y_test.shape)\n",
    "    results_outer = []\n",
    "    for train_valid_index, test_index in StratifiedShuffleSplit(\n",
    "        n_splits=outer_split, test_size=outer_test, random_state=0\n",
    "    ).split(X, y):\n",
    "        results_inner = []\n",
    "        preds_outer = np.zeros(shape=y_test.shape)\n",
    "        preds_split = np.zeros(shape=y[test_index].shape)\n",
    "        for train_index, valid_index in StratifiedShuffleSplit(\n",
    "            n_splits=inner_split, test_size=inner_test, random_state=0\n",
    "        ).split(X[train_valid_index], y[train_valid_index]):\n",
    "            X_train = X[train_valid_index][train_index]\n",
    "            y_train = y[train_valid_index][train_index]\n",
    "            X_valid = X[train_valid_index][valid_index]\n",
    "            y_valid = y[train_valid_index][valid_index]\n",
    "            \n",
    "            \n",
    "            model_params = {}\n",
    "            fit_params = {}\n",
    "            for elt, value in params.items():\n",
    "                if elt in MODEL_PARAMS_KEYS:\n",
    "                    model_params[elt] = value\n",
    "                elif elt in [\"emb_generator\", \"max_emd_dims\"]:\n",
    "                    model_params[\"cat_emb_dim\"] = params[\"emb_generator\"](cat_dims, params[\"max_emd_dims\"])\n",
    "                else:\n",
    "                    fit_params[elt] = value\n",
    "\n",
    "            clf = TabNetClassifier(cat_idxs=cat_idxs, cat_dims=cat_dims, **model_params)\n",
    "            history = clf.fit(\n",
    "                X_train, y_train, X_valid=X_valid, y_valid=y_valid, **fit_params\n",
    "            )\n",
    "            preds_inner = clf.predict_proba(X_test)[:, 1]\n",
    "            preds_split += clf.predict_proba(X[test_index])[:, 1]\n",
    "\n",
    "            preds_outer += preds_inner\n",
    "            auc_inner_test = roc_auc_score(y_score=preds_inner, y_true=y_test)\n",
    "\n",
    "            # preds_inner = clf.predict_proba(X_valid)[:, 1]\n",
    "            # auc_inner_valid = roc_auc_score(y_score=preds_inner, y_true=y_valid)\n",
    "\n",
    "            res_inner = {\n",
    "                #\"history\": history,\n",
    "                #'auc_valid': auc_inner_valid,\n",
    "                \"auc_test\": auc_inner_test,\n",
    "            }\n",
    "            results_inner.append(res_inner)\n",
    "            del preds_inner, clf\n",
    "        preds_outer = preds_outer / len(results_inner)\n",
    "        preds_split = preds_split / len(results_inner)\n",
    "        preds_params += preds_outer\n",
    "        res_outer = {\n",
    "            \"inner_results\": results_inner,\n",
    "            \"auc_test\": roc_auc_score(y_score=preds_outer, y_true=y_test),\n",
    "            \"auc_split\": roc_auc_score(y_score=preds_split, y_true=y[test_index]),\n",
    "        }\n",
    "        results_outer.append(res_outer)\n",
    "        del preds_outer\n",
    "    preds_params = preds_params / len(results_outer)\n",
    "    res_params = {\n",
    "        \"params\": params,\n",
    "        \"outer_results\": results_outer,\n",
    "        \"auc_test\": roc_auc_score(y_score=preds_params, y_true=y_test),\n",
    "    }\n",
    "    params_results.append(res_params)\n",
    "    del preds_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"agg_results = []\\nfor res in params_results:\\n    outer_auc = np.array(list(map(lambda elt: elt[\\\"auc_test\\\"], res[\\\"outer_results\\\"])))\\n    inner_auc = np.array(\\n        list(\\n            map(\\n                lambda elt: list(map(lambda e: e[\\\"auc_test\\\"], elt[\\\"inner_results\\\"])),\\n                res[\\\"outer_results\\\"],\\n            )\\n        )\\n    )\\n    split_auc = np.array(list(map(lambda elt: elt[\\\"auc_split\\\"], res[\\\"outer_results\\\"])))\\n    agg = {\\n        \\\"params\\\": res[\\\"params\\\"],\\n        \\\"auc_test\\\": res[\\\"auc_test\\\"],\\n        \\\"outer_auc\\\": outer_auc,\\n        \\\"inner_auc\\\": inner_auc,\\n        \\\"split_auc\\\": split_auc,\\n    }\\n    agg_results.append(agg)\";\n",
       "                var nbb_formatted_code = \"agg_results = []\\nfor res in params_results:\\n    outer_auc = np.array(list(map(lambda elt: elt[\\\"auc_test\\\"], res[\\\"outer_results\\\"])))\\n    inner_auc = np.array(\\n        list(\\n            map(\\n                lambda elt: list(map(lambda e: e[\\\"auc_test\\\"], elt[\\\"inner_results\\\"])),\\n                res[\\\"outer_results\\\"],\\n            )\\n        )\\n    )\\n    split_auc = np.array(list(map(lambda elt: elt[\\\"auc_split\\\"], res[\\\"outer_results\\\"])))\\n    agg = {\\n        \\\"params\\\": res[\\\"params\\\"],\\n        \\\"auc_test\\\": res[\\\"auc_test\\\"],\\n        \\\"outer_auc\\\": outer_auc,\\n        \\\"inner_auc\\\": inner_auc,\\n        \\\"split_auc\\\": split_auc,\\n    }\\n    agg_results.append(agg)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agg_results = []\n",
    "for res in params_results:\n",
    "    outer_auc = np.array(list(map(lambda elt: elt[\"auc_test\"], res[\"outer_results\"])))\n",
    "    inner_auc = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda elt: list(map(lambda e: e[\"auc_test\"], elt[\"inner_results\"])),\n",
    "                res[\"outer_results\"],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    split_auc = np.array(list(map(lambda elt: elt[\"auc_split\"], res[\"outer_results\"])))\n",
    "    agg = {\n",
    "        \"params\": res[\"params\"],\n",
    "        \"auc_test\": res[\"auc_test\"],\n",
    "        \"outer_auc\": outer_auc,\n",
    "        \"inner_auc\": inner_auc,\n",
    "        \"split_auc\": split_auc,\n",
    "    }\n",
    "    agg_results.append(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.430378732744839,\n",
       "   'lambda_sparse': 0.0010323260351976567,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.2750776403889842,\n",
       "   'n_a': 27,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 9,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 27},\n",
       "  'auc_test': 0.9176059369863938,\n",
       "  'outer_auc': array([0.89990042, 0.91493624]),\n",
       "  'inner_auc': array([[0.89990042],\n",
       "         [0.91493624]]),\n",
       "  'split_auc': array([0.89421693, 0.91595626])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.5453125891602264,\n",
       "   'lambda_sparse': 0.00024452630570839887,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.04648679303610664,\n",
       "   'n_a': 33,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 1,\n",
       "   'n_steps': 6,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 33},\n",
       "  'auc_test': 0.8987475398967704,\n",
       "  'outer_auc': array([0.88291681, 0.8885047 ]),\n",
       "  'inner_auc': array([[0.88291681],\n",
       "         [0.8885047 ]]),\n",
       "  'split_auc': array([0.87486968, 0.88706737])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.5563135018997007,\n",
       "   'lambda_sparse': 0.022390342721683706,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 5,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.06377432383761507,\n",
       "   'n_a': 57,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 5,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 57},\n",
       "  'auc_test': 0.7076961572913615,\n",
       "  'outer_auc': array([0.78583377, 0.61582371]),\n",
       "  'inner_auc': array([[0.78583377],\n",
       "         [0.61582371]]),\n",
       "  'split_auc': array([0.78413086, 0.61846633])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.1640395841502142,\n",
       "   'lambda_sparse': 0.0004862573145115566,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.07702020060945171,\n",
       "   'n_a': 60,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 60},\n",
       "  'auc_test': 0.8669074847278915,\n",
       "  'outer_auc': array([0.88099207, 0.80502413]),\n",
       "  'inner_auc': array([[0.88099207],\n",
       "         [0.80502413]]),\n",
       "  'split_auc': array([0.87566161, 0.80647473])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.0375796008727103,\n",
       "   'lambda_sparse': 0.001225116771800308,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.1117255124590943,\n",
       "   'n_a': 23,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 1,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 23},\n",
       "  'auc_test': 0.900749707807196,\n",
       "  'outer_auc': array([0.88467812, 0.89614797]),\n",
       "  'inner_auc': array([[0.88467812],\n",
       "         [0.89614797]]),\n",
       "  'split_auc': array([0.88000003, 0.89780165])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.1204509432585397,\n",
       "   'lambda_sparse': 0.0021569177143056176,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.02277370342677341,\n",
       "   'n_a': 35,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 7,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 35},\n",
       "  'auc_test': 0.9087712712304535,\n",
       "  'outer_auc': array([0.88670179, 0.90713307]),\n",
       "  'inner_auc': array([[0.88670179],\n",
       "         [0.90713307]]),\n",
       "  'split_auc': array([0.8816814 , 0.90458713])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.8772030269246407,\n",
       "   'lambda_sparse': 0.08747202693855002,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 20,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.12854972967613487,\n",
       "   'n_a': 60,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 6,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 60},\n",
       "  'auc_test': 0.8846300353542209,\n",
       "  'outer_auc': array([0.87037917, 0.86263343]),\n",
       "  'inner_auc': array([[0.87037917],\n",
       "         [0.86263343]]),\n",
       "  'split_auc': array([0.86642809, 0.86346102])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.9326215457126126,\n",
       "   'lambda_sparse': 1.667738520656325e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.14008252323611042,\n",
       "   'n_a': 56,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 5,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 56},\n",
       "  'auc_test': 0.9116989080399633,\n",
       "  'outer_auc': array([0.88386799, 0.90271734]),\n",
       "  'inner_auc': array([[0.88386799],\n",
       "         [0.90271734]]),\n",
       "  'split_auc': array([0.87476457, 0.90103975])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.0271432712242197,\n",
       "   'lambda_sparse': 0.0013008597541185904,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 20,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.014563585589417805,\n",
       "   'n_a': 37,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 2,\n",
       "   'n_steps': 2,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 37},\n",
       "  'auc_test': 0.8982523093749872,\n",
       "  'outer_auc': array([0.90490343, 0.83064617]),\n",
       "  'inner_auc': array([[0.90490343],\n",
       "         [0.83064617]]),\n",
       "  'split_auc': array([0.89643231, 0.82536705])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.0783755845086413,\n",
       "   'lambda_sparse': 2.5943873188938756e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 5,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.04080236743995599,\n",
       "   'n_a': 10,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 2,\n",
       "   'n_steps': 5,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 10},\n",
       "  'auc_test': 0.9191371314173782,\n",
       "  'outer_auc': array([0.9193154 , 0.89494833]),\n",
       "  'inner_auc': array([[0.9193154 ],\n",
       "         [0.89494833]]),\n",
       "  'split_auc': array([0.91540594, 0.89569918])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.3849442387400397,\n",
       "   'lambda_sparse': 0.0006807807563467865,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 2,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.07323572703689346,\n",
       "   'n_a': 22,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 2,\n",
       "   'n_steps': 3,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 22},\n",
       "  'auc_test': 0.9083151761006905,\n",
       "  'outer_auc': array([0.91606689, 0.80058651]),\n",
       "  'inner_auc': array([[0.91606689],\n",
       "         [0.80058651]]),\n",
       "  'split_auc': array([0.91746907, 0.80018913])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.6371379049026473,\n",
       "   'lambda_sparse': 0.002172960840313279,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.1257562133284671,\n",
       "   'n_a': 62,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 5,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 62},\n",
       "  'auc_test': 0.9056394674960232,\n",
       "  'outer_auc': array([0.88882086, 0.90317411]),\n",
       "  'inner_auc': array([[0.88882086],\n",
       "         [0.90317411]]),\n",
       "  'split_auc': array([0.88173712, 0.90584691])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.73961618549668,\n",
       "   'lambda_sparse': 5.397700876300653e-06,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 5,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.028756461671198243,\n",
       "   'n_a': 42,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 2,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 42},\n",
       "  'auc_test': 0.9258064170081681,\n",
       "  'outer_auc': array([0.92072953, 0.92419146]),\n",
       "  'inner_auc': array([[0.92072953],\n",
       "         [0.92419146]]),\n",
       "  'split_auc': array([0.91845711, 0.92206265])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.7921965508467246,\n",
       "   'lambda_sparse': 0.030212866906827295,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.023933653673248477,\n",
       "   'n_a': 52,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 5,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 52},\n",
       "  'auc_test': 0.8922803301065612,\n",
       "  'outer_auc': array([0.85120724, 0.87948671]),\n",
       "  'inner_auc': array([[0.85120724],\n",
       "         [0.87948671]]),\n",
       "  'split_auc': array([0.85291762, 0.88104597])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.5340474071174537,\n",
       "   'lambda_sparse': 0.00011457785690374962,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.3140276597012327,\n",
       "   'n_a': 32,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 0,\n",
       "   'n_steps': 8,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 32},\n",
       "  'auc_test': 0.8813573212061505,\n",
       "  'outer_auc': array([0.84068358, 0.86989616]),\n",
       "  'inner_auc': array([[0.84068358],\n",
       "         [0.86989616]]),\n",
       "  'split_auc': array([0.83166296, 0.87016596])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.2879803984592746,\n",
       "   'lambda_sparse': 0.0001316058646360065,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.06408932015535629,\n",
       "   'n_a': 14,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 0,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 14},\n",
       "  'auc_test': 0.915440745022434,\n",
       "  'outer_auc': array([0.90715372, 0.90323039]),\n",
       "  'inner_auc': array([[0.90715372],\n",
       "         [0.90323039]]),\n",
       "  'split_auc': array([0.90607905, 0.90136256])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.5965646519120615,\n",
       "   'lambda_sparse': 0.0007076598458663506,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 2,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.08660027520867856,\n",
       "   'n_a': 62,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 0,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 62},\n",
       "  'auc_test': 0.8932592272325282,\n",
       "  'outer_auc': array([0.85312445, 0.88020954]),\n",
       "  'inner_auc': array([[0.85312445],\n",
       "         [0.88020954]]),\n",
       "  'split_auc': array([0.84653568, 0.88005084])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.735123740095793,\n",
       "   'lambda_sparse': 0.00015112093304567623,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.0817306881303552,\n",
       "   'n_a': 51,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 51},\n",
       "  'auc_test': 0.8465007668989404,\n",
       "  'outer_auc': array([0.83196181, 0.8207985 ]),\n",
       "  'inner_auc': array([[0.83196181],\n",
       "         [0.8207985 ]]),\n",
       "  'split_auc': array([0.8249911 , 0.82646381])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.997694013135733,\n",
       "   'lambda_sparse': 5.587808619859327e-06,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.018882983860104733,\n",
       "   'n_a': 27,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 5,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 27},\n",
       "  'auc_test': 0.8612526453768462,\n",
       "  'outer_auc': array([0.88326034, 0.63000298]),\n",
       "  'inner_auc': array([[0.88326034],\n",
       "         [0.63000298]]),\n",
       "  'split_auc': array([0.87635799, 0.62202731])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.1382014772291864,\n",
       "   'lambda_sparse': 0.00010862167264542122,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 2,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.4670521587423068,\n",
       "   'n_a': 31,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 2,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 31},\n",
       "  'auc_test': 0.907244624994908,\n",
       "  'outer_auc': array([0.88601243, 0.89795309]),\n",
       "  'inner_auc': array([[0.88601243],\n",
       "         [0.89795309]]),\n",
       "  'split_auc': array([0.87863398, 0.89952417])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.0332559474677345,\n",
       "   'lambda_sparse': 1.4246614636919328e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.17387296857786808,\n",
       "   'n_a': 52,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 2,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 52},\n",
       "  'auc_test': 0.8897920385208524,\n",
       "  'outer_auc': array([0.88161955, 0.87337457]),\n",
       "  'inner_auc': array([[0.88161955],\n",
       "         [0.87337457]]),\n",
       "  'split_auc': array([0.87349563, 0.8741428 ])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.3589805218357114,\n",
       "   'lambda_sparse': 7.160329261424339e-06,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 5,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.024012718420491894,\n",
       "   'n_a': 61,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 0,\n",
       "   'n_steps': 3,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 61},\n",
       "  'auc_test': 0.9226366384557265,\n",
       "  'outer_auc': array([0.91449731, 0.91621167]),\n",
       "  'inner_auc': array([[0.91449731],\n",
       "         [0.91621167]]),\n",
       "  'split_auc': array([0.91148107, 0.92068042])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.242956802999527,\n",
       "   'lambda_sparse': 0.0007693836323674735,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 20,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.485346289897919,\n",
       "   'n_a': 11,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 11},\n",
       "  'auc_test': 0.9020965591720018,\n",
       "  'outer_auc': array([0.90060573, 0.86839462]),\n",
       "  'inner_auc': array([[0.90060573],\n",
       "         [0.86839462]]),\n",
       "  'split_auc': array([0.89618843, 0.86525247])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.9289795355788617,\n",
       "   'lambda_sparse': 0.0005904894945158314,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 20,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.022725750538844285,\n",
       "   'n_a': 13,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 7,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 13},\n",
       "  'auc_test': 0.8939361973706872,\n",
       "  'outer_auc': array([0.83356801, 0.89677335]),\n",
       "  'inner_auc': array([[0.83356801],\n",
       "         [0.89677335]]),\n",
       "  'split_auc': array([0.82760222, 0.89318724])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.4548292559466465,\n",
       "   'lambda_sparse': 1.8697451219611917e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.012469615291558125,\n",
       "   'n_a': 25,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 5,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 25},\n",
       "  'auc_test': 0.8610251519599146,\n",
       "  'outer_auc': array([0.84434739, 0.82788919]),\n",
       "  'inner_auc': array([[0.84434739],\n",
       "         [0.82788919]]),\n",
       "  'split_auc': array([0.8355768 , 0.82670356])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.709227160621429,\n",
       "   'lambda_sparse': 0.00523808723099433,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 5,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.05899528859454533,\n",
       "   'n_a': 17,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 3,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 17},\n",
       "  'auc_test': 0.9217881020211052,\n",
       "  'outer_auc': array([0.92019441, 0.90313568]),\n",
       "  'inner_auc': array([[0.92019441],\n",
       "         [0.90313568]]),\n",
       "  'split_auc': array([0.91994149, 0.90413683])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.9120292464357345,\n",
       "   'lambda_sparse': 0.03404790819729568,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 20,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.010841401507229618,\n",
       "   'n_a': 60,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 60},\n",
       "  'auc_test': 0.6867864195024612,\n",
       "  'outer_auc': array([0.65341767, 0.65244079]),\n",
       "  'inner_auc': array([[0.65341767],\n",
       "         [0.65244079]]),\n",
       "  'split_auc': array([0.64840871, 0.6480859 ])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.662096910472381,\n",
       "   'lambda_sparse': 0.0013960765039601655,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.03387202494294889,\n",
       "   'n_a': 31,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 1,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 31},\n",
       "  'auc_test': 0.8905008251158419,\n",
       "  'outer_auc': array([0.87264678, 0.8798572 ]),\n",
       "  'inner_auc': array([[0.87264678],\n",
       "         [0.8798572 ]]),\n",
       "  'split_auc': array([0.87150731, 0.88103597])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.9668172331975053,\n",
       "   'lambda_sparse': 0.008784097298362247,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.17446250067951194,\n",
       "   'n_a': 30,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 30},\n",
       "  'auc_test': 0.7800246769401096,\n",
       "  'outer_auc': array([0.78291711, 0.65544207]),\n",
       "  'inner_auc': array([[0.78291711],\n",
       "         [0.65544207]]),\n",
       "  'split_auc': array([0.78099303, 0.65261338])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.4231676341216275,\n",
       "   'lambda_sparse': 5.8677489586249575e-06,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.043221872025090195,\n",
       "   'n_a': 8,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 6,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 8},\n",
       "  'auc_test': 0.8800563281981175,\n",
       "  'outer_auc': array([0.80347767, 0.88522152]),\n",
       "  'inner_auc': array([[0.80347767],\n",
       "         [0.88522152]]),\n",
       "  'split_auc': array([0.80161664, 0.88442033])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.553787501634182,\n",
       "   'lambda_sparse': 7.491078904028411e-06,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.04708619046214657,\n",
       "   'n_a': 49,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 49},\n",
       "  'auc_test': 0.9168309663060481,\n",
       "  'outer_auc': array([0.91867168, 0.88671412]),\n",
       "  'inner_auc': array([[0.91867168],\n",
       "         [0.88671412]]),\n",
       "  'split_auc': array([0.91525466, 0.88474817])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.7112254756999112,\n",
       "   'lambda_sparse': 0.05036858219677769,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 5,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.18583213116478564,\n",
       "   'n_a': 9,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 2,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 9},\n",
       "  'auc_test': 0.9252979287580703,\n",
       "  'outer_auc': array([0.919327  , 0.92204417]),\n",
       "  'inner_auc': array([[0.919327  ],\n",
       "         [0.92204417]]),\n",
       "  'split_auc': array([0.92073297, 0.9248738 ])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.0912292743017729,\n",
       "   'lambda_sparse': 1.111181871403766e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 2,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.02565434159849112,\n",
       "   'n_a': 52,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 0,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 52},\n",
       "  'auc_test': 0.8500207251333322,\n",
       "  'outer_auc': array([0.83372943, 0.83024773]),\n",
       "  'inner_auc': array([[0.83372943],\n",
       "         [0.83024773]]),\n",
       "  'split_auc': array([0.82185411, 0.83132996])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.3632624009840975,\n",
       "   'lambda_sparse': 0.0003647264978213262,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.4578623382232408,\n",
       "   'n_a': 15,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 1,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 15},\n",
       "  'auc_test': 0.9113318843514558,\n",
       "  'outer_auc': array([0.90655867, 0.89983467]),\n",
       "  'inner_auc': array([[0.90655867],\n",
       "         [0.89983467]]),\n",
       "  'split_auc': array([0.90223511, 0.89565043])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.757739483554942,\n",
       "   'lambda_sparse': 0.00028929518644642103,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 20,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.22794035656379164,\n",
       "   'n_a': 57,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 3,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 57},\n",
       "  'auc_test': 0.9236419673933491,\n",
       "  'outer_auc': array([0.92049096, 0.91430829]),\n",
       "  'inner_auc': array([[0.92049096],\n",
       "         [0.91430829]]),\n",
       "  'split_auc': array([0.91734616, 0.91521004])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.026473715517799,\n",
       "   'lambda_sparse': 5.4471281122092484e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.21179781275161064,\n",
       "   'n_a': 30,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 1,\n",
       "   'n_steps': 8,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 30},\n",
       "  'auc_test': 0.9164870390858741,\n",
       "  'outer_auc': array([0.91079908, 0.90889067]),\n",
       "  'inner_auc': array([[0.91079908],\n",
       "         [0.90889067]]),\n",
       "  'split_auc': array([0.90786109, 0.90459403])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.8564476642328163,\n",
       "   'lambda_sparse': 3.0846727205308465e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.07391211230637222,\n",
       "   'n_a': 50,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 6,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 50},\n",
       "  'auc_test': 0.672109089607271,\n",
       "  'outer_auc': array([0.57958001, 0.66114044]),\n",
       "  'inner_auc': array([[0.57958001],\n",
       "         [0.66114044]]),\n",
       "  'split_auc': array([0.5823111 , 0.66101175])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.6872110062961974,\n",
       "   'lambda_sparse': 0.008797593447827013,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.040774755074231915,\n",
       "   'n_a': 39,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 1,\n",
       "   'n_steps': 5,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 39},\n",
       "  'auc_test': 0.9218048205602023,\n",
       "  'outer_auc': array([0.91632741, 0.91299846]),\n",
       "  'inner_auc': array([[0.91632741],\n",
       "         [0.91299846]]),\n",
       "  'split_auc': array([0.91496442, 0.91153634])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.0278967918668307,\n",
       "   'lambda_sparse': 0.0004648666694554819,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.017768793603742103,\n",
       "   'n_a': 9,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 0,\n",
       "   'n_steps': 9,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 9},\n",
       "  'auc_test': 0.903543021244736,\n",
       "  'outer_auc': array([0.91024471, 0.87375565]),\n",
       "  'inner_auc': array([[0.91024471],\n",
       "         [0.87375565]]),\n",
       "  'split_auc': array([0.90481657, 0.87092436])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.841078933360197,\n",
       "   'lambda_sparse': 0.0006121304118151219,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 5,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.044052139865132345,\n",
       "   'n_a': 41,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 2,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 41},\n",
       "  'auc_test': 0.9114357295861724,\n",
       "  'outer_auc': array([0.9202577 , 0.85834304]),\n",
       "  'inner_auc': array([[0.9202577 ],\n",
       "         [0.85834304]]),\n",
       "  'split_auc': array([0.9190612 , 0.85866825])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.5242362984793565,\n",
       "   'lambda_sparse': 0.0001908546888474114,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.15200111243859488,\n",
       "   'n_a': 62,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 7,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 62},\n",
       "  'auc_test': 0.9140862819650313,\n",
       "  'outer_auc': array([0.89732358, 0.90997706]),\n",
       "  'inner_auc': array([[0.89732358],\n",
       "         [0.90997706]]),\n",
       "  'split_auc': array([0.89564018, 0.90860802])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.3200797244458977,\n",
       "   'lambda_sparse': 1.0788797342980058e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.20931103669844459,\n",
       "   'n_a': 51,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 9,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 51},\n",
       "  'auc_test': 0.8357607209226443,\n",
       "  'outer_auc': array([0.76147729, 0.82988707]),\n",
       "  'inner_auc': array([[0.76147729],\n",
       "         [0.82988707]]),\n",
       "  'split_auc': array([0.76104664, 0.83057678])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.7417055984357774,\n",
       "   'lambda_sparse': 9.666547757849907e-06,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.37309970839416084,\n",
       "   'n_a': 33,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 33},\n",
       "  'auc_test': 0.9112405649515349,\n",
       "  'outer_auc': array([0.90453866, 0.89715097]),\n",
       "  'inner_auc': array([[0.90453866],\n",
       "         [0.89715097]]),\n",
       "  'split_auc': array([0.89690963, 0.89653117])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.155085897662751,\n",
       "   'lambda_sparse': 0.06268544369054335,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 2,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.06419519245270071,\n",
       "   'n_a': 58,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 3,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 58},\n",
       "  'auc_test': 0.8692094571349205,\n",
       "  'outer_auc': array([0.82370807, 0.86659103]),\n",
       "  'inner_auc': array([[0.82370807],\n",
       "         [0.86659103]]),\n",
       "  'split_auc': array([0.82232323, 0.86796283])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.7853513530941887,\n",
       "   'lambda_sparse': 0.060538075963128726,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 2,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.01659081342720583,\n",
       "   'n_a': 57,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 9,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 57},\n",
       "  'auc_test': 0.9082377731332512,\n",
       "  'outer_auc': array([0.89216734, 0.90450957]),\n",
       "  'inner_auc': array([[0.89216734],\n",
       "         [0.90450957]]),\n",
       "  'split_auc': array([0.88856632, 0.90419506])},\n",
       " {'params': {'batch_size': 8192,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.4483352732230865,\n",
       "   'lambda_sparse': 9.88841326468412e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 20,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.14870713670612168,\n",
       "   'n_a': 34,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 0,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 34},\n",
       "  'auc_test': 0.8702921576731173,\n",
       "  'outer_auc': array([0.73057123, 0.88287395]),\n",
       "  'inner_auc': array([[0.73057123],\n",
       "         [0.88287395]]),\n",
       "  'split_auc': array([0.7091208 , 0.87753355])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.6165723347539693,\n",
       "   'lambda_sparse': 1.4599142759978118e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.10094024858241127,\n",
       "   'n_a': 23,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 23},\n",
       "  'auc_test': 0.9008114273420864,\n",
       "  'outer_auc': array([0.89049852, 0.88293981]),\n",
       "  'inner_auc': array([[0.89049852],\n",
       "         [0.88293981]]),\n",
       "  'split_auc': array([0.88428337, 0.88482633])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.8176874368254765,\n",
       "   'lambda_sparse': 0.011956913676401462,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.014470420874060181,\n",
       "   'n_a': 44,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 44},\n",
       "  'auc_test': 0.9100782343984768,\n",
       "  'outer_auc': array([0.91551085, 0.8691434 ]),\n",
       "  'inner_auc': array([[0.91551085],\n",
       "         [0.8691434 ]]),\n",
       "  'split_auc': array([0.91296126, 0.87156373])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.0666092530933924,\n",
       "   'lambda_sparse': 0.062361077330484585,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.05677230869378823,\n",
       "   'n_a': 56,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 56},\n",
       "  'auc_test': 0.9000876473855801,\n",
       "  'outer_auc': array([0.88628945, 0.88643706]),\n",
       "  'inner_auc': array([[0.88628945],\n",
       "         [0.88643706]]),\n",
       "  'split_auc': array([0.89388619, 0.88606083])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.8585828346054276,\n",
       "   'lambda_sparse': 3.148289428715468e-06,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.014699791465777386,\n",
       "   'n_a': 45,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 3,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 45},\n",
       "  'auc_test': 0.9041737252568758,\n",
       "  'outer_auc': array([0.89237549, 0.89197667]),\n",
       "  'inner_auc': array([[0.89237549],\n",
       "         [0.89197667]]),\n",
       "  'split_auc': array([0.88671574, 0.89063126])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.2289294129537485,\n",
       "   'lambda_sparse': 1.463433381960151e-06,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 20,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.07442156568746662,\n",
       "   'n_a': 45,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 6,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 45},\n",
       "  'auc_test': 0.9120609653081635,\n",
       "  'outer_auc': array([0.89949563, 0.90217729]),\n",
       "  'inner_auc': array([[0.89949563],\n",
       "         [0.90217729]]),\n",
       "  'split_auc': array([0.89411376, 0.89717086])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.2621104624305155,\n",
       "   'lambda_sparse': 1.1486325376024042e-06,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.46534641455915526,\n",
       "   'n_a': 16,\n",
       "   'n_independent': 2,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 7,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 16},\n",
       "  'auc_test': 0.9113384713931263,\n",
       "  'outer_auc': array([0.89223014, 0.9050142 ]),\n",
       "  'inner_auc': array([[0.89223014],\n",
       "         [0.9050142 ]]),\n",
       "  'split_auc': array([0.8863445 , 0.90820711])},\n",
       " {'params': {'batch_size': 1024,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.2775235147330584,\n",
       "   'lambda_sparse': 0.00028283082005521094,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 2,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.042208016096625854,\n",
       "   'n_a': 20,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 2,\n",
       "   'n_steps': 8,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 20},\n",
       "  'auc_test': 0.9041107092248947,\n",
       "  'outer_auc': array([0.86469136, 0.90034745]),\n",
       "  'inner_auc': array([[0.86469136],\n",
       "         [0.90034745]]),\n",
       "  'split_auc': array([0.85949817, 0.90077962])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.7614584899852153,\n",
       "   'lambda_sparse': 0.001477933119821969,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 10,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.07566441065172122,\n",
       "   'n_a': 18,\n",
       "   'n_independent': 3,\n",
       "   'n_shared': 1,\n",
       "   'n_steps': 5,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 18},\n",
       "  'auc_test': 0.8979360372742078,\n",
       "  'outer_auc': array([0.87703294, 0.89587897]),\n",
       "  'inner_auc': array([[0.87703294],\n",
       "         [0.89587897]]),\n",
       "  'split_auc': array([0.87109854, 0.89126166])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.8942876809916727,\n",
       "   'lambda_sparse': 0.06990612118305053,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 50,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.3728129919348172,\n",
       "   'n_a': 32,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 5,\n",
       "   'n_steps': 10,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 32},\n",
       "  'auc_test': 0.853584691079456,\n",
       "  'outer_auc': array([0.82067712, 0.78963729]),\n",
       "  'inner_auc': array([[0.82067712],\n",
       "         [0.78963729]]),\n",
       "  'split_auc': array([0.81825205, 0.79014098])},\n",
       " {'params': {'batch_size': 4096,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.5144669709427867,\n",
       "   'lambda_sparse': 8.105858835503054e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 2,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.34473080583208465,\n",
       "   'n_a': 15,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 0,\n",
       "   'n_steps': 6,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 15},\n",
       "  'auc_test': 0.9062446597912172,\n",
       "  'outer_auc': array([0.88437407, 0.90563438]),\n",
       "  'inner_auc': array([[0.88437407],\n",
       "         [0.90563438]]),\n",
       "  'split_auc': array([0.88138426, 0.9065405 ])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.4668405109361926,\n",
       "   'lambda_sparse': 0.0028108736569220014,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 1,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.36098712118593945,\n",
       "   'n_a': 31,\n",
       "   'n_independent': 5,\n",
       "   'n_shared': 1,\n",
       "   'n_steps': 4,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 31},\n",
       "  'auc_test': 0.9119130496279848,\n",
       "  'outer_auc': array([0.88787597, 0.91282614]),\n",
       "  'inner_auc': array([[0.88787597],\n",
       "         [0.91282614]]),\n",
       "  'split_auc': array([0.88129037, 0.91557651])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.497576515080266,\n",
       "   'lambda_sparse': 1.5453832925710306e-05,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 5,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.07180721624973735,\n",
       "   'n_a': 11,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 4,\n",
       "   'n_steps': 7,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 128,\n",
       "   'n_d': 11},\n",
       "  'auc_test': 0.8995198653048263,\n",
       "  'outer_auc': array([0.88040098, 0.89242449]),\n",
       "  'inner_auc': array([[0.88040098],\n",
       "         [0.89242449]]),\n",
       "  'split_auc': array([0.87327696, 0.89167913])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 2.004778914978523,\n",
       "   'lambda_sparse': 0.05163188710850387,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 5,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.2975080250343681,\n",
       "   'n_a': 37,\n",
       "   'n_independent': 4,\n",
       "   'n_shared': 2,\n",
       "   'n_steps': 6,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 512,\n",
       "   'n_d': 37},\n",
       "  'auc_test': 0.8725768364818556,\n",
       "  'outer_auc': array([0.83047637, 0.86389725]),\n",
       "  'inner_auc': array([[0.83047637],\n",
       "         [0.86389725]]),\n",
       "  'split_auc': array([0.82375344, 0.86239818])},\n",
       " {'params': {'batch_size': 2048,\n",
       "   'clip_value': 1,\n",
       "   'drop_last': False,\n",
       "   'emb_generator': <function __main__.log_emb_generator(cat_dim_list, max_dim)>,\n",
       "   'gamma': 1.263704076325905,\n",
       "   'lambda_sparse': 0.0034640752162101687,\n",
       "   'lr': 0.1,\n",
       "   'max_emd_dims': 20,\n",
       "   'max_epochs': 1000,\n",
       "   'momentum': 0.013194885542909613,\n",
       "   'n_a': 60,\n",
       "   'n_independent': 1,\n",
       "   'n_shared': 2,\n",
       "   'n_steps': 8,\n",
       "   'num_workers': 12,\n",
       "   'patience': 5,\n",
       "   'verbose': 1,\n",
       "   'virtual_batch_size': 256,\n",
       "   'n_d': 60},\n",
       "  'auc_test': 0.893258714907065,\n",
       "  'outer_auc': array([0.90293224, 0.81521748]),\n",
       "  'inner_auc': array([[0.90293224],\n",
       "         [0.81521748]]),\n",
       "  'split_auc': array([0.89908367, 0.80745883])}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"agg_results\";\n",
       "                var nbb_formatted_code = \"agg_results\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>auc_test</th>\n",
       "      <th>outer_auc_mean</th>\n",
       "      <th>outer_auc_max</th>\n",
       "      <th>outer_auc_min</th>\n",
       "      <th>outer_auc_std</th>\n",
       "      <th>split_auc_mean</th>\n",
       "      <th>split_auc_max</th>\n",
       "      <th>split_auc_min</th>\n",
       "      <th>split_auc_std</th>\n",
       "      <th>inner_auc_mean</th>\n",
       "      <th>inner_auc_max</th>\n",
       "      <th>inner_auc_min</th>\n",
       "      <th>inner_auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.925806</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.924191</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.920260</td>\n",
       "      <td>0.922063</td>\n",
       "      <td>0.918457</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.924191</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.925298</td>\n",
       "      <td>0.920686</td>\n",
       "      <td>0.922044</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.922803</td>\n",
       "      <td>0.924874</td>\n",
       "      <td>0.920733</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.920686</td>\n",
       "      <td>0.922044</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.001569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.923642</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.920491</td>\n",
       "      <td>0.914308</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.916278</td>\n",
       "      <td>0.917346</td>\n",
       "      <td>0.915210</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.920491</td>\n",
       "      <td>0.914308</td>\n",
       "      <td>0.003570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.922637</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.916212</td>\n",
       "      <td>0.914497</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.916081</td>\n",
       "      <td>0.920680</td>\n",
       "      <td>0.911481</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.916212</td>\n",
       "      <td>0.914497</td>\n",
       "      <td>0.000990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.921805</td>\n",
       "      <td>0.914663</td>\n",
       "      <td>0.916327</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.913250</td>\n",
       "      <td>0.914964</td>\n",
       "      <td>0.911536</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.914663</td>\n",
       "      <td>0.916327</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  auc_test  \\\n",
       "0  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.925806   \n",
       "1  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.925298   \n",
       "2  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.923642   \n",
       "3  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.922637   \n",
       "4  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.921805   \n",
       "\n",
       "   outer_auc_mean  outer_auc_max  outer_auc_min  outer_auc_std  \\\n",
       "0        0.922460       0.924191       0.920730       0.002448   \n",
       "1        0.920686       0.922044       0.919327       0.001921   \n",
       "2        0.917400       0.920491       0.914308       0.004372   \n",
       "3        0.915354       0.916212       0.914497       0.001212   \n",
       "4        0.914663       0.916327       0.912998       0.002354   \n",
       "\n",
       "   split_auc_mean  split_auc_max  split_auc_min  split_auc_std  \\\n",
       "0        0.920260       0.922063       0.918457       0.002550   \n",
       "1        0.922803       0.924874       0.920733       0.002928   \n",
       "2        0.916278       0.917346       0.915210       0.001510   \n",
       "3        0.916081       0.920680       0.911481       0.006505   \n",
       "4        0.913250       0.914964       0.911536       0.002424   \n",
       "\n",
       "   inner_auc_mean  inner_auc_max  inner_auc_min  inner_auc_std  \n",
       "0        0.922460       0.924191       0.920730       0.001999  \n",
       "1        0.920686       0.922044       0.919327       0.001569  \n",
       "2        0.917400       0.920491       0.914308       0.003570  \n",
       "3        0.915354       0.916212       0.914497       0.000990  \n",
       "4        0.914663       0.916327       0.912998       0.001922  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"df = pd.DataFrame(agg_results).reset_index().rename(columns={\\\"index\\\": \\\"params_idx\\\"})\\n\\nouter_agg = df[[\\\"params_idx\\\", \\\"outer_auc\\\"]].explode(\\\"outer_auc\\\")\\nouter_agg[\\\"outer_auc\\\"] = outer_agg[\\\"outer_auc\\\"].astype(\\\"float\\\")\\nouter_agg = outer_agg.groupby(\\\"params_idx\\\").agg(\\n    {\\\"outer_auc\\\": [\\\"mean\\\", \\\"max\\\", \\\"min\\\", \\\"std\\\"]}\\n)\\nouter_agg.columns = [\\\"_\\\".join(col).rstrip(\\\"_\\\") for col in outer_agg.columns.values]\\nouter_agg.head()\\n\\nsplit_agg = df[[\\\"params_idx\\\", \\\"split_auc\\\"]].explode(\\\"split_auc\\\")\\nsplit_agg[\\\"split_auc\\\"] = split_agg[\\\"split_auc\\\"].astype(\\\"float\\\")\\nsplit_agg = split_agg.groupby(\\\"params_idx\\\").agg(\\n    {\\\"split_auc\\\": [\\\"mean\\\", \\\"max\\\", \\\"min\\\", \\\"std\\\"]}\\n)\\nsplit_agg.columns = [\\\"_\\\".join(col).rstrip(\\\"_\\\") for col in split_agg.columns.values]\\nsplit_agg.head()\\n\\ninner_agg = df[[\\\"params_idx\\\", \\\"inner_auc\\\"]].explode(\\\"inner_auc\\\").explode(\\\"inner_auc\\\")\\ninner_agg[\\\"inner_auc\\\"] = inner_agg[\\\"inner_auc\\\"].astype(\\\"float\\\")\\ninner_agg = inner_agg.groupby(\\\"params_idx\\\").agg(\\n    {\\\"inner_auc\\\": [\\\"mean\\\", \\\"max\\\", \\\"min\\\", \\\"std\\\"]}\\n)\\ninner_agg.columns = [\\\"_\\\".join(col).rstrip(\\\"_\\\") for col in inner_agg.columns.values]\\ninner_agg.head()\\n\\ndf = (\\n    df.merge(outer_agg, on=\\\"params_idx\\\")\\n    .merge(split_agg, on=\\\"params_idx\\\")\\n    .merge(inner_agg, on=\\\"params_idx\\\")\\n    .drop(columns=[\\\"params_idx\\\", \\\"outer_auc\\\", \\\"inner_auc\\\", \\\"split_auc\\\"])\\n)\\ndf = df.sort_values(by=[\\\"auc_test\\\"], ascending=False).reset_index(drop=True)\\ndf.head()\";\n",
       "                var nbb_formatted_code = \"df = pd.DataFrame(agg_results).reset_index().rename(columns={\\\"index\\\": \\\"params_idx\\\"})\\n\\nouter_agg = df[[\\\"params_idx\\\", \\\"outer_auc\\\"]].explode(\\\"outer_auc\\\")\\nouter_agg[\\\"outer_auc\\\"] = outer_agg[\\\"outer_auc\\\"].astype(\\\"float\\\")\\nouter_agg = outer_agg.groupby(\\\"params_idx\\\").agg(\\n    {\\\"outer_auc\\\": [\\\"mean\\\", \\\"max\\\", \\\"min\\\", \\\"std\\\"]}\\n)\\nouter_agg.columns = [\\\"_\\\".join(col).rstrip(\\\"_\\\") for col in outer_agg.columns.values]\\nouter_agg.head()\\n\\nsplit_agg = df[[\\\"params_idx\\\", \\\"split_auc\\\"]].explode(\\\"split_auc\\\")\\nsplit_agg[\\\"split_auc\\\"] = split_agg[\\\"split_auc\\\"].astype(\\\"float\\\")\\nsplit_agg = split_agg.groupby(\\\"params_idx\\\").agg(\\n    {\\\"split_auc\\\": [\\\"mean\\\", \\\"max\\\", \\\"min\\\", \\\"std\\\"]}\\n)\\nsplit_agg.columns = [\\\"_\\\".join(col).rstrip(\\\"_\\\") for col in split_agg.columns.values]\\nsplit_agg.head()\\n\\ninner_agg = df[[\\\"params_idx\\\", \\\"inner_auc\\\"]].explode(\\\"inner_auc\\\").explode(\\\"inner_auc\\\")\\ninner_agg[\\\"inner_auc\\\"] = inner_agg[\\\"inner_auc\\\"].astype(\\\"float\\\")\\ninner_agg = inner_agg.groupby(\\\"params_idx\\\").agg(\\n    {\\\"inner_auc\\\": [\\\"mean\\\", \\\"max\\\", \\\"min\\\", \\\"std\\\"]}\\n)\\ninner_agg.columns = [\\\"_\\\".join(col).rstrip(\\\"_\\\") for col in inner_agg.columns.values]\\ninner_agg.head()\\n\\ndf = (\\n    df.merge(outer_agg, on=\\\"params_idx\\\")\\n    .merge(split_agg, on=\\\"params_idx\\\")\\n    .merge(inner_agg, on=\\\"params_idx\\\")\\n    .drop(columns=[\\\"params_idx\\\", \\\"outer_auc\\\", \\\"inner_auc\\\", \\\"split_auc\\\"])\\n)\\ndf = df.sort_values(by=[\\\"auc_test\\\"], ascending=False).reset_index(drop=True)\\ndf.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(agg_results).reset_index().rename(columns={\"index\": \"params_idx\"})\n",
    "\n",
    "outer_agg = df[[\"params_idx\", \"outer_auc\"]].explode(\"outer_auc\")\n",
    "outer_agg[\"outer_auc\"] = outer_agg[\"outer_auc\"].astype(\"float\")\n",
    "outer_agg = outer_agg.groupby(\"params_idx\").agg(\n",
    "    {\"outer_auc\": [\"mean\", \"max\", \"min\", \"std\"]}\n",
    ")\n",
    "outer_agg.columns = [\"_\".join(col).rstrip(\"_\") for col in outer_agg.columns.values]\n",
    "outer_agg.head()\n",
    "\n",
    "split_agg = df[[\"params_idx\", \"split_auc\"]].explode(\"split_auc\")\n",
    "split_agg[\"split_auc\"] = split_agg[\"split_auc\"].astype(\"float\")\n",
    "split_agg = split_agg.groupby(\"params_idx\").agg(\n",
    "    {\"split_auc\": [\"mean\", \"max\", \"min\", \"std\"]}\n",
    ")\n",
    "split_agg.columns = [\"_\".join(col).rstrip(\"_\") for col in split_agg.columns.values]\n",
    "split_agg.head()\n",
    "\n",
    "inner_agg = df[[\"params_idx\", \"inner_auc\"]].explode(\"inner_auc\").explode(\"inner_auc\")\n",
    "inner_agg[\"inner_auc\"] = inner_agg[\"inner_auc\"].astype(\"float\")\n",
    "inner_agg = inner_agg.groupby(\"params_idx\").agg(\n",
    "    {\"inner_auc\": [\"mean\", \"max\", \"min\", \"std\"]}\n",
    ")\n",
    "inner_agg.columns = [\"_\".join(col).rstrip(\"_\") for col in inner_agg.columns.values]\n",
    "inner_agg.head()\n",
    "\n",
    "df = (\n",
    "    df.merge(outer_agg, on=\"params_idx\")\n",
    "    .merge(split_agg, on=\"params_idx\")\n",
    "    .merge(inner_agg, on=\"params_idx\")\n",
    "    .drop(columns=[\"params_idx\", \"outer_auc\", \"inner_auc\", \"split_auc\"])\n",
    ")\n",
    "df = df.sort_values(by=[\"auc_test\"], ascending=False).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1024,\n",
       " 'clip_value': 1,\n",
       " 'drop_last': False,\n",
       " 'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       " 'gamma': 1.73961618549668,\n",
       " 'lambda_sparse': 5.397700876300653e-06,\n",
       " 'lr': 0.1,\n",
       " 'max_emd_dims': 5,\n",
       " 'max_epochs': 1000,\n",
       " 'momentum': 0.028756461671198243,\n",
       " 'n_a': 42,\n",
       " 'n_independent': 4,\n",
       " 'n_shared': 5,\n",
       " 'n_steps': 2,\n",
       " 'num_workers': 12,\n",
       " 'patience': 5,\n",
       " 'verbose': 1,\n",
       " 'virtual_batch_size': 128,\n",
       " 'n_d': 42}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"df[\\\"params\\\"][0]\";\n",
       "                var nbb_formatted_code = \"df[\\\"params\\\"][0]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"params\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 2048,\n",
       " 'clip_value': 1,\n",
       " 'drop_last': False,\n",
       " 'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       " 'gamma': 1.7112254756999112,\n",
       " 'lambda_sparse': 0.05036858219677769,\n",
       " 'lr': 0.1,\n",
       " 'max_emd_dims': 5,\n",
       " 'max_epochs': 1000,\n",
       " 'momentum': 0.18583213116478564,\n",
       " 'n_a': 9,\n",
       " 'n_independent': 3,\n",
       " 'n_shared': 5,\n",
       " 'n_steps': 2,\n",
       " 'num_workers': 12,\n",
       " 'patience': 5,\n",
       " 'verbose': 1,\n",
       " 'virtual_batch_size': 512,\n",
       " 'n_d': 9}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"df[\\\"params\\\"][1]\";\n",
       "                var nbb_formatted_code = \"df[\\\"params\\\"][1]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"params\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1024,\n",
       " 'clip_value': 1,\n",
       " 'drop_last': False,\n",
       " 'emb_generator': <function __main__.emb_generator(cat_dim_list, max_dim)>,\n",
       " 'gamma': 2.757739483554942,\n",
       " 'lambda_sparse': 0.00028929518644642103,\n",
       " 'lr': 0.1,\n",
       " 'max_emd_dims': 20,\n",
       " 'max_epochs': 1000,\n",
       " 'momentum': 0.22794035656379164,\n",
       " 'n_a': 57,\n",
       " 'n_independent': 2,\n",
       " 'n_shared': 4,\n",
       " 'n_steps': 3,\n",
       " 'num_workers': 12,\n",
       " 'patience': 5,\n",
       " 'verbose': 1,\n",
       " 'virtual_batch_size': 256,\n",
       " 'n_d': 57}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"df[\\\"params\\\"][2]\";\n",
       "                var nbb_formatted_code = \"df[\\\"params\\\"][2]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"params\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"df.to_csv(\\\"tabnet_grid_census.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"df.to_csv(\\\"tabnet_grid_census.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.to_csv(\"tabnet_grid_census.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>auc_test</th>\n",
       "      <th>outer_auc_mean</th>\n",
       "      <th>outer_auc_max</th>\n",
       "      <th>outer_auc_min</th>\n",
       "      <th>outer_auc_std</th>\n",
       "      <th>split_auc_mean</th>\n",
       "      <th>split_auc_max</th>\n",
       "      <th>split_auc_min</th>\n",
       "      <th>split_auc_std</th>\n",
       "      <th>inner_auc_mean</th>\n",
       "      <th>inner_auc_max</th>\n",
       "      <th>inner_auc_min</th>\n",
       "      <th>inner_auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.925806</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.924191</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.920260</td>\n",
       "      <td>0.922063</td>\n",
       "      <td>0.918457</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.924191</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.925298</td>\n",
       "      <td>0.920686</td>\n",
       "      <td>0.922044</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.922803</td>\n",
       "      <td>0.924874</td>\n",
       "      <td>0.920733</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.920686</td>\n",
       "      <td>0.922044</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.001569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.923642</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.920491</td>\n",
       "      <td>0.914308</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.916278</td>\n",
       "      <td>0.917346</td>\n",
       "      <td>0.915210</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.920491</td>\n",
       "      <td>0.914308</td>\n",
       "      <td>0.003570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.922637</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.916212</td>\n",
       "      <td>0.914497</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.916081</td>\n",
       "      <td>0.920680</td>\n",
       "      <td>0.911481</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.916212</td>\n",
       "      <td>0.914497</td>\n",
       "      <td>0.000990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.921805</td>\n",
       "      <td>0.914663</td>\n",
       "      <td>0.916327</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.913250</td>\n",
       "      <td>0.914964</td>\n",
       "      <td>0.911536</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.914663</td>\n",
       "      <td>0.916327</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.911665</td>\n",
       "      <td>0.920194</td>\n",
       "      <td>0.903136</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.912039</td>\n",
       "      <td>0.919941</td>\n",
       "      <td>0.904137</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>0.911665</td>\n",
       "      <td>0.920194</td>\n",
       "      <td>0.903136</td>\n",
       "      <td>0.009849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.919137</td>\n",
       "      <td>0.907132</td>\n",
       "      <td>0.919315</td>\n",
       "      <td>0.894948</td>\n",
       "      <td>0.017230</td>\n",
       "      <td>0.905553</td>\n",
       "      <td>0.915406</td>\n",
       "      <td>0.895699</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>0.907132</td>\n",
       "      <td>0.919315</td>\n",
       "      <td>0.894948</td>\n",
       "      <td>0.014068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.917606</td>\n",
       "      <td>0.907418</td>\n",
       "      <td>0.914936</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.010632</td>\n",
       "      <td>0.905087</td>\n",
       "      <td>0.915956</td>\n",
       "      <td>0.894217</td>\n",
       "      <td>0.015372</td>\n",
       "      <td>0.907418</td>\n",
       "      <td>0.914936</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.008681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.916831</td>\n",
       "      <td>0.902693</td>\n",
       "      <td>0.918672</td>\n",
       "      <td>0.886714</td>\n",
       "      <td>0.022597</td>\n",
       "      <td>0.900001</td>\n",
       "      <td>0.915255</td>\n",
       "      <td>0.884748</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.902693</td>\n",
       "      <td>0.918672</td>\n",
       "      <td>0.886714</td>\n",
       "      <td>0.018451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.916487</td>\n",
       "      <td>0.909845</td>\n",
       "      <td>0.910799</td>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.906228</td>\n",
       "      <td>0.907861</td>\n",
       "      <td>0.904594</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.909845</td>\n",
       "      <td>0.910799</td>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.001102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.915441</td>\n",
       "      <td>0.905192</td>\n",
       "      <td>0.907154</td>\n",
       "      <td>0.903230</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.903721</td>\n",
       "      <td>0.906079</td>\n",
       "      <td>0.901363</td>\n",
       "      <td>0.003335</td>\n",
       "      <td>0.905192</td>\n",
       "      <td>0.907154</td>\n",
       "      <td>0.903230</td>\n",
       "      <td>0.002265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.914086</td>\n",
       "      <td>0.903650</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.897324</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>0.902124</td>\n",
       "      <td>0.908608</td>\n",
       "      <td>0.895640</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>0.903650</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.897324</td>\n",
       "      <td>0.007305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.912061</td>\n",
       "      <td>0.900836</td>\n",
       "      <td>0.902177</td>\n",
       "      <td>0.899496</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.895642</td>\n",
       "      <td>0.897171</td>\n",
       "      <td>0.894114</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.900836</td>\n",
       "      <td>0.902177</td>\n",
       "      <td>0.899496</td>\n",
       "      <td>0.001548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911913</td>\n",
       "      <td>0.900351</td>\n",
       "      <td>0.912826</td>\n",
       "      <td>0.887876</td>\n",
       "      <td>0.017642</td>\n",
       "      <td>0.898433</td>\n",
       "      <td>0.915577</td>\n",
       "      <td>0.881290</td>\n",
       "      <td>0.024244</td>\n",
       "      <td>0.900351</td>\n",
       "      <td>0.912826</td>\n",
       "      <td>0.887876</td>\n",
       "      <td>0.014405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911699</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>0.883868</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.887902</td>\n",
       "      <td>0.901040</td>\n",
       "      <td>0.874765</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>0.883868</td>\n",
       "      <td>0.010883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911436</td>\n",
       "      <td>0.889300</td>\n",
       "      <td>0.920258</td>\n",
       "      <td>0.858343</td>\n",
       "      <td>0.043780</td>\n",
       "      <td>0.888865</td>\n",
       "      <td>0.919061</td>\n",
       "      <td>0.858668</td>\n",
       "      <td>0.042704</td>\n",
       "      <td>0.889300</td>\n",
       "      <td>0.920258</td>\n",
       "      <td>0.858343</td>\n",
       "      <td>0.035746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911338</td>\n",
       "      <td>0.898622</td>\n",
       "      <td>0.905014</td>\n",
       "      <td>0.892230</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>0.897276</td>\n",
       "      <td>0.908207</td>\n",
       "      <td>0.886344</td>\n",
       "      <td>0.015459</td>\n",
       "      <td>0.898622</td>\n",
       "      <td>0.905014</td>\n",
       "      <td>0.892230</td>\n",
       "      <td>0.007381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911332</td>\n",
       "      <td>0.903197</td>\n",
       "      <td>0.906559</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.898943</td>\n",
       "      <td>0.902235</td>\n",
       "      <td>0.895650</td>\n",
       "      <td>0.004656</td>\n",
       "      <td>0.903197</td>\n",
       "      <td>0.906559</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911241</td>\n",
       "      <td>0.900845</td>\n",
       "      <td>0.904539</td>\n",
       "      <td>0.897151</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.896720</td>\n",
       "      <td>0.896910</td>\n",
       "      <td>0.896531</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.900845</td>\n",
       "      <td>0.904539</td>\n",
       "      <td>0.897151</td>\n",
       "      <td>0.004265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.910078</td>\n",
       "      <td>0.892327</td>\n",
       "      <td>0.915511</td>\n",
       "      <td>0.869143</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.892262</td>\n",
       "      <td>0.912961</td>\n",
       "      <td>0.871564</td>\n",
       "      <td>0.029272</td>\n",
       "      <td>0.892327</td>\n",
       "      <td>0.915511</td>\n",
       "      <td>0.869143</td>\n",
       "      <td>0.026770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.908771</td>\n",
       "      <td>0.896917</td>\n",
       "      <td>0.907133</td>\n",
       "      <td>0.886702</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.893134</td>\n",
       "      <td>0.904587</td>\n",
       "      <td>0.881681</td>\n",
       "      <td>0.016197</td>\n",
       "      <td>0.896917</td>\n",
       "      <td>0.907133</td>\n",
       "      <td>0.886702</td>\n",
       "      <td>0.011796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.908315</td>\n",
       "      <td>0.858327</td>\n",
       "      <td>0.916067</td>\n",
       "      <td>0.800587</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>0.858829</td>\n",
       "      <td>0.917469</td>\n",
       "      <td>0.800189</td>\n",
       "      <td>0.082929</td>\n",
       "      <td>0.858327</td>\n",
       "      <td>0.916067</td>\n",
       "      <td>0.800587</td>\n",
       "      <td>0.066673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.908238</td>\n",
       "      <td>0.898338</td>\n",
       "      <td>0.904510</td>\n",
       "      <td>0.892167</td>\n",
       "      <td>0.008727</td>\n",
       "      <td>0.896381</td>\n",
       "      <td>0.904195</td>\n",
       "      <td>0.888566</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>0.898338</td>\n",
       "      <td>0.904510</td>\n",
       "      <td>0.892167</td>\n",
       "      <td>0.007126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.907245</td>\n",
       "      <td>0.891983</td>\n",
       "      <td>0.897953</td>\n",
       "      <td>0.886012</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.889079</td>\n",
       "      <td>0.899524</td>\n",
       "      <td>0.878634</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.891983</td>\n",
       "      <td>0.897953</td>\n",
       "      <td>0.886012</td>\n",
       "      <td>0.006894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.906245</td>\n",
       "      <td>0.895004</td>\n",
       "      <td>0.905634</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.893962</td>\n",
       "      <td>0.906541</td>\n",
       "      <td>0.881384</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>0.895004</td>\n",
       "      <td>0.905634</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.012275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.905639</td>\n",
       "      <td>0.895997</td>\n",
       "      <td>0.903174</td>\n",
       "      <td>0.888821</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>0.893792</td>\n",
       "      <td>0.905847</td>\n",
       "      <td>0.881737</td>\n",
       "      <td>0.017048</td>\n",
       "      <td>0.895997</td>\n",
       "      <td>0.903174</td>\n",
       "      <td>0.888821</td>\n",
       "      <td>0.008287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.904174</td>\n",
       "      <td>0.892176</td>\n",
       "      <td>0.892375</td>\n",
       "      <td>0.891977</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.888673</td>\n",
       "      <td>0.890631</td>\n",
       "      <td>0.886716</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.892176</td>\n",
       "      <td>0.892375</td>\n",
       "      <td>0.891977</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.904111</td>\n",
       "      <td>0.882519</td>\n",
       "      <td>0.900347</td>\n",
       "      <td>0.864691</td>\n",
       "      <td>0.025213</td>\n",
       "      <td>0.880139</td>\n",
       "      <td>0.900780</td>\n",
       "      <td>0.859498</td>\n",
       "      <td>0.029190</td>\n",
       "      <td>0.882519</td>\n",
       "      <td>0.900347</td>\n",
       "      <td>0.864691</td>\n",
       "      <td>0.020586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.903543</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.910245</td>\n",
       "      <td>0.873756</td>\n",
       "      <td>0.025802</td>\n",
       "      <td>0.887870</td>\n",
       "      <td>0.904817</td>\n",
       "      <td>0.870924</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.910245</td>\n",
       "      <td>0.873756</td>\n",
       "      <td>0.021067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.902097</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>0.900606</td>\n",
       "      <td>0.868395</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>0.880720</td>\n",
       "      <td>0.896188</td>\n",
       "      <td>0.865252</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>0.900606</td>\n",
       "      <td>0.868395</td>\n",
       "      <td>0.018597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.900811</td>\n",
       "      <td>0.886719</td>\n",
       "      <td>0.890499</td>\n",
       "      <td>0.882940</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.884555</td>\n",
       "      <td>0.884826</td>\n",
       "      <td>0.884283</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.886719</td>\n",
       "      <td>0.890499</td>\n",
       "      <td>0.882940</td>\n",
       "      <td>0.004364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.900750</td>\n",
       "      <td>0.890413</td>\n",
       "      <td>0.896148</td>\n",
       "      <td>0.884678</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.888901</td>\n",
       "      <td>0.897802</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.890413</td>\n",
       "      <td>0.896148</td>\n",
       "      <td>0.884678</td>\n",
       "      <td>0.006622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.900088</td>\n",
       "      <td>0.886363</td>\n",
       "      <td>0.886437</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.889974</td>\n",
       "      <td>0.893886</td>\n",
       "      <td>0.886061</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.886363</td>\n",
       "      <td>0.886437</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.899520</td>\n",
       "      <td>0.886413</td>\n",
       "      <td>0.892424</td>\n",
       "      <td>0.880401</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>0.873277</td>\n",
       "      <td>0.013012</td>\n",
       "      <td>0.886413</td>\n",
       "      <td>0.892424</td>\n",
       "      <td>0.880401</td>\n",
       "      <td>0.006942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.898748</td>\n",
       "      <td>0.885711</td>\n",
       "      <td>0.888505</td>\n",
       "      <td>0.882917</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.880969</td>\n",
       "      <td>0.887067</td>\n",
       "      <td>0.874870</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>0.885711</td>\n",
       "      <td>0.888505</td>\n",
       "      <td>0.882917</td>\n",
       "      <td>0.003226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.898252</td>\n",
       "      <td>0.867775</td>\n",
       "      <td>0.904903</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>0.052508</td>\n",
       "      <td>0.860900</td>\n",
       "      <td>0.896432</td>\n",
       "      <td>0.825367</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.867775</td>\n",
       "      <td>0.904903</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>0.042872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.897936</td>\n",
       "      <td>0.886456</td>\n",
       "      <td>0.895879</td>\n",
       "      <td>0.877033</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.881180</td>\n",
       "      <td>0.891262</td>\n",
       "      <td>0.871099</td>\n",
       "      <td>0.014257</td>\n",
       "      <td>0.886456</td>\n",
       "      <td>0.895879</td>\n",
       "      <td>0.877033</td>\n",
       "      <td>0.010881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.893936</td>\n",
       "      <td>0.865171</td>\n",
       "      <td>0.896773</td>\n",
       "      <td>0.833568</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.860395</td>\n",
       "      <td>0.893187</td>\n",
       "      <td>0.827602</td>\n",
       "      <td>0.046376</td>\n",
       "      <td>0.865171</td>\n",
       "      <td>0.896773</td>\n",
       "      <td>0.833568</td>\n",
       "      <td>0.036492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.893259</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880210</td>\n",
       "      <td>0.853124</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>0.863293</td>\n",
       "      <td>0.880051</td>\n",
       "      <td>0.846536</td>\n",
       "      <td>0.023699</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880210</td>\n",
       "      <td>0.853124</td>\n",
       "      <td>0.015638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.893259</td>\n",
       "      <td>0.859075</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.062024</td>\n",
       "      <td>0.853271</td>\n",
       "      <td>0.899084</td>\n",
       "      <td>0.807459</td>\n",
       "      <td>0.064789</td>\n",
       "      <td>0.859075</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.050642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.892280</td>\n",
       "      <td>0.865347</td>\n",
       "      <td>0.879487</td>\n",
       "      <td>0.851207</td>\n",
       "      <td>0.019997</td>\n",
       "      <td>0.866982</td>\n",
       "      <td>0.881046</td>\n",
       "      <td>0.852918</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.865347</td>\n",
       "      <td>0.879487</td>\n",
       "      <td>0.851207</td>\n",
       "      <td>0.016327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.890501</td>\n",
       "      <td>0.876252</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.872647</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.876272</td>\n",
       "      <td>0.881036</td>\n",
       "      <td>0.871507</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.876252</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.872647</td>\n",
       "      <td>0.004163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.889792</td>\n",
       "      <td>0.877497</td>\n",
       "      <td>0.881620</td>\n",
       "      <td>0.873375</td>\n",
       "      <td>0.005830</td>\n",
       "      <td>0.873819</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.873496</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.877497</td>\n",
       "      <td>0.881620</td>\n",
       "      <td>0.873375</td>\n",
       "      <td>0.004760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.884630</td>\n",
       "      <td>0.866506</td>\n",
       "      <td>0.870379</td>\n",
       "      <td>0.862633</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.864945</td>\n",
       "      <td>0.866428</td>\n",
       "      <td>0.863461</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.866506</td>\n",
       "      <td>0.870379</td>\n",
       "      <td>0.862633</td>\n",
       "      <td>0.004472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.881357</td>\n",
       "      <td>0.855290</td>\n",
       "      <td>0.869896</td>\n",
       "      <td>0.840684</td>\n",
       "      <td>0.020656</td>\n",
       "      <td>0.850914</td>\n",
       "      <td>0.870166</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.855290</td>\n",
       "      <td>0.869896</td>\n",
       "      <td>0.840684</td>\n",
       "      <td>0.016866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.880056</td>\n",
       "      <td>0.844350</td>\n",
       "      <td>0.885222</td>\n",
       "      <td>0.803478</td>\n",
       "      <td>0.057802</td>\n",
       "      <td>0.843018</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>0.801617</td>\n",
       "      <td>0.058551</td>\n",
       "      <td>0.844350</td>\n",
       "      <td>0.885222</td>\n",
       "      <td>0.803478</td>\n",
       "      <td>0.047195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.872577</td>\n",
       "      <td>0.847187</td>\n",
       "      <td>0.863897</td>\n",
       "      <td>0.830476</td>\n",
       "      <td>0.023632</td>\n",
       "      <td>0.843076</td>\n",
       "      <td>0.862398</td>\n",
       "      <td>0.823753</td>\n",
       "      <td>0.027326</td>\n",
       "      <td>0.847187</td>\n",
       "      <td>0.863897</td>\n",
       "      <td>0.830476</td>\n",
       "      <td>0.019296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.870292</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.882874</td>\n",
       "      <td>0.730571</td>\n",
       "      <td>0.107694</td>\n",
       "      <td>0.793327</td>\n",
       "      <td>0.877534</td>\n",
       "      <td>0.709121</td>\n",
       "      <td>0.119086</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.882874</td>\n",
       "      <td>0.730571</td>\n",
       "      <td>0.087932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.869209</td>\n",
       "      <td>0.845150</td>\n",
       "      <td>0.866591</td>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.030323</td>\n",
       "      <td>0.845143</td>\n",
       "      <td>0.867963</td>\n",
       "      <td>0.822323</td>\n",
       "      <td>0.032272</td>\n",
       "      <td>0.845150</td>\n",
       "      <td>0.866591</td>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.024758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.866907</td>\n",
       "      <td>0.843008</td>\n",
       "      <td>0.880992</td>\n",
       "      <td>0.805024</td>\n",
       "      <td>0.053717</td>\n",
       "      <td>0.841068</td>\n",
       "      <td>0.875662</td>\n",
       "      <td>0.806475</td>\n",
       "      <td>0.048923</td>\n",
       "      <td>0.843008</td>\n",
       "      <td>0.880992</td>\n",
       "      <td>0.805024</td>\n",
       "      <td>0.043860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.861253</td>\n",
       "      <td>0.756632</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.630003</td>\n",
       "      <td>0.179080</td>\n",
       "      <td>0.749193</td>\n",
       "      <td>0.876358</td>\n",
       "      <td>0.622027</td>\n",
       "      <td>0.179839</td>\n",
       "      <td>0.756632</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.630003</td>\n",
       "      <td>0.146218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.861025</td>\n",
       "      <td>0.836118</td>\n",
       "      <td>0.844347</td>\n",
       "      <td>0.827889</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.831140</td>\n",
       "      <td>0.835577</td>\n",
       "      <td>0.826704</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.836118</td>\n",
       "      <td>0.844347</td>\n",
       "      <td>0.827889</td>\n",
       "      <td>0.009502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.853585</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.820677</td>\n",
       "      <td>0.789637</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>0.804197</td>\n",
       "      <td>0.818252</td>\n",
       "      <td>0.790141</td>\n",
       "      <td>0.019878</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.820677</td>\n",
       "      <td>0.789637</td>\n",
       "      <td>0.017921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.850021</td>\n",
       "      <td>0.831989</td>\n",
       "      <td>0.833729</td>\n",
       "      <td>0.830248</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.826592</td>\n",
       "      <td>0.831330</td>\n",
       "      <td>0.821854</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.831989</td>\n",
       "      <td>0.833729</td>\n",
       "      <td>0.830248</td>\n",
       "      <td>0.002010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.846501</td>\n",
       "      <td>0.826380</td>\n",
       "      <td>0.831962</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.825727</td>\n",
       "      <td>0.826464</td>\n",
       "      <td>0.824991</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.826380</td>\n",
       "      <td>0.831962</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.006445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.835761</td>\n",
       "      <td>0.795682</td>\n",
       "      <td>0.829887</td>\n",
       "      <td>0.761477</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.795812</td>\n",
       "      <td>0.830577</td>\n",
       "      <td>0.761047</td>\n",
       "      <td>0.049165</td>\n",
       "      <td>0.795682</td>\n",
       "      <td>0.829887</td>\n",
       "      <td>0.761477</td>\n",
       "      <td>0.039496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.780025</td>\n",
       "      <td>0.719180</td>\n",
       "      <td>0.782917</td>\n",
       "      <td>0.655442</td>\n",
       "      <td>0.090138</td>\n",
       "      <td>0.716803</td>\n",
       "      <td>0.780993</td>\n",
       "      <td>0.652613</td>\n",
       "      <td>0.090778</td>\n",
       "      <td>0.719180</td>\n",
       "      <td>0.782917</td>\n",
       "      <td>0.655442</td>\n",
       "      <td>0.073598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.707696</td>\n",
       "      <td>0.700829</td>\n",
       "      <td>0.785834</td>\n",
       "      <td>0.615824</td>\n",
       "      <td>0.120215</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.784131</td>\n",
       "      <td>0.618466</td>\n",
       "      <td>0.117143</td>\n",
       "      <td>0.700829</td>\n",
       "      <td>0.785834</td>\n",
       "      <td>0.615824</td>\n",
       "      <td>0.098155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.686786</td>\n",
       "      <td>0.652929</td>\n",
       "      <td>0.653418</td>\n",
       "      <td>0.652441</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.648247</td>\n",
       "      <td>0.648409</td>\n",
       "      <td>0.648086</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.652929</td>\n",
       "      <td>0.653418</td>\n",
       "      <td>0.652441</td>\n",
       "      <td>0.000564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.672109</td>\n",
       "      <td>0.620360</td>\n",
       "      <td>0.661140</td>\n",
       "      <td>0.579580</td>\n",
       "      <td>0.057672</td>\n",
       "      <td>0.621661</td>\n",
       "      <td>0.661012</td>\n",
       "      <td>0.582311</td>\n",
       "      <td>0.055650</td>\n",
       "      <td>0.620360</td>\n",
       "      <td>0.661140</td>\n",
       "      <td>0.579580</td>\n",
       "      <td>0.047089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  auc_test  \\\n",
       "0   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.925806   \n",
       "1   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.925298   \n",
       "2   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.923642   \n",
       "3   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.922637   \n",
       "4   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.921805   \n",
       "5   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.921788   \n",
       "6   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.919137   \n",
       "7   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.917606   \n",
       "8   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.916831   \n",
       "9   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.916487   \n",
       "10  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.915441   \n",
       "11  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.914086   \n",
       "12  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.912061   \n",
       "13  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.911913   \n",
       "14  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.911699   \n",
       "15  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.911436   \n",
       "16  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.911338   \n",
       "17  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.911332   \n",
       "18  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.911241   \n",
       "19  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.910078   \n",
       "20  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.908771   \n",
       "21  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.908315   \n",
       "22  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.908238   \n",
       "23  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.907245   \n",
       "24  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.906245   \n",
       "25  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.905639   \n",
       "26  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.904174   \n",
       "27  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.904111   \n",
       "28  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.903543   \n",
       "29  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.902097   \n",
       "30  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.900811   \n",
       "31  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.900750   \n",
       "32  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.900088   \n",
       "33  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.899520   \n",
       "34  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.898748   \n",
       "35  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.898252   \n",
       "36  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.897936   \n",
       "37  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.893936   \n",
       "38  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.893259   \n",
       "39  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.893259   \n",
       "40  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.892280   \n",
       "41  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.890501   \n",
       "42  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.889792   \n",
       "43  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.884630   \n",
       "44  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.881357   \n",
       "45  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.880056   \n",
       "46  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.872577   \n",
       "47  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.870292   \n",
       "48  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.869209   \n",
       "49  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.866907   \n",
       "50  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.861253   \n",
       "51  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.861025   \n",
       "52  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.853585   \n",
       "53  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.850021   \n",
       "54  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.846501   \n",
       "55  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.835761   \n",
       "56  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.780025   \n",
       "57  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.707696   \n",
       "58  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.686786   \n",
       "59  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.672109   \n",
       "\n",
       "    outer_auc_mean  outer_auc_max  outer_auc_min  outer_auc_std  \\\n",
       "0         0.922460       0.924191       0.920730       0.002448   \n",
       "1         0.920686       0.922044       0.919327       0.001921   \n",
       "2         0.917400       0.920491       0.914308       0.004372   \n",
       "3         0.915354       0.916212       0.914497       0.001212   \n",
       "4         0.914663       0.916327       0.912998       0.002354   \n",
       "5         0.911665       0.920194       0.903136       0.012062   \n",
       "6         0.907132       0.919315       0.894948       0.017230   \n",
       "7         0.907418       0.914936       0.899900       0.010632   \n",
       "8         0.902693       0.918672       0.886714       0.022597   \n",
       "9         0.909845       0.910799       0.908891       0.001349   \n",
       "10        0.905192       0.907154       0.903230       0.002774   \n",
       "11        0.903650       0.909977       0.897324       0.008947   \n",
       "12        0.900836       0.902177       0.899496       0.001896   \n",
       "13        0.900351       0.912826       0.887876       0.017642   \n",
       "14        0.893293       0.902717       0.883868       0.013329   \n",
       "15        0.889300       0.920258       0.858343       0.043780   \n",
       "16        0.898622       0.905014       0.892230       0.009040   \n",
       "17        0.903197       0.906559       0.899835       0.004755   \n",
       "18        0.900845       0.904539       0.897151       0.005224   \n",
       "19        0.892327       0.915511       0.869143       0.032787   \n",
       "20        0.896917       0.907133       0.886702       0.014447   \n",
       "21        0.858327       0.916067       0.800587       0.081657   \n",
       "22        0.898338       0.904510       0.892167       0.008727   \n",
       "23        0.891983       0.897953       0.886012       0.008443   \n",
       "24        0.895004       0.905634       0.884374       0.015033   \n",
       "25        0.895997       0.903174       0.888821       0.010149   \n",
       "26        0.892176       0.892375       0.891977       0.000282   \n",
       "27        0.882519       0.900347       0.864691       0.025213   \n",
       "28        0.892000       0.910245       0.873756       0.025802   \n",
       "29        0.884500       0.900606       0.868395       0.022777   \n",
       "30        0.886719       0.890499       0.882940       0.005345   \n",
       "31        0.890413       0.896148       0.884678       0.008110   \n",
       "32        0.886363       0.886437       0.886289       0.000104   \n",
       "33        0.886413       0.892424       0.880401       0.008502   \n",
       "34        0.885711       0.888505       0.882917       0.003951   \n",
       "35        0.867775       0.904903       0.830646       0.052508   \n",
       "36        0.886456       0.895879       0.877033       0.013326   \n",
       "37        0.865171       0.896773       0.833568       0.044693   \n",
       "38        0.866667       0.880210       0.853124       0.019152   \n",
       "39        0.859075       0.902932       0.815217       0.062024   \n",
       "40        0.865347       0.879487       0.851207       0.019997   \n",
       "41        0.876252       0.879857       0.872647       0.005099   \n",
       "42        0.877497       0.881620       0.873375       0.005830   \n",
       "43        0.866506       0.870379       0.862633       0.005477   \n",
       "44        0.855290       0.869896       0.840684       0.020656   \n",
       "45        0.844350       0.885222       0.803478       0.057802   \n",
       "46        0.847187       0.863897       0.830476       0.023632   \n",
       "47        0.806723       0.882874       0.730571       0.107694   \n",
       "48        0.845150       0.866591       0.823708       0.030323   \n",
       "49        0.843008       0.880992       0.805024       0.053717   \n",
       "50        0.756632       0.883260       0.630003       0.179080   \n",
       "51        0.836118       0.844347       0.827889       0.011638   \n",
       "52        0.805157       0.820677       0.789637       0.021948   \n",
       "53        0.831989       0.833729       0.830248       0.002462   \n",
       "54        0.826380       0.831962       0.820798       0.007894   \n",
       "55        0.795682       0.829887       0.761477       0.048373   \n",
       "56        0.719180       0.782917       0.655442       0.090138   \n",
       "57        0.700829       0.785834       0.615824       0.120215   \n",
       "58        0.652929       0.653418       0.652441       0.000691   \n",
       "59        0.620360       0.661140       0.579580       0.057672   \n",
       "\n",
       "    split_auc_mean  split_auc_max  split_auc_min  split_auc_std  \\\n",
       "0         0.920260       0.922063       0.918457       0.002550   \n",
       "1         0.922803       0.924874       0.920733       0.002928   \n",
       "2         0.916278       0.917346       0.915210       0.001510   \n",
       "3         0.916081       0.920680       0.911481       0.006505   \n",
       "4         0.913250       0.914964       0.911536       0.002424   \n",
       "5         0.912039       0.919941       0.904137       0.011176   \n",
       "6         0.905553       0.915406       0.895699       0.013935   \n",
       "7         0.905087       0.915956       0.894217       0.015372   \n",
       "8         0.900001       0.915255       0.884748       0.021571   \n",
       "9         0.906228       0.907861       0.904594       0.002310   \n",
       "10        0.903721       0.906079       0.901363       0.003335   \n",
       "11        0.902124       0.908608       0.895640       0.009170   \n",
       "12        0.895642       0.897171       0.894114       0.002162   \n",
       "13        0.898433       0.915577       0.881290       0.024244   \n",
       "14        0.887902       0.901040       0.874765       0.018579   \n",
       "15        0.888865       0.919061       0.858668       0.042704   \n",
       "16        0.897276       0.908207       0.886344       0.015459   \n",
       "17        0.898943       0.902235       0.895650       0.004656   \n",
       "18        0.896720       0.896910       0.896531       0.000268   \n",
       "19        0.892262       0.912961       0.871564       0.029272   \n",
       "20        0.893134       0.904587       0.881681       0.016197   \n",
       "21        0.858829       0.917469       0.800189       0.082929   \n",
       "22        0.896381       0.904195       0.888566       0.011051   \n",
       "23        0.889079       0.899524       0.878634       0.014772   \n",
       "24        0.893962       0.906541       0.881384       0.017788   \n",
       "25        0.893792       0.905847       0.881737       0.017048   \n",
       "26        0.888673       0.890631       0.886716       0.002769   \n",
       "27        0.880139       0.900780       0.859498       0.029190   \n",
       "28        0.887870       0.904817       0.870924       0.023965   \n",
       "29        0.880720       0.896188       0.865252       0.021875   \n",
       "30        0.884555       0.884826       0.884283       0.000384   \n",
       "31        0.888901       0.897802       0.880000       0.012588   \n",
       "32        0.889974       0.893886       0.886061       0.005533   \n",
       "33        0.882478       0.891679       0.873277       0.013012   \n",
       "34        0.880969       0.887067       0.874870       0.008625   \n",
       "35        0.860900       0.896432       0.825367       0.050251   \n",
       "36        0.881180       0.891262       0.871099       0.014257   \n",
       "37        0.860395       0.893187       0.827602       0.046376   \n",
       "38        0.863293       0.880051       0.846536       0.023699   \n",
       "39        0.853271       0.899084       0.807459       0.064789   \n",
       "40        0.866982       0.881046       0.852918       0.019890   \n",
       "41        0.876272       0.881036       0.871507       0.006738   \n",
       "42        0.873819       0.874143       0.873496       0.000458   \n",
       "43        0.864945       0.866428       0.863461       0.002098   \n",
       "44        0.850914       0.870166       0.831663       0.027226   \n",
       "45        0.843018       0.884420       0.801617       0.058551   \n",
       "46        0.843076       0.862398       0.823753       0.027326   \n",
       "47        0.793327       0.877534       0.709121       0.119086   \n",
       "48        0.845143       0.867963       0.822323       0.032272   \n",
       "49        0.841068       0.875662       0.806475       0.048923   \n",
       "50        0.749193       0.876358       0.622027       0.179839   \n",
       "51        0.831140       0.835577       0.826704       0.006274   \n",
       "52        0.804197       0.818252       0.790141       0.019878   \n",
       "53        0.826592       0.831330       0.821854       0.006700   \n",
       "54        0.825727       0.826464       0.824991       0.001041   \n",
       "55        0.795812       0.830577       0.761047       0.049165   \n",
       "56        0.716803       0.780993       0.652613       0.090778   \n",
       "57        0.701299       0.784131       0.618466       0.117143   \n",
       "58        0.648247       0.648409       0.648086       0.000228   \n",
       "59        0.621661       0.661012       0.582311       0.055650   \n",
       "\n",
       "    inner_auc_mean  inner_auc_max  inner_auc_min  inner_auc_std  \n",
       "0         0.922460       0.924191       0.920730       0.001999  \n",
       "1         0.920686       0.922044       0.919327       0.001569  \n",
       "2         0.917400       0.920491       0.914308       0.003570  \n",
       "3         0.915354       0.916212       0.914497       0.000990  \n",
       "4         0.914663       0.916327       0.912998       0.001922  \n",
       "5         0.911665       0.920194       0.903136       0.009849  \n",
       "6         0.907132       0.919315       0.894948       0.014068  \n",
       "7         0.907418       0.914936       0.899900       0.008681  \n",
       "8         0.902693       0.918672       0.886714       0.018451  \n",
       "9         0.909845       0.910799       0.908891       0.001102  \n",
       "10        0.905192       0.907154       0.903230       0.002265  \n",
       "11        0.903650       0.909977       0.897324       0.007305  \n",
       "12        0.900836       0.902177       0.899496       0.001548  \n",
       "13        0.900351       0.912826       0.887876       0.014405  \n",
       "14        0.893293       0.902717       0.883868       0.010883  \n",
       "15        0.889300       0.920258       0.858343       0.035746  \n",
       "16        0.898622       0.905014       0.892230       0.007381  \n",
       "17        0.903197       0.906559       0.899835       0.003882  \n",
       "18        0.900845       0.904539       0.897151       0.004265  \n",
       "19        0.892327       0.915511       0.869143       0.026770  \n",
       "20        0.896917       0.907133       0.886702       0.011796  \n",
       "21        0.858327       0.916067       0.800587       0.066673  \n",
       "22        0.898338       0.904510       0.892167       0.007126  \n",
       "23        0.891983       0.897953       0.886012       0.006894  \n",
       "24        0.895004       0.905634       0.884374       0.012275  \n",
       "25        0.895997       0.903174       0.888821       0.008287  \n",
       "26        0.892176       0.892375       0.891977       0.000230  \n",
       "27        0.882519       0.900347       0.864691       0.020586  \n",
       "28        0.892000       0.910245       0.873756       0.021067  \n",
       "29        0.884500       0.900606       0.868395       0.018597  \n",
       "30        0.886719       0.890499       0.882940       0.004364  \n",
       "31        0.890413       0.896148       0.884678       0.006622  \n",
       "32        0.886363       0.886437       0.886289       0.000085  \n",
       "33        0.886413       0.892424       0.880401       0.006942  \n",
       "34        0.885711       0.888505       0.882917       0.003226  \n",
       "35        0.867775       0.904903       0.830646       0.042872  \n",
       "36        0.886456       0.895879       0.877033       0.010881  \n",
       "37        0.865171       0.896773       0.833568       0.036492  \n",
       "38        0.866667       0.880210       0.853124       0.015638  \n",
       "39        0.859075       0.902932       0.815217       0.050642  \n",
       "40        0.865347       0.879487       0.851207       0.016327  \n",
       "41        0.876252       0.879857       0.872647       0.004163  \n",
       "42        0.877497       0.881620       0.873375       0.004760  \n",
       "43        0.866506       0.870379       0.862633       0.004472  \n",
       "44        0.855290       0.869896       0.840684       0.016866  \n",
       "45        0.844350       0.885222       0.803478       0.047195  \n",
       "46        0.847187       0.863897       0.830476       0.019296  \n",
       "47        0.806723       0.882874       0.730571       0.087932  \n",
       "48        0.845150       0.866591       0.823708       0.024758  \n",
       "49        0.843008       0.880992       0.805024       0.043860  \n",
       "50        0.756632       0.883260       0.630003       0.146218  \n",
       "51        0.836118       0.844347       0.827889       0.009502  \n",
       "52        0.805157       0.820677       0.789637       0.017921  \n",
       "53        0.831989       0.833729       0.830248       0.002010  \n",
       "54        0.826380       0.831962       0.820798       0.006445  \n",
       "55        0.795682       0.829887       0.761477       0.039496  \n",
       "56        0.719180       0.782917       0.655442       0.073598  \n",
       "57        0.700829       0.785834       0.615824       0.098155  \n",
       "58        0.652929       0.653418       0.652441       0.000564  \n",
       "59        0.620360       0.661140       0.579580       0.047089  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"df\";\n",
       "                var nbb_formatted_code = \"df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>auc_test</th>\n",
       "      <th>outer_auc_mean</th>\n",
       "      <th>outer_auc_max</th>\n",
       "      <th>outer_auc_min</th>\n",
       "      <th>outer_auc_std</th>\n",
       "      <th>split_auc_mean</th>\n",
       "      <th>split_auc_max</th>\n",
       "      <th>split_auc_min</th>\n",
       "      <th>split_auc_std</th>\n",
       "      <th>inner_auc_mean</th>\n",
       "      <th>inner_auc_max</th>\n",
       "      <th>inner_auc_min</th>\n",
       "      <th>inner_auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.900088</td>\n",
       "      <td>0.886363</td>\n",
       "      <td>0.886437</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.889974</td>\n",
       "      <td>0.893886</td>\n",
       "      <td>0.886061</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.886363</td>\n",
       "      <td>0.886437</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.904174</td>\n",
       "      <td>0.892176</td>\n",
       "      <td>0.892375</td>\n",
       "      <td>0.891977</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.888673</td>\n",
       "      <td>0.890631</td>\n",
       "      <td>0.886716</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.892176</td>\n",
       "      <td>0.892375</td>\n",
       "      <td>0.891977</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.686786</td>\n",
       "      <td>0.652929</td>\n",
       "      <td>0.653418</td>\n",
       "      <td>0.652441</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.648247</td>\n",
       "      <td>0.648409</td>\n",
       "      <td>0.648086</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.652929</td>\n",
       "      <td>0.653418</td>\n",
       "      <td>0.652441</td>\n",
       "      <td>0.000564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.922637</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.916212</td>\n",
       "      <td>0.914497</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.916081</td>\n",
       "      <td>0.920680</td>\n",
       "      <td>0.911481</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.916212</td>\n",
       "      <td>0.914497</td>\n",
       "      <td>0.000990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.916487</td>\n",
       "      <td>0.909845</td>\n",
       "      <td>0.910799</td>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.906228</td>\n",
       "      <td>0.907861</td>\n",
       "      <td>0.904594</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.909845</td>\n",
       "      <td>0.910799</td>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.001102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.912061</td>\n",
       "      <td>0.900836</td>\n",
       "      <td>0.902177</td>\n",
       "      <td>0.899496</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.895642</td>\n",
       "      <td>0.897171</td>\n",
       "      <td>0.894114</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.900836</td>\n",
       "      <td>0.902177</td>\n",
       "      <td>0.899496</td>\n",
       "      <td>0.001548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.925298</td>\n",
       "      <td>0.920686</td>\n",
       "      <td>0.922044</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.922803</td>\n",
       "      <td>0.924874</td>\n",
       "      <td>0.920733</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.920686</td>\n",
       "      <td>0.922044</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.001569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.921805</td>\n",
       "      <td>0.914663</td>\n",
       "      <td>0.916327</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.913250</td>\n",
       "      <td>0.914964</td>\n",
       "      <td>0.911536</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.914663</td>\n",
       "      <td>0.916327</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.925806</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.924191</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.920260</td>\n",
       "      <td>0.922063</td>\n",
       "      <td>0.918457</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.924191</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.850021</td>\n",
       "      <td>0.831989</td>\n",
       "      <td>0.833729</td>\n",
       "      <td>0.830248</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.826592</td>\n",
       "      <td>0.831330</td>\n",
       "      <td>0.821854</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.831989</td>\n",
       "      <td>0.833729</td>\n",
       "      <td>0.830248</td>\n",
       "      <td>0.002010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.915441</td>\n",
       "      <td>0.905192</td>\n",
       "      <td>0.907154</td>\n",
       "      <td>0.903230</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.903721</td>\n",
       "      <td>0.906079</td>\n",
       "      <td>0.901363</td>\n",
       "      <td>0.003335</td>\n",
       "      <td>0.905192</td>\n",
       "      <td>0.907154</td>\n",
       "      <td>0.903230</td>\n",
       "      <td>0.002265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.898748</td>\n",
       "      <td>0.885711</td>\n",
       "      <td>0.888505</td>\n",
       "      <td>0.882917</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.880969</td>\n",
       "      <td>0.887067</td>\n",
       "      <td>0.874870</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>0.885711</td>\n",
       "      <td>0.888505</td>\n",
       "      <td>0.882917</td>\n",
       "      <td>0.003226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.923642</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.920491</td>\n",
       "      <td>0.914308</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.916278</td>\n",
       "      <td>0.917346</td>\n",
       "      <td>0.915210</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.920491</td>\n",
       "      <td>0.914308</td>\n",
       "      <td>0.003570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911332</td>\n",
       "      <td>0.903197</td>\n",
       "      <td>0.906559</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.898943</td>\n",
       "      <td>0.902235</td>\n",
       "      <td>0.895650</td>\n",
       "      <td>0.004656</td>\n",
       "      <td>0.903197</td>\n",
       "      <td>0.906559</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.890501</td>\n",
       "      <td>0.876252</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.872647</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.876272</td>\n",
       "      <td>0.881036</td>\n",
       "      <td>0.871507</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.876252</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.872647</td>\n",
       "      <td>0.004163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911241</td>\n",
       "      <td>0.900845</td>\n",
       "      <td>0.904539</td>\n",
       "      <td>0.897151</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.896720</td>\n",
       "      <td>0.896910</td>\n",
       "      <td>0.896531</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.900845</td>\n",
       "      <td>0.904539</td>\n",
       "      <td>0.897151</td>\n",
       "      <td>0.004265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.900811</td>\n",
       "      <td>0.886719</td>\n",
       "      <td>0.890499</td>\n",
       "      <td>0.882940</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.884555</td>\n",
       "      <td>0.884826</td>\n",
       "      <td>0.884283</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.886719</td>\n",
       "      <td>0.890499</td>\n",
       "      <td>0.882940</td>\n",
       "      <td>0.004364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.884630</td>\n",
       "      <td>0.866506</td>\n",
       "      <td>0.870379</td>\n",
       "      <td>0.862633</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.864945</td>\n",
       "      <td>0.866428</td>\n",
       "      <td>0.863461</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.866506</td>\n",
       "      <td>0.870379</td>\n",
       "      <td>0.862633</td>\n",
       "      <td>0.004472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.889792</td>\n",
       "      <td>0.877497</td>\n",
       "      <td>0.881620</td>\n",
       "      <td>0.873375</td>\n",
       "      <td>0.005830</td>\n",
       "      <td>0.873819</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.873496</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.877497</td>\n",
       "      <td>0.881620</td>\n",
       "      <td>0.873375</td>\n",
       "      <td>0.004760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.846501</td>\n",
       "      <td>0.826380</td>\n",
       "      <td>0.831962</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.825727</td>\n",
       "      <td>0.826464</td>\n",
       "      <td>0.824991</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.826380</td>\n",
       "      <td>0.831962</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.006445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.900750</td>\n",
       "      <td>0.890413</td>\n",
       "      <td>0.896148</td>\n",
       "      <td>0.884678</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.888901</td>\n",
       "      <td>0.897802</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.890413</td>\n",
       "      <td>0.896148</td>\n",
       "      <td>0.884678</td>\n",
       "      <td>0.006622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.907245</td>\n",
       "      <td>0.891983</td>\n",
       "      <td>0.897953</td>\n",
       "      <td>0.886012</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.889079</td>\n",
       "      <td>0.899524</td>\n",
       "      <td>0.878634</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.891983</td>\n",
       "      <td>0.897953</td>\n",
       "      <td>0.886012</td>\n",
       "      <td>0.006894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.899520</td>\n",
       "      <td>0.886413</td>\n",
       "      <td>0.892424</td>\n",
       "      <td>0.880401</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>0.873277</td>\n",
       "      <td>0.013012</td>\n",
       "      <td>0.886413</td>\n",
       "      <td>0.892424</td>\n",
       "      <td>0.880401</td>\n",
       "      <td>0.006942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.908238</td>\n",
       "      <td>0.898338</td>\n",
       "      <td>0.904510</td>\n",
       "      <td>0.892167</td>\n",
       "      <td>0.008727</td>\n",
       "      <td>0.896381</td>\n",
       "      <td>0.904195</td>\n",
       "      <td>0.888566</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>0.898338</td>\n",
       "      <td>0.904510</td>\n",
       "      <td>0.892167</td>\n",
       "      <td>0.007126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.914086</td>\n",
       "      <td>0.903650</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.897324</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>0.902124</td>\n",
       "      <td>0.908608</td>\n",
       "      <td>0.895640</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>0.903650</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.897324</td>\n",
       "      <td>0.007305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911338</td>\n",
       "      <td>0.898622</td>\n",
       "      <td>0.905014</td>\n",
       "      <td>0.892230</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>0.897276</td>\n",
       "      <td>0.908207</td>\n",
       "      <td>0.886344</td>\n",
       "      <td>0.015459</td>\n",
       "      <td>0.898622</td>\n",
       "      <td>0.905014</td>\n",
       "      <td>0.892230</td>\n",
       "      <td>0.007381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.905639</td>\n",
       "      <td>0.895997</td>\n",
       "      <td>0.903174</td>\n",
       "      <td>0.888821</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>0.893792</td>\n",
       "      <td>0.905847</td>\n",
       "      <td>0.881737</td>\n",
       "      <td>0.017048</td>\n",
       "      <td>0.895997</td>\n",
       "      <td>0.903174</td>\n",
       "      <td>0.888821</td>\n",
       "      <td>0.008287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.917606</td>\n",
       "      <td>0.907418</td>\n",
       "      <td>0.914936</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.010632</td>\n",
       "      <td>0.905087</td>\n",
       "      <td>0.915956</td>\n",
       "      <td>0.894217</td>\n",
       "      <td>0.015372</td>\n",
       "      <td>0.907418</td>\n",
       "      <td>0.914936</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.008681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.861025</td>\n",
       "      <td>0.836118</td>\n",
       "      <td>0.844347</td>\n",
       "      <td>0.827889</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.831140</td>\n",
       "      <td>0.835577</td>\n",
       "      <td>0.826704</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.836118</td>\n",
       "      <td>0.844347</td>\n",
       "      <td>0.827889</td>\n",
       "      <td>0.009502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.911665</td>\n",
       "      <td>0.920194</td>\n",
       "      <td>0.903136</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.912039</td>\n",
       "      <td>0.919941</td>\n",
       "      <td>0.904137</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>0.911665</td>\n",
       "      <td>0.920194</td>\n",
       "      <td>0.903136</td>\n",
       "      <td>0.009849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.897936</td>\n",
       "      <td>0.886456</td>\n",
       "      <td>0.895879</td>\n",
       "      <td>0.877033</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.881180</td>\n",
       "      <td>0.891262</td>\n",
       "      <td>0.871099</td>\n",
       "      <td>0.014257</td>\n",
       "      <td>0.886456</td>\n",
       "      <td>0.895879</td>\n",
       "      <td>0.877033</td>\n",
       "      <td>0.010881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911699</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>0.883868</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.887902</td>\n",
       "      <td>0.901040</td>\n",
       "      <td>0.874765</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>0.883868</td>\n",
       "      <td>0.010883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.908771</td>\n",
       "      <td>0.896917</td>\n",
       "      <td>0.907133</td>\n",
       "      <td>0.886702</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.893134</td>\n",
       "      <td>0.904587</td>\n",
       "      <td>0.881681</td>\n",
       "      <td>0.016197</td>\n",
       "      <td>0.896917</td>\n",
       "      <td>0.907133</td>\n",
       "      <td>0.886702</td>\n",
       "      <td>0.011796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.906245</td>\n",
       "      <td>0.895004</td>\n",
       "      <td>0.905634</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.893962</td>\n",
       "      <td>0.906541</td>\n",
       "      <td>0.881384</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>0.895004</td>\n",
       "      <td>0.905634</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.012275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.919137</td>\n",
       "      <td>0.907132</td>\n",
       "      <td>0.919315</td>\n",
       "      <td>0.894948</td>\n",
       "      <td>0.017230</td>\n",
       "      <td>0.905553</td>\n",
       "      <td>0.915406</td>\n",
       "      <td>0.895699</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>0.907132</td>\n",
       "      <td>0.919315</td>\n",
       "      <td>0.894948</td>\n",
       "      <td>0.014068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911913</td>\n",
       "      <td>0.900351</td>\n",
       "      <td>0.912826</td>\n",
       "      <td>0.887876</td>\n",
       "      <td>0.017642</td>\n",
       "      <td>0.898433</td>\n",
       "      <td>0.915577</td>\n",
       "      <td>0.881290</td>\n",
       "      <td>0.024244</td>\n",
       "      <td>0.900351</td>\n",
       "      <td>0.912826</td>\n",
       "      <td>0.887876</td>\n",
       "      <td>0.014405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.893259</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880210</td>\n",
       "      <td>0.853124</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>0.863293</td>\n",
       "      <td>0.880051</td>\n",
       "      <td>0.846536</td>\n",
       "      <td>0.023699</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880210</td>\n",
       "      <td>0.853124</td>\n",
       "      <td>0.015638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.892280</td>\n",
       "      <td>0.865347</td>\n",
       "      <td>0.879487</td>\n",
       "      <td>0.851207</td>\n",
       "      <td>0.019997</td>\n",
       "      <td>0.866982</td>\n",
       "      <td>0.881046</td>\n",
       "      <td>0.852918</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.865347</td>\n",
       "      <td>0.879487</td>\n",
       "      <td>0.851207</td>\n",
       "      <td>0.016327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.881357</td>\n",
       "      <td>0.855290</td>\n",
       "      <td>0.869896</td>\n",
       "      <td>0.840684</td>\n",
       "      <td>0.020656</td>\n",
       "      <td>0.850914</td>\n",
       "      <td>0.870166</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.855290</td>\n",
       "      <td>0.869896</td>\n",
       "      <td>0.840684</td>\n",
       "      <td>0.016866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.853585</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.820677</td>\n",
       "      <td>0.789637</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>0.804197</td>\n",
       "      <td>0.818252</td>\n",
       "      <td>0.790141</td>\n",
       "      <td>0.019878</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.820677</td>\n",
       "      <td>0.789637</td>\n",
       "      <td>0.017921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.916831</td>\n",
       "      <td>0.902693</td>\n",
       "      <td>0.918672</td>\n",
       "      <td>0.886714</td>\n",
       "      <td>0.022597</td>\n",
       "      <td>0.900001</td>\n",
       "      <td>0.915255</td>\n",
       "      <td>0.884748</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.902693</td>\n",
       "      <td>0.918672</td>\n",
       "      <td>0.886714</td>\n",
       "      <td>0.018451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.902097</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>0.900606</td>\n",
       "      <td>0.868395</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>0.880720</td>\n",
       "      <td>0.896188</td>\n",
       "      <td>0.865252</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>0.900606</td>\n",
       "      <td>0.868395</td>\n",
       "      <td>0.018597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.872577</td>\n",
       "      <td>0.847187</td>\n",
       "      <td>0.863897</td>\n",
       "      <td>0.830476</td>\n",
       "      <td>0.023632</td>\n",
       "      <td>0.843076</td>\n",
       "      <td>0.862398</td>\n",
       "      <td>0.823753</td>\n",
       "      <td>0.027326</td>\n",
       "      <td>0.847187</td>\n",
       "      <td>0.863897</td>\n",
       "      <td>0.830476</td>\n",
       "      <td>0.019296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.904111</td>\n",
       "      <td>0.882519</td>\n",
       "      <td>0.900347</td>\n",
       "      <td>0.864691</td>\n",
       "      <td>0.025213</td>\n",
       "      <td>0.880139</td>\n",
       "      <td>0.900780</td>\n",
       "      <td>0.859498</td>\n",
       "      <td>0.029190</td>\n",
       "      <td>0.882519</td>\n",
       "      <td>0.900347</td>\n",
       "      <td>0.864691</td>\n",
       "      <td>0.020586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.903543</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.910245</td>\n",
       "      <td>0.873756</td>\n",
       "      <td>0.025802</td>\n",
       "      <td>0.887870</td>\n",
       "      <td>0.904817</td>\n",
       "      <td>0.870924</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.910245</td>\n",
       "      <td>0.873756</td>\n",
       "      <td>0.021067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.869209</td>\n",
       "      <td>0.845150</td>\n",
       "      <td>0.866591</td>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.030323</td>\n",
       "      <td>0.845143</td>\n",
       "      <td>0.867963</td>\n",
       "      <td>0.822323</td>\n",
       "      <td>0.032272</td>\n",
       "      <td>0.845150</td>\n",
       "      <td>0.866591</td>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.024758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.910078</td>\n",
       "      <td>0.892327</td>\n",
       "      <td>0.915511</td>\n",
       "      <td>0.869143</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.892262</td>\n",
       "      <td>0.912961</td>\n",
       "      <td>0.871564</td>\n",
       "      <td>0.029272</td>\n",
       "      <td>0.892327</td>\n",
       "      <td>0.915511</td>\n",
       "      <td>0.869143</td>\n",
       "      <td>0.026770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911436</td>\n",
       "      <td>0.889300</td>\n",
       "      <td>0.920258</td>\n",
       "      <td>0.858343</td>\n",
       "      <td>0.043780</td>\n",
       "      <td>0.888865</td>\n",
       "      <td>0.919061</td>\n",
       "      <td>0.858668</td>\n",
       "      <td>0.042704</td>\n",
       "      <td>0.889300</td>\n",
       "      <td>0.920258</td>\n",
       "      <td>0.858343</td>\n",
       "      <td>0.035746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.893936</td>\n",
       "      <td>0.865171</td>\n",
       "      <td>0.896773</td>\n",
       "      <td>0.833568</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.860395</td>\n",
       "      <td>0.893187</td>\n",
       "      <td>0.827602</td>\n",
       "      <td>0.046376</td>\n",
       "      <td>0.865171</td>\n",
       "      <td>0.896773</td>\n",
       "      <td>0.833568</td>\n",
       "      <td>0.036492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.835761</td>\n",
       "      <td>0.795682</td>\n",
       "      <td>0.829887</td>\n",
       "      <td>0.761477</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.795812</td>\n",
       "      <td>0.830577</td>\n",
       "      <td>0.761047</td>\n",
       "      <td>0.049165</td>\n",
       "      <td>0.795682</td>\n",
       "      <td>0.829887</td>\n",
       "      <td>0.761477</td>\n",
       "      <td>0.039496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.898252</td>\n",
       "      <td>0.867775</td>\n",
       "      <td>0.904903</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>0.052508</td>\n",
       "      <td>0.860900</td>\n",
       "      <td>0.896432</td>\n",
       "      <td>0.825367</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.867775</td>\n",
       "      <td>0.904903</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>0.042872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.866907</td>\n",
       "      <td>0.843008</td>\n",
       "      <td>0.880992</td>\n",
       "      <td>0.805024</td>\n",
       "      <td>0.053717</td>\n",
       "      <td>0.841068</td>\n",
       "      <td>0.875662</td>\n",
       "      <td>0.806475</td>\n",
       "      <td>0.048923</td>\n",
       "      <td>0.843008</td>\n",
       "      <td>0.880992</td>\n",
       "      <td>0.805024</td>\n",
       "      <td>0.043860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.672109</td>\n",
       "      <td>0.620360</td>\n",
       "      <td>0.661140</td>\n",
       "      <td>0.579580</td>\n",
       "      <td>0.057672</td>\n",
       "      <td>0.621661</td>\n",
       "      <td>0.661012</td>\n",
       "      <td>0.582311</td>\n",
       "      <td>0.055650</td>\n",
       "      <td>0.620360</td>\n",
       "      <td>0.661140</td>\n",
       "      <td>0.579580</td>\n",
       "      <td>0.047089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.880056</td>\n",
       "      <td>0.844350</td>\n",
       "      <td>0.885222</td>\n",
       "      <td>0.803478</td>\n",
       "      <td>0.057802</td>\n",
       "      <td>0.843018</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>0.801617</td>\n",
       "      <td>0.058551</td>\n",
       "      <td>0.844350</td>\n",
       "      <td>0.885222</td>\n",
       "      <td>0.803478</td>\n",
       "      <td>0.047195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.893259</td>\n",
       "      <td>0.859075</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.062024</td>\n",
       "      <td>0.853271</td>\n",
       "      <td>0.899084</td>\n",
       "      <td>0.807459</td>\n",
       "      <td>0.064789</td>\n",
       "      <td>0.859075</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.050642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.908315</td>\n",
       "      <td>0.858327</td>\n",
       "      <td>0.916067</td>\n",
       "      <td>0.800587</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>0.858829</td>\n",
       "      <td>0.917469</td>\n",
       "      <td>0.800189</td>\n",
       "      <td>0.082929</td>\n",
       "      <td>0.858327</td>\n",
       "      <td>0.916067</td>\n",
       "      <td>0.800587</td>\n",
       "      <td>0.066673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.780025</td>\n",
       "      <td>0.719180</td>\n",
       "      <td>0.782917</td>\n",
       "      <td>0.655442</td>\n",
       "      <td>0.090138</td>\n",
       "      <td>0.716803</td>\n",
       "      <td>0.780993</td>\n",
       "      <td>0.652613</td>\n",
       "      <td>0.090778</td>\n",
       "      <td>0.719180</td>\n",
       "      <td>0.782917</td>\n",
       "      <td>0.655442</td>\n",
       "      <td>0.073598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.870292</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.882874</td>\n",
       "      <td>0.730571</td>\n",
       "      <td>0.107694</td>\n",
       "      <td>0.793327</td>\n",
       "      <td>0.877534</td>\n",
       "      <td>0.709121</td>\n",
       "      <td>0.119086</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.882874</td>\n",
       "      <td>0.730571</td>\n",
       "      <td>0.087932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.707696</td>\n",
       "      <td>0.700829</td>\n",
       "      <td>0.785834</td>\n",
       "      <td>0.615824</td>\n",
       "      <td>0.120215</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.784131</td>\n",
       "      <td>0.618466</td>\n",
       "      <td>0.117143</td>\n",
       "      <td>0.700829</td>\n",
       "      <td>0.785834</td>\n",
       "      <td>0.615824</td>\n",
       "      <td>0.098155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.861253</td>\n",
       "      <td>0.756632</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.630003</td>\n",
       "      <td>0.179080</td>\n",
       "      <td>0.749193</td>\n",
       "      <td>0.876358</td>\n",
       "      <td>0.622027</td>\n",
       "      <td>0.179839</td>\n",
       "      <td>0.756632</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.630003</td>\n",
       "      <td>0.146218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  auc_test  \\\n",
       "32  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.900088   \n",
       "26  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.904174   \n",
       "58  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.686786   \n",
       "3   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.922637   \n",
       "9   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.916487   \n",
       "12  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.912061   \n",
       "1   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.925298   \n",
       "4   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.921805   \n",
       "0   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.925806   \n",
       "53  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.850021   \n",
       "10  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.915441   \n",
       "34  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.898748   \n",
       "2   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.923642   \n",
       "17  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.911332   \n",
       "41  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.890501   \n",
       "18  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.911241   \n",
       "30  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.900811   \n",
       "43  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.884630   \n",
       "42  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.889792   \n",
       "54  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.846501   \n",
       "31  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.900750   \n",
       "23  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.907245   \n",
       "33  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.899520   \n",
       "22  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.908238   \n",
       "11  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.914086   \n",
       "16  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.911338   \n",
       "25  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.905639   \n",
       "7   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.917606   \n",
       "51  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.861025   \n",
       "5   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.921788   \n",
       "36  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.897936   \n",
       "14  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.911699   \n",
       "20  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.908771   \n",
       "24  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.906245   \n",
       "6   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.919137   \n",
       "13  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.911913   \n",
       "38  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.893259   \n",
       "40  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.892280   \n",
       "44  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.881357   \n",
       "52  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.853585   \n",
       "8   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.916831   \n",
       "29  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.902097   \n",
       "46  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.872577   \n",
       "27  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.904111   \n",
       "28  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.903543   \n",
       "48  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.869209   \n",
       "19  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.910078   \n",
       "15  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.911436   \n",
       "37  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.893936   \n",
       "55  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.835761   \n",
       "35  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.898252   \n",
       "49  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.866907   \n",
       "59  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.672109   \n",
       "45  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.880056   \n",
       "39  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.893259   \n",
       "21  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.908315   \n",
       "56  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.780025   \n",
       "47  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.870292   \n",
       "57  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.707696   \n",
       "50  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.861253   \n",
       "\n",
       "    outer_auc_mean  outer_auc_max  outer_auc_min  outer_auc_std  \\\n",
       "32        0.886363       0.886437       0.886289       0.000104   \n",
       "26        0.892176       0.892375       0.891977       0.000282   \n",
       "58        0.652929       0.653418       0.652441       0.000691   \n",
       "3         0.915354       0.916212       0.914497       0.001212   \n",
       "9         0.909845       0.910799       0.908891       0.001349   \n",
       "12        0.900836       0.902177       0.899496       0.001896   \n",
       "1         0.920686       0.922044       0.919327       0.001921   \n",
       "4         0.914663       0.916327       0.912998       0.002354   \n",
       "0         0.922460       0.924191       0.920730       0.002448   \n",
       "53        0.831989       0.833729       0.830248       0.002462   \n",
       "10        0.905192       0.907154       0.903230       0.002774   \n",
       "34        0.885711       0.888505       0.882917       0.003951   \n",
       "2         0.917400       0.920491       0.914308       0.004372   \n",
       "17        0.903197       0.906559       0.899835       0.004755   \n",
       "41        0.876252       0.879857       0.872647       0.005099   \n",
       "18        0.900845       0.904539       0.897151       0.005224   \n",
       "30        0.886719       0.890499       0.882940       0.005345   \n",
       "43        0.866506       0.870379       0.862633       0.005477   \n",
       "42        0.877497       0.881620       0.873375       0.005830   \n",
       "54        0.826380       0.831962       0.820798       0.007894   \n",
       "31        0.890413       0.896148       0.884678       0.008110   \n",
       "23        0.891983       0.897953       0.886012       0.008443   \n",
       "33        0.886413       0.892424       0.880401       0.008502   \n",
       "22        0.898338       0.904510       0.892167       0.008727   \n",
       "11        0.903650       0.909977       0.897324       0.008947   \n",
       "16        0.898622       0.905014       0.892230       0.009040   \n",
       "25        0.895997       0.903174       0.888821       0.010149   \n",
       "7         0.907418       0.914936       0.899900       0.010632   \n",
       "51        0.836118       0.844347       0.827889       0.011638   \n",
       "5         0.911665       0.920194       0.903136       0.012062   \n",
       "36        0.886456       0.895879       0.877033       0.013326   \n",
       "14        0.893293       0.902717       0.883868       0.013329   \n",
       "20        0.896917       0.907133       0.886702       0.014447   \n",
       "24        0.895004       0.905634       0.884374       0.015033   \n",
       "6         0.907132       0.919315       0.894948       0.017230   \n",
       "13        0.900351       0.912826       0.887876       0.017642   \n",
       "38        0.866667       0.880210       0.853124       0.019152   \n",
       "40        0.865347       0.879487       0.851207       0.019997   \n",
       "44        0.855290       0.869896       0.840684       0.020656   \n",
       "52        0.805157       0.820677       0.789637       0.021948   \n",
       "8         0.902693       0.918672       0.886714       0.022597   \n",
       "29        0.884500       0.900606       0.868395       0.022777   \n",
       "46        0.847187       0.863897       0.830476       0.023632   \n",
       "27        0.882519       0.900347       0.864691       0.025213   \n",
       "28        0.892000       0.910245       0.873756       0.025802   \n",
       "48        0.845150       0.866591       0.823708       0.030323   \n",
       "19        0.892327       0.915511       0.869143       0.032787   \n",
       "15        0.889300       0.920258       0.858343       0.043780   \n",
       "37        0.865171       0.896773       0.833568       0.044693   \n",
       "55        0.795682       0.829887       0.761477       0.048373   \n",
       "35        0.867775       0.904903       0.830646       0.052508   \n",
       "49        0.843008       0.880992       0.805024       0.053717   \n",
       "59        0.620360       0.661140       0.579580       0.057672   \n",
       "45        0.844350       0.885222       0.803478       0.057802   \n",
       "39        0.859075       0.902932       0.815217       0.062024   \n",
       "21        0.858327       0.916067       0.800587       0.081657   \n",
       "56        0.719180       0.782917       0.655442       0.090138   \n",
       "47        0.806723       0.882874       0.730571       0.107694   \n",
       "57        0.700829       0.785834       0.615824       0.120215   \n",
       "50        0.756632       0.883260       0.630003       0.179080   \n",
       "\n",
       "    split_auc_mean  split_auc_max  split_auc_min  split_auc_std  \\\n",
       "32        0.889974       0.893886       0.886061       0.005533   \n",
       "26        0.888673       0.890631       0.886716       0.002769   \n",
       "58        0.648247       0.648409       0.648086       0.000228   \n",
       "3         0.916081       0.920680       0.911481       0.006505   \n",
       "9         0.906228       0.907861       0.904594       0.002310   \n",
       "12        0.895642       0.897171       0.894114       0.002162   \n",
       "1         0.922803       0.924874       0.920733       0.002928   \n",
       "4         0.913250       0.914964       0.911536       0.002424   \n",
       "0         0.920260       0.922063       0.918457       0.002550   \n",
       "53        0.826592       0.831330       0.821854       0.006700   \n",
       "10        0.903721       0.906079       0.901363       0.003335   \n",
       "34        0.880969       0.887067       0.874870       0.008625   \n",
       "2         0.916278       0.917346       0.915210       0.001510   \n",
       "17        0.898943       0.902235       0.895650       0.004656   \n",
       "41        0.876272       0.881036       0.871507       0.006738   \n",
       "18        0.896720       0.896910       0.896531       0.000268   \n",
       "30        0.884555       0.884826       0.884283       0.000384   \n",
       "43        0.864945       0.866428       0.863461       0.002098   \n",
       "42        0.873819       0.874143       0.873496       0.000458   \n",
       "54        0.825727       0.826464       0.824991       0.001041   \n",
       "31        0.888901       0.897802       0.880000       0.012588   \n",
       "23        0.889079       0.899524       0.878634       0.014772   \n",
       "33        0.882478       0.891679       0.873277       0.013012   \n",
       "22        0.896381       0.904195       0.888566       0.011051   \n",
       "11        0.902124       0.908608       0.895640       0.009170   \n",
       "16        0.897276       0.908207       0.886344       0.015459   \n",
       "25        0.893792       0.905847       0.881737       0.017048   \n",
       "7         0.905087       0.915956       0.894217       0.015372   \n",
       "51        0.831140       0.835577       0.826704       0.006274   \n",
       "5         0.912039       0.919941       0.904137       0.011176   \n",
       "36        0.881180       0.891262       0.871099       0.014257   \n",
       "14        0.887902       0.901040       0.874765       0.018579   \n",
       "20        0.893134       0.904587       0.881681       0.016197   \n",
       "24        0.893962       0.906541       0.881384       0.017788   \n",
       "6         0.905553       0.915406       0.895699       0.013935   \n",
       "13        0.898433       0.915577       0.881290       0.024244   \n",
       "38        0.863293       0.880051       0.846536       0.023699   \n",
       "40        0.866982       0.881046       0.852918       0.019890   \n",
       "44        0.850914       0.870166       0.831663       0.027226   \n",
       "52        0.804197       0.818252       0.790141       0.019878   \n",
       "8         0.900001       0.915255       0.884748       0.021571   \n",
       "29        0.880720       0.896188       0.865252       0.021875   \n",
       "46        0.843076       0.862398       0.823753       0.027326   \n",
       "27        0.880139       0.900780       0.859498       0.029190   \n",
       "28        0.887870       0.904817       0.870924       0.023965   \n",
       "48        0.845143       0.867963       0.822323       0.032272   \n",
       "19        0.892262       0.912961       0.871564       0.029272   \n",
       "15        0.888865       0.919061       0.858668       0.042704   \n",
       "37        0.860395       0.893187       0.827602       0.046376   \n",
       "55        0.795812       0.830577       0.761047       0.049165   \n",
       "35        0.860900       0.896432       0.825367       0.050251   \n",
       "49        0.841068       0.875662       0.806475       0.048923   \n",
       "59        0.621661       0.661012       0.582311       0.055650   \n",
       "45        0.843018       0.884420       0.801617       0.058551   \n",
       "39        0.853271       0.899084       0.807459       0.064789   \n",
       "21        0.858829       0.917469       0.800189       0.082929   \n",
       "56        0.716803       0.780993       0.652613       0.090778   \n",
       "47        0.793327       0.877534       0.709121       0.119086   \n",
       "57        0.701299       0.784131       0.618466       0.117143   \n",
       "50        0.749193       0.876358       0.622027       0.179839   \n",
       "\n",
       "    inner_auc_mean  inner_auc_max  inner_auc_min  inner_auc_std  \n",
       "32        0.886363       0.886437       0.886289       0.000085  \n",
       "26        0.892176       0.892375       0.891977       0.000230  \n",
       "58        0.652929       0.653418       0.652441       0.000564  \n",
       "3         0.915354       0.916212       0.914497       0.000990  \n",
       "9         0.909845       0.910799       0.908891       0.001102  \n",
       "12        0.900836       0.902177       0.899496       0.001548  \n",
       "1         0.920686       0.922044       0.919327       0.001569  \n",
       "4         0.914663       0.916327       0.912998       0.001922  \n",
       "0         0.922460       0.924191       0.920730       0.001999  \n",
       "53        0.831989       0.833729       0.830248       0.002010  \n",
       "10        0.905192       0.907154       0.903230       0.002265  \n",
       "34        0.885711       0.888505       0.882917       0.003226  \n",
       "2         0.917400       0.920491       0.914308       0.003570  \n",
       "17        0.903197       0.906559       0.899835       0.003882  \n",
       "41        0.876252       0.879857       0.872647       0.004163  \n",
       "18        0.900845       0.904539       0.897151       0.004265  \n",
       "30        0.886719       0.890499       0.882940       0.004364  \n",
       "43        0.866506       0.870379       0.862633       0.004472  \n",
       "42        0.877497       0.881620       0.873375       0.004760  \n",
       "54        0.826380       0.831962       0.820798       0.006445  \n",
       "31        0.890413       0.896148       0.884678       0.006622  \n",
       "23        0.891983       0.897953       0.886012       0.006894  \n",
       "33        0.886413       0.892424       0.880401       0.006942  \n",
       "22        0.898338       0.904510       0.892167       0.007126  \n",
       "11        0.903650       0.909977       0.897324       0.007305  \n",
       "16        0.898622       0.905014       0.892230       0.007381  \n",
       "25        0.895997       0.903174       0.888821       0.008287  \n",
       "7         0.907418       0.914936       0.899900       0.008681  \n",
       "51        0.836118       0.844347       0.827889       0.009502  \n",
       "5         0.911665       0.920194       0.903136       0.009849  \n",
       "36        0.886456       0.895879       0.877033       0.010881  \n",
       "14        0.893293       0.902717       0.883868       0.010883  \n",
       "20        0.896917       0.907133       0.886702       0.011796  \n",
       "24        0.895004       0.905634       0.884374       0.012275  \n",
       "6         0.907132       0.919315       0.894948       0.014068  \n",
       "13        0.900351       0.912826       0.887876       0.014405  \n",
       "38        0.866667       0.880210       0.853124       0.015638  \n",
       "40        0.865347       0.879487       0.851207       0.016327  \n",
       "44        0.855290       0.869896       0.840684       0.016866  \n",
       "52        0.805157       0.820677       0.789637       0.017921  \n",
       "8         0.902693       0.918672       0.886714       0.018451  \n",
       "29        0.884500       0.900606       0.868395       0.018597  \n",
       "46        0.847187       0.863897       0.830476       0.019296  \n",
       "27        0.882519       0.900347       0.864691       0.020586  \n",
       "28        0.892000       0.910245       0.873756       0.021067  \n",
       "48        0.845150       0.866591       0.823708       0.024758  \n",
       "19        0.892327       0.915511       0.869143       0.026770  \n",
       "15        0.889300       0.920258       0.858343       0.035746  \n",
       "37        0.865171       0.896773       0.833568       0.036492  \n",
       "55        0.795682       0.829887       0.761477       0.039496  \n",
       "35        0.867775       0.904903       0.830646       0.042872  \n",
       "49        0.843008       0.880992       0.805024       0.043860  \n",
       "59        0.620360       0.661140       0.579580       0.047089  \n",
       "45        0.844350       0.885222       0.803478       0.047195  \n",
       "39        0.859075       0.902932       0.815217       0.050642  \n",
       "21        0.858327       0.916067       0.800587       0.066673  \n",
       "56        0.719180       0.782917       0.655442       0.073598  \n",
       "47        0.806723       0.882874       0.730571       0.087932  \n",
       "57        0.700829       0.785834       0.615824       0.098155  \n",
       "50        0.756632       0.883260       0.630003       0.146218  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"df.sort_values(by=[\\\"outer_auc_std\\\"], ascending=True)\";\n",
       "                var nbb_formatted_code = \"df.sort_values(by=[\\\"outer_auc_std\\\"], ascending=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.sort_values(by=[\"outer_auc_std\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>auc_test</th>\n",
       "      <th>outer_auc_mean</th>\n",
       "      <th>outer_auc_max</th>\n",
       "      <th>outer_auc_min</th>\n",
       "      <th>outer_auc_std</th>\n",
       "      <th>split_auc_mean</th>\n",
       "      <th>split_auc_max</th>\n",
       "      <th>split_auc_min</th>\n",
       "      <th>split_auc_std</th>\n",
       "      <th>inner_auc_mean</th>\n",
       "      <th>inner_auc_max</th>\n",
       "      <th>inner_auc_min</th>\n",
       "      <th>inner_auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.686786</td>\n",
       "      <td>0.652929</td>\n",
       "      <td>0.653418</td>\n",
       "      <td>0.652441</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.648247</td>\n",
       "      <td>0.648409</td>\n",
       "      <td>0.648086</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.652929</td>\n",
       "      <td>0.653418</td>\n",
       "      <td>0.652441</td>\n",
       "      <td>0.000564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911241</td>\n",
       "      <td>0.900845</td>\n",
       "      <td>0.904539</td>\n",
       "      <td>0.897151</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.896720</td>\n",
       "      <td>0.896910</td>\n",
       "      <td>0.896531</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.900845</td>\n",
       "      <td>0.904539</td>\n",
       "      <td>0.897151</td>\n",
       "      <td>0.004265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.900811</td>\n",
       "      <td>0.886719</td>\n",
       "      <td>0.890499</td>\n",
       "      <td>0.882940</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.884555</td>\n",
       "      <td>0.884826</td>\n",
       "      <td>0.884283</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.886719</td>\n",
       "      <td>0.890499</td>\n",
       "      <td>0.882940</td>\n",
       "      <td>0.004364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.889792</td>\n",
       "      <td>0.877497</td>\n",
       "      <td>0.881620</td>\n",
       "      <td>0.873375</td>\n",
       "      <td>0.005830</td>\n",
       "      <td>0.873819</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.873496</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.877497</td>\n",
       "      <td>0.881620</td>\n",
       "      <td>0.873375</td>\n",
       "      <td>0.004760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.846501</td>\n",
       "      <td>0.826380</td>\n",
       "      <td>0.831962</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.825727</td>\n",
       "      <td>0.826464</td>\n",
       "      <td>0.824991</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.826380</td>\n",
       "      <td>0.831962</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.006445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.923642</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.920491</td>\n",
       "      <td>0.914308</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.916278</td>\n",
       "      <td>0.917346</td>\n",
       "      <td>0.915210</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.920491</td>\n",
       "      <td>0.914308</td>\n",
       "      <td>0.003570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.884630</td>\n",
       "      <td>0.866506</td>\n",
       "      <td>0.870379</td>\n",
       "      <td>0.862633</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.864945</td>\n",
       "      <td>0.866428</td>\n",
       "      <td>0.863461</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.866506</td>\n",
       "      <td>0.870379</td>\n",
       "      <td>0.862633</td>\n",
       "      <td>0.004472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.912061</td>\n",
       "      <td>0.900836</td>\n",
       "      <td>0.902177</td>\n",
       "      <td>0.899496</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.895642</td>\n",
       "      <td>0.897171</td>\n",
       "      <td>0.894114</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.900836</td>\n",
       "      <td>0.902177</td>\n",
       "      <td>0.899496</td>\n",
       "      <td>0.001548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.916487</td>\n",
       "      <td>0.909845</td>\n",
       "      <td>0.910799</td>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.906228</td>\n",
       "      <td>0.907861</td>\n",
       "      <td>0.904594</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.909845</td>\n",
       "      <td>0.910799</td>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.001102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.921805</td>\n",
       "      <td>0.914663</td>\n",
       "      <td>0.916327</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.913250</td>\n",
       "      <td>0.914964</td>\n",
       "      <td>0.911536</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.914663</td>\n",
       "      <td>0.916327</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.925806</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.924191</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.920260</td>\n",
       "      <td>0.922063</td>\n",
       "      <td>0.918457</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.924191</td>\n",
       "      <td>0.920730</td>\n",
       "      <td>0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.904174</td>\n",
       "      <td>0.892176</td>\n",
       "      <td>0.892375</td>\n",
       "      <td>0.891977</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.888673</td>\n",
       "      <td>0.890631</td>\n",
       "      <td>0.886716</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.892176</td>\n",
       "      <td>0.892375</td>\n",
       "      <td>0.891977</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.925298</td>\n",
       "      <td>0.920686</td>\n",
       "      <td>0.922044</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.922803</td>\n",
       "      <td>0.924874</td>\n",
       "      <td>0.920733</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.920686</td>\n",
       "      <td>0.922044</td>\n",
       "      <td>0.919327</td>\n",
       "      <td>0.001569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.915441</td>\n",
       "      <td>0.905192</td>\n",
       "      <td>0.907154</td>\n",
       "      <td>0.903230</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.903721</td>\n",
       "      <td>0.906079</td>\n",
       "      <td>0.901363</td>\n",
       "      <td>0.003335</td>\n",
       "      <td>0.905192</td>\n",
       "      <td>0.907154</td>\n",
       "      <td>0.903230</td>\n",
       "      <td>0.002265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911332</td>\n",
       "      <td>0.903197</td>\n",
       "      <td>0.906559</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.898943</td>\n",
       "      <td>0.902235</td>\n",
       "      <td>0.895650</td>\n",
       "      <td>0.004656</td>\n",
       "      <td>0.903197</td>\n",
       "      <td>0.906559</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.900088</td>\n",
       "      <td>0.886363</td>\n",
       "      <td>0.886437</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.889974</td>\n",
       "      <td>0.893886</td>\n",
       "      <td>0.886061</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.886363</td>\n",
       "      <td>0.886437</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.861025</td>\n",
       "      <td>0.836118</td>\n",
       "      <td>0.844347</td>\n",
       "      <td>0.827889</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.831140</td>\n",
       "      <td>0.835577</td>\n",
       "      <td>0.826704</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.836118</td>\n",
       "      <td>0.844347</td>\n",
       "      <td>0.827889</td>\n",
       "      <td>0.009502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.922637</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.916212</td>\n",
       "      <td>0.914497</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.916081</td>\n",
       "      <td>0.920680</td>\n",
       "      <td>0.911481</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.916212</td>\n",
       "      <td>0.914497</td>\n",
       "      <td>0.000990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.850021</td>\n",
       "      <td>0.831989</td>\n",
       "      <td>0.833729</td>\n",
       "      <td>0.830248</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.826592</td>\n",
       "      <td>0.831330</td>\n",
       "      <td>0.821854</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.831989</td>\n",
       "      <td>0.833729</td>\n",
       "      <td>0.830248</td>\n",
       "      <td>0.002010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.890501</td>\n",
       "      <td>0.876252</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.872647</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.876272</td>\n",
       "      <td>0.881036</td>\n",
       "      <td>0.871507</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.876252</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.872647</td>\n",
       "      <td>0.004163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.898748</td>\n",
       "      <td>0.885711</td>\n",
       "      <td>0.888505</td>\n",
       "      <td>0.882917</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.880969</td>\n",
       "      <td>0.887067</td>\n",
       "      <td>0.874870</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>0.885711</td>\n",
       "      <td>0.888505</td>\n",
       "      <td>0.882917</td>\n",
       "      <td>0.003226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.914086</td>\n",
       "      <td>0.903650</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.897324</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>0.902124</td>\n",
       "      <td>0.908608</td>\n",
       "      <td>0.895640</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>0.903650</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.897324</td>\n",
       "      <td>0.007305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.908238</td>\n",
       "      <td>0.898338</td>\n",
       "      <td>0.904510</td>\n",
       "      <td>0.892167</td>\n",
       "      <td>0.008727</td>\n",
       "      <td>0.896381</td>\n",
       "      <td>0.904195</td>\n",
       "      <td>0.888566</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>0.898338</td>\n",
       "      <td>0.904510</td>\n",
       "      <td>0.892167</td>\n",
       "      <td>0.007126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.911665</td>\n",
       "      <td>0.920194</td>\n",
       "      <td>0.903136</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.912039</td>\n",
       "      <td>0.919941</td>\n",
       "      <td>0.904137</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>0.911665</td>\n",
       "      <td>0.920194</td>\n",
       "      <td>0.903136</td>\n",
       "      <td>0.009849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.900750</td>\n",
       "      <td>0.890413</td>\n",
       "      <td>0.896148</td>\n",
       "      <td>0.884678</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.888901</td>\n",
       "      <td>0.897802</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.890413</td>\n",
       "      <td>0.896148</td>\n",
       "      <td>0.884678</td>\n",
       "      <td>0.006622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.899520</td>\n",
       "      <td>0.886413</td>\n",
       "      <td>0.892424</td>\n",
       "      <td>0.880401</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>0.873277</td>\n",
       "      <td>0.013012</td>\n",
       "      <td>0.886413</td>\n",
       "      <td>0.892424</td>\n",
       "      <td>0.880401</td>\n",
       "      <td>0.006942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.919137</td>\n",
       "      <td>0.907132</td>\n",
       "      <td>0.919315</td>\n",
       "      <td>0.894948</td>\n",
       "      <td>0.017230</td>\n",
       "      <td>0.905553</td>\n",
       "      <td>0.915406</td>\n",
       "      <td>0.895699</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>0.907132</td>\n",
       "      <td>0.919315</td>\n",
       "      <td>0.894948</td>\n",
       "      <td>0.014068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.897936</td>\n",
       "      <td>0.886456</td>\n",
       "      <td>0.895879</td>\n",
       "      <td>0.877033</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.881180</td>\n",
       "      <td>0.891262</td>\n",
       "      <td>0.871099</td>\n",
       "      <td>0.014257</td>\n",
       "      <td>0.886456</td>\n",
       "      <td>0.895879</td>\n",
       "      <td>0.877033</td>\n",
       "      <td>0.010881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.907245</td>\n",
       "      <td>0.891983</td>\n",
       "      <td>0.897953</td>\n",
       "      <td>0.886012</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.889079</td>\n",
       "      <td>0.899524</td>\n",
       "      <td>0.878634</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.891983</td>\n",
       "      <td>0.897953</td>\n",
       "      <td>0.886012</td>\n",
       "      <td>0.006894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.917606</td>\n",
       "      <td>0.907418</td>\n",
       "      <td>0.914936</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.010632</td>\n",
       "      <td>0.905087</td>\n",
       "      <td>0.915956</td>\n",
       "      <td>0.894217</td>\n",
       "      <td>0.015372</td>\n",
       "      <td>0.907418</td>\n",
       "      <td>0.914936</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.008681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911338</td>\n",
       "      <td>0.898622</td>\n",
       "      <td>0.905014</td>\n",
       "      <td>0.892230</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>0.897276</td>\n",
       "      <td>0.908207</td>\n",
       "      <td>0.886344</td>\n",
       "      <td>0.015459</td>\n",
       "      <td>0.898622</td>\n",
       "      <td>0.905014</td>\n",
       "      <td>0.892230</td>\n",
       "      <td>0.007381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.908771</td>\n",
       "      <td>0.896917</td>\n",
       "      <td>0.907133</td>\n",
       "      <td>0.886702</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.893134</td>\n",
       "      <td>0.904587</td>\n",
       "      <td>0.881681</td>\n",
       "      <td>0.016197</td>\n",
       "      <td>0.896917</td>\n",
       "      <td>0.907133</td>\n",
       "      <td>0.886702</td>\n",
       "      <td>0.011796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.905639</td>\n",
       "      <td>0.895997</td>\n",
       "      <td>0.903174</td>\n",
       "      <td>0.888821</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>0.893792</td>\n",
       "      <td>0.905847</td>\n",
       "      <td>0.881737</td>\n",
       "      <td>0.017048</td>\n",
       "      <td>0.895997</td>\n",
       "      <td>0.903174</td>\n",
       "      <td>0.888821</td>\n",
       "      <td>0.008287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.906245</td>\n",
       "      <td>0.895004</td>\n",
       "      <td>0.905634</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.893962</td>\n",
       "      <td>0.906541</td>\n",
       "      <td>0.881384</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>0.895004</td>\n",
       "      <td>0.905634</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.012275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911699</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>0.883868</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.887902</td>\n",
       "      <td>0.901040</td>\n",
       "      <td>0.874765</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>0.883868</td>\n",
       "      <td>0.010883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.853585</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.820677</td>\n",
       "      <td>0.789637</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>0.804197</td>\n",
       "      <td>0.818252</td>\n",
       "      <td>0.790141</td>\n",
       "      <td>0.019878</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.820677</td>\n",
       "      <td>0.789637</td>\n",
       "      <td>0.017921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.892280</td>\n",
       "      <td>0.865347</td>\n",
       "      <td>0.879487</td>\n",
       "      <td>0.851207</td>\n",
       "      <td>0.019997</td>\n",
       "      <td>0.866982</td>\n",
       "      <td>0.881046</td>\n",
       "      <td>0.852918</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.865347</td>\n",
       "      <td>0.879487</td>\n",
       "      <td>0.851207</td>\n",
       "      <td>0.016327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.916831</td>\n",
       "      <td>0.902693</td>\n",
       "      <td>0.918672</td>\n",
       "      <td>0.886714</td>\n",
       "      <td>0.022597</td>\n",
       "      <td>0.900001</td>\n",
       "      <td>0.915255</td>\n",
       "      <td>0.884748</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.902693</td>\n",
       "      <td>0.918672</td>\n",
       "      <td>0.886714</td>\n",
       "      <td>0.018451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.902097</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>0.900606</td>\n",
       "      <td>0.868395</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>0.880720</td>\n",
       "      <td>0.896188</td>\n",
       "      <td>0.865252</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>0.900606</td>\n",
       "      <td>0.868395</td>\n",
       "      <td>0.018597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.893259</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880210</td>\n",
       "      <td>0.853124</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>0.863293</td>\n",
       "      <td>0.880051</td>\n",
       "      <td>0.846536</td>\n",
       "      <td>0.023699</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880210</td>\n",
       "      <td>0.853124</td>\n",
       "      <td>0.015638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.903543</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.910245</td>\n",
       "      <td>0.873756</td>\n",
       "      <td>0.025802</td>\n",
       "      <td>0.887870</td>\n",
       "      <td>0.904817</td>\n",
       "      <td>0.870924</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.910245</td>\n",
       "      <td>0.873756</td>\n",
       "      <td>0.021067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911913</td>\n",
       "      <td>0.900351</td>\n",
       "      <td>0.912826</td>\n",
       "      <td>0.887876</td>\n",
       "      <td>0.017642</td>\n",
       "      <td>0.898433</td>\n",
       "      <td>0.915577</td>\n",
       "      <td>0.881290</td>\n",
       "      <td>0.024244</td>\n",
       "      <td>0.900351</td>\n",
       "      <td>0.912826</td>\n",
       "      <td>0.887876</td>\n",
       "      <td>0.014405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.881357</td>\n",
       "      <td>0.855290</td>\n",
       "      <td>0.869896</td>\n",
       "      <td>0.840684</td>\n",
       "      <td>0.020656</td>\n",
       "      <td>0.850914</td>\n",
       "      <td>0.870166</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.855290</td>\n",
       "      <td>0.869896</td>\n",
       "      <td>0.840684</td>\n",
       "      <td>0.016866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.872577</td>\n",
       "      <td>0.847187</td>\n",
       "      <td>0.863897</td>\n",
       "      <td>0.830476</td>\n",
       "      <td>0.023632</td>\n",
       "      <td>0.843076</td>\n",
       "      <td>0.862398</td>\n",
       "      <td>0.823753</td>\n",
       "      <td>0.027326</td>\n",
       "      <td>0.847187</td>\n",
       "      <td>0.863897</td>\n",
       "      <td>0.830476</td>\n",
       "      <td>0.019296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'batch_size': 1024, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.904111</td>\n",
       "      <td>0.882519</td>\n",
       "      <td>0.900347</td>\n",
       "      <td>0.864691</td>\n",
       "      <td>0.025213</td>\n",
       "      <td>0.880139</td>\n",
       "      <td>0.900780</td>\n",
       "      <td>0.859498</td>\n",
       "      <td>0.029190</td>\n",
       "      <td>0.882519</td>\n",
       "      <td>0.900347</td>\n",
       "      <td>0.864691</td>\n",
       "      <td>0.020586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.910078</td>\n",
       "      <td>0.892327</td>\n",
       "      <td>0.915511</td>\n",
       "      <td>0.869143</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.892262</td>\n",
       "      <td>0.912961</td>\n",
       "      <td>0.871564</td>\n",
       "      <td>0.029272</td>\n",
       "      <td>0.892327</td>\n",
       "      <td>0.915511</td>\n",
       "      <td>0.869143</td>\n",
       "      <td>0.026770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.869209</td>\n",
       "      <td>0.845150</td>\n",
       "      <td>0.866591</td>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.030323</td>\n",
       "      <td>0.845143</td>\n",
       "      <td>0.867963</td>\n",
       "      <td>0.822323</td>\n",
       "      <td>0.032272</td>\n",
       "      <td>0.845150</td>\n",
       "      <td>0.866591</td>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.024758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.911436</td>\n",
       "      <td>0.889300</td>\n",
       "      <td>0.920258</td>\n",
       "      <td>0.858343</td>\n",
       "      <td>0.043780</td>\n",
       "      <td>0.888865</td>\n",
       "      <td>0.919061</td>\n",
       "      <td>0.858668</td>\n",
       "      <td>0.042704</td>\n",
       "      <td>0.889300</td>\n",
       "      <td>0.920258</td>\n",
       "      <td>0.858343</td>\n",
       "      <td>0.035746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.893936</td>\n",
       "      <td>0.865171</td>\n",
       "      <td>0.896773</td>\n",
       "      <td>0.833568</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.860395</td>\n",
       "      <td>0.893187</td>\n",
       "      <td>0.827602</td>\n",
       "      <td>0.046376</td>\n",
       "      <td>0.865171</td>\n",
       "      <td>0.896773</td>\n",
       "      <td>0.833568</td>\n",
       "      <td>0.036492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.866907</td>\n",
       "      <td>0.843008</td>\n",
       "      <td>0.880992</td>\n",
       "      <td>0.805024</td>\n",
       "      <td>0.053717</td>\n",
       "      <td>0.841068</td>\n",
       "      <td>0.875662</td>\n",
       "      <td>0.806475</td>\n",
       "      <td>0.048923</td>\n",
       "      <td>0.843008</td>\n",
       "      <td>0.880992</td>\n",
       "      <td>0.805024</td>\n",
       "      <td>0.043860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.835761</td>\n",
       "      <td>0.795682</td>\n",
       "      <td>0.829887</td>\n",
       "      <td>0.761477</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.795812</td>\n",
       "      <td>0.830577</td>\n",
       "      <td>0.761047</td>\n",
       "      <td>0.049165</td>\n",
       "      <td>0.795682</td>\n",
       "      <td>0.829887</td>\n",
       "      <td>0.761477</td>\n",
       "      <td>0.039496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.898252</td>\n",
       "      <td>0.867775</td>\n",
       "      <td>0.904903</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>0.052508</td>\n",
       "      <td>0.860900</td>\n",
       "      <td>0.896432</td>\n",
       "      <td>0.825367</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.867775</td>\n",
       "      <td>0.904903</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>0.042872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.672109</td>\n",
       "      <td>0.620360</td>\n",
       "      <td>0.661140</td>\n",
       "      <td>0.579580</td>\n",
       "      <td>0.057672</td>\n",
       "      <td>0.621661</td>\n",
       "      <td>0.661012</td>\n",
       "      <td>0.582311</td>\n",
       "      <td>0.055650</td>\n",
       "      <td>0.620360</td>\n",
       "      <td>0.661140</td>\n",
       "      <td>0.579580</td>\n",
       "      <td>0.047089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.880056</td>\n",
       "      <td>0.844350</td>\n",
       "      <td>0.885222</td>\n",
       "      <td>0.803478</td>\n",
       "      <td>0.057802</td>\n",
       "      <td>0.843018</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>0.801617</td>\n",
       "      <td>0.058551</td>\n",
       "      <td>0.844350</td>\n",
       "      <td>0.885222</td>\n",
       "      <td>0.803478</td>\n",
       "      <td>0.047195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'batch_size': 2048, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.893259</td>\n",
       "      <td>0.859075</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.062024</td>\n",
       "      <td>0.853271</td>\n",
       "      <td>0.899084</td>\n",
       "      <td>0.807459</td>\n",
       "      <td>0.064789</td>\n",
       "      <td>0.859075</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.050642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.908315</td>\n",
       "      <td>0.858327</td>\n",
       "      <td>0.916067</td>\n",
       "      <td>0.800587</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>0.858829</td>\n",
       "      <td>0.917469</td>\n",
       "      <td>0.800189</td>\n",
       "      <td>0.082929</td>\n",
       "      <td>0.858327</td>\n",
       "      <td>0.916067</td>\n",
       "      <td>0.800587</td>\n",
       "      <td>0.066673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.780025</td>\n",
       "      <td>0.719180</td>\n",
       "      <td>0.782917</td>\n",
       "      <td>0.655442</td>\n",
       "      <td>0.090138</td>\n",
       "      <td>0.716803</td>\n",
       "      <td>0.780993</td>\n",
       "      <td>0.652613</td>\n",
       "      <td>0.090778</td>\n",
       "      <td>0.719180</td>\n",
       "      <td>0.782917</td>\n",
       "      <td>0.655442</td>\n",
       "      <td>0.073598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.707696</td>\n",
       "      <td>0.700829</td>\n",
       "      <td>0.785834</td>\n",
       "      <td>0.615824</td>\n",
       "      <td>0.120215</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.784131</td>\n",
       "      <td>0.618466</td>\n",
       "      <td>0.117143</td>\n",
       "      <td>0.700829</td>\n",
       "      <td>0.785834</td>\n",
       "      <td>0.615824</td>\n",
       "      <td>0.098155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'batch_size': 8192, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.870292</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.882874</td>\n",
       "      <td>0.730571</td>\n",
       "      <td>0.107694</td>\n",
       "      <td>0.793327</td>\n",
       "      <td>0.877534</td>\n",
       "      <td>0.709121</td>\n",
       "      <td>0.119086</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.882874</td>\n",
       "      <td>0.730571</td>\n",
       "      <td>0.087932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'batch_size': 4096, 'clip_value': 1, 'drop_la...</td>\n",
       "      <td>0.861253</td>\n",
       "      <td>0.756632</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.630003</td>\n",
       "      <td>0.179080</td>\n",
       "      <td>0.749193</td>\n",
       "      <td>0.876358</td>\n",
       "      <td>0.622027</td>\n",
       "      <td>0.179839</td>\n",
       "      <td>0.756632</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.630003</td>\n",
       "      <td>0.146218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  auc_test  \\\n",
       "58  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.686786   \n",
       "18  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.911241   \n",
       "30  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.900811   \n",
       "42  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.889792   \n",
       "54  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.846501   \n",
       "2   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.923642   \n",
       "43  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.884630   \n",
       "12  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.912061   \n",
       "9   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.916487   \n",
       "4   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.921805   \n",
       "0   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.925806   \n",
       "26  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.904174   \n",
       "1   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.925298   \n",
       "10  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.915441   \n",
       "17  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.911332   \n",
       "32  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.900088   \n",
       "51  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.861025   \n",
       "3   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.922637   \n",
       "53  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.850021   \n",
       "41  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.890501   \n",
       "34  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.898748   \n",
       "11  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.914086   \n",
       "22  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.908238   \n",
       "5   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.921788   \n",
       "31  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.900750   \n",
       "33  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.899520   \n",
       "6   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.919137   \n",
       "36  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.897936   \n",
       "23  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.907245   \n",
       "7   {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.917606   \n",
       "16  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.911338   \n",
       "20  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.908771   \n",
       "25  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.905639   \n",
       "24  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.906245   \n",
       "14  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.911699   \n",
       "52  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.853585   \n",
       "40  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.892280   \n",
       "8   {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.916831   \n",
       "29  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.902097   \n",
       "38  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.893259   \n",
       "28  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.903543   \n",
       "13  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.911913   \n",
       "44  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.881357   \n",
       "46  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.872577   \n",
       "27  {'batch_size': 1024, 'clip_value': 1, 'drop_la...  0.904111   \n",
       "19  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.910078   \n",
       "48  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.869209   \n",
       "15  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.911436   \n",
       "37  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.893936   \n",
       "49  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.866907   \n",
       "55  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.835761   \n",
       "35  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.898252   \n",
       "59  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.672109   \n",
       "45  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.880056   \n",
       "39  {'batch_size': 2048, 'clip_value': 1, 'drop_la...  0.893259   \n",
       "21  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.908315   \n",
       "56  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.780025   \n",
       "57  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.707696   \n",
       "47  {'batch_size': 8192, 'clip_value': 1, 'drop_la...  0.870292   \n",
       "50  {'batch_size': 4096, 'clip_value': 1, 'drop_la...  0.861253   \n",
       "\n",
       "    outer_auc_mean  outer_auc_max  outer_auc_min  outer_auc_std  \\\n",
       "58        0.652929       0.653418       0.652441       0.000691   \n",
       "18        0.900845       0.904539       0.897151       0.005224   \n",
       "30        0.886719       0.890499       0.882940       0.005345   \n",
       "42        0.877497       0.881620       0.873375       0.005830   \n",
       "54        0.826380       0.831962       0.820798       0.007894   \n",
       "2         0.917400       0.920491       0.914308       0.004372   \n",
       "43        0.866506       0.870379       0.862633       0.005477   \n",
       "12        0.900836       0.902177       0.899496       0.001896   \n",
       "9         0.909845       0.910799       0.908891       0.001349   \n",
       "4         0.914663       0.916327       0.912998       0.002354   \n",
       "0         0.922460       0.924191       0.920730       0.002448   \n",
       "26        0.892176       0.892375       0.891977       0.000282   \n",
       "1         0.920686       0.922044       0.919327       0.001921   \n",
       "10        0.905192       0.907154       0.903230       0.002774   \n",
       "17        0.903197       0.906559       0.899835       0.004755   \n",
       "32        0.886363       0.886437       0.886289       0.000104   \n",
       "51        0.836118       0.844347       0.827889       0.011638   \n",
       "3         0.915354       0.916212       0.914497       0.001212   \n",
       "53        0.831989       0.833729       0.830248       0.002462   \n",
       "41        0.876252       0.879857       0.872647       0.005099   \n",
       "34        0.885711       0.888505       0.882917       0.003951   \n",
       "11        0.903650       0.909977       0.897324       0.008947   \n",
       "22        0.898338       0.904510       0.892167       0.008727   \n",
       "5         0.911665       0.920194       0.903136       0.012062   \n",
       "31        0.890413       0.896148       0.884678       0.008110   \n",
       "33        0.886413       0.892424       0.880401       0.008502   \n",
       "6         0.907132       0.919315       0.894948       0.017230   \n",
       "36        0.886456       0.895879       0.877033       0.013326   \n",
       "23        0.891983       0.897953       0.886012       0.008443   \n",
       "7         0.907418       0.914936       0.899900       0.010632   \n",
       "16        0.898622       0.905014       0.892230       0.009040   \n",
       "20        0.896917       0.907133       0.886702       0.014447   \n",
       "25        0.895997       0.903174       0.888821       0.010149   \n",
       "24        0.895004       0.905634       0.884374       0.015033   \n",
       "14        0.893293       0.902717       0.883868       0.013329   \n",
       "52        0.805157       0.820677       0.789637       0.021948   \n",
       "40        0.865347       0.879487       0.851207       0.019997   \n",
       "8         0.902693       0.918672       0.886714       0.022597   \n",
       "29        0.884500       0.900606       0.868395       0.022777   \n",
       "38        0.866667       0.880210       0.853124       0.019152   \n",
       "28        0.892000       0.910245       0.873756       0.025802   \n",
       "13        0.900351       0.912826       0.887876       0.017642   \n",
       "44        0.855290       0.869896       0.840684       0.020656   \n",
       "46        0.847187       0.863897       0.830476       0.023632   \n",
       "27        0.882519       0.900347       0.864691       0.025213   \n",
       "19        0.892327       0.915511       0.869143       0.032787   \n",
       "48        0.845150       0.866591       0.823708       0.030323   \n",
       "15        0.889300       0.920258       0.858343       0.043780   \n",
       "37        0.865171       0.896773       0.833568       0.044693   \n",
       "49        0.843008       0.880992       0.805024       0.053717   \n",
       "55        0.795682       0.829887       0.761477       0.048373   \n",
       "35        0.867775       0.904903       0.830646       0.052508   \n",
       "59        0.620360       0.661140       0.579580       0.057672   \n",
       "45        0.844350       0.885222       0.803478       0.057802   \n",
       "39        0.859075       0.902932       0.815217       0.062024   \n",
       "21        0.858327       0.916067       0.800587       0.081657   \n",
       "56        0.719180       0.782917       0.655442       0.090138   \n",
       "57        0.700829       0.785834       0.615824       0.120215   \n",
       "47        0.806723       0.882874       0.730571       0.107694   \n",
       "50        0.756632       0.883260       0.630003       0.179080   \n",
       "\n",
       "    split_auc_mean  split_auc_max  split_auc_min  split_auc_std  \\\n",
       "58        0.648247       0.648409       0.648086       0.000228   \n",
       "18        0.896720       0.896910       0.896531       0.000268   \n",
       "30        0.884555       0.884826       0.884283       0.000384   \n",
       "42        0.873819       0.874143       0.873496       0.000458   \n",
       "54        0.825727       0.826464       0.824991       0.001041   \n",
       "2         0.916278       0.917346       0.915210       0.001510   \n",
       "43        0.864945       0.866428       0.863461       0.002098   \n",
       "12        0.895642       0.897171       0.894114       0.002162   \n",
       "9         0.906228       0.907861       0.904594       0.002310   \n",
       "4         0.913250       0.914964       0.911536       0.002424   \n",
       "0         0.920260       0.922063       0.918457       0.002550   \n",
       "26        0.888673       0.890631       0.886716       0.002769   \n",
       "1         0.922803       0.924874       0.920733       0.002928   \n",
       "10        0.903721       0.906079       0.901363       0.003335   \n",
       "17        0.898943       0.902235       0.895650       0.004656   \n",
       "32        0.889974       0.893886       0.886061       0.005533   \n",
       "51        0.831140       0.835577       0.826704       0.006274   \n",
       "3         0.916081       0.920680       0.911481       0.006505   \n",
       "53        0.826592       0.831330       0.821854       0.006700   \n",
       "41        0.876272       0.881036       0.871507       0.006738   \n",
       "34        0.880969       0.887067       0.874870       0.008625   \n",
       "11        0.902124       0.908608       0.895640       0.009170   \n",
       "22        0.896381       0.904195       0.888566       0.011051   \n",
       "5         0.912039       0.919941       0.904137       0.011176   \n",
       "31        0.888901       0.897802       0.880000       0.012588   \n",
       "33        0.882478       0.891679       0.873277       0.013012   \n",
       "6         0.905553       0.915406       0.895699       0.013935   \n",
       "36        0.881180       0.891262       0.871099       0.014257   \n",
       "23        0.889079       0.899524       0.878634       0.014772   \n",
       "7         0.905087       0.915956       0.894217       0.015372   \n",
       "16        0.897276       0.908207       0.886344       0.015459   \n",
       "20        0.893134       0.904587       0.881681       0.016197   \n",
       "25        0.893792       0.905847       0.881737       0.017048   \n",
       "24        0.893962       0.906541       0.881384       0.017788   \n",
       "14        0.887902       0.901040       0.874765       0.018579   \n",
       "52        0.804197       0.818252       0.790141       0.019878   \n",
       "40        0.866982       0.881046       0.852918       0.019890   \n",
       "8         0.900001       0.915255       0.884748       0.021571   \n",
       "29        0.880720       0.896188       0.865252       0.021875   \n",
       "38        0.863293       0.880051       0.846536       0.023699   \n",
       "28        0.887870       0.904817       0.870924       0.023965   \n",
       "13        0.898433       0.915577       0.881290       0.024244   \n",
       "44        0.850914       0.870166       0.831663       0.027226   \n",
       "46        0.843076       0.862398       0.823753       0.027326   \n",
       "27        0.880139       0.900780       0.859498       0.029190   \n",
       "19        0.892262       0.912961       0.871564       0.029272   \n",
       "48        0.845143       0.867963       0.822323       0.032272   \n",
       "15        0.888865       0.919061       0.858668       0.042704   \n",
       "37        0.860395       0.893187       0.827602       0.046376   \n",
       "49        0.841068       0.875662       0.806475       0.048923   \n",
       "55        0.795812       0.830577       0.761047       0.049165   \n",
       "35        0.860900       0.896432       0.825367       0.050251   \n",
       "59        0.621661       0.661012       0.582311       0.055650   \n",
       "45        0.843018       0.884420       0.801617       0.058551   \n",
       "39        0.853271       0.899084       0.807459       0.064789   \n",
       "21        0.858829       0.917469       0.800189       0.082929   \n",
       "56        0.716803       0.780993       0.652613       0.090778   \n",
       "57        0.701299       0.784131       0.618466       0.117143   \n",
       "47        0.793327       0.877534       0.709121       0.119086   \n",
       "50        0.749193       0.876358       0.622027       0.179839   \n",
       "\n",
       "    inner_auc_mean  inner_auc_max  inner_auc_min  inner_auc_std  \n",
       "58        0.652929       0.653418       0.652441       0.000564  \n",
       "18        0.900845       0.904539       0.897151       0.004265  \n",
       "30        0.886719       0.890499       0.882940       0.004364  \n",
       "42        0.877497       0.881620       0.873375       0.004760  \n",
       "54        0.826380       0.831962       0.820798       0.006445  \n",
       "2         0.917400       0.920491       0.914308       0.003570  \n",
       "43        0.866506       0.870379       0.862633       0.004472  \n",
       "12        0.900836       0.902177       0.899496       0.001548  \n",
       "9         0.909845       0.910799       0.908891       0.001102  \n",
       "4         0.914663       0.916327       0.912998       0.001922  \n",
       "0         0.922460       0.924191       0.920730       0.001999  \n",
       "26        0.892176       0.892375       0.891977       0.000230  \n",
       "1         0.920686       0.922044       0.919327       0.001569  \n",
       "10        0.905192       0.907154       0.903230       0.002265  \n",
       "17        0.903197       0.906559       0.899835       0.003882  \n",
       "32        0.886363       0.886437       0.886289       0.000085  \n",
       "51        0.836118       0.844347       0.827889       0.009502  \n",
       "3         0.915354       0.916212       0.914497       0.000990  \n",
       "53        0.831989       0.833729       0.830248       0.002010  \n",
       "41        0.876252       0.879857       0.872647       0.004163  \n",
       "34        0.885711       0.888505       0.882917       0.003226  \n",
       "11        0.903650       0.909977       0.897324       0.007305  \n",
       "22        0.898338       0.904510       0.892167       0.007126  \n",
       "5         0.911665       0.920194       0.903136       0.009849  \n",
       "31        0.890413       0.896148       0.884678       0.006622  \n",
       "33        0.886413       0.892424       0.880401       0.006942  \n",
       "6         0.907132       0.919315       0.894948       0.014068  \n",
       "36        0.886456       0.895879       0.877033       0.010881  \n",
       "23        0.891983       0.897953       0.886012       0.006894  \n",
       "7         0.907418       0.914936       0.899900       0.008681  \n",
       "16        0.898622       0.905014       0.892230       0.007381  \n",
       "20        0.896917       0.907133       0.886702       0.011796  \n",
       "25        0.895997       0.903174       0.888821       0.008287  \n",
       "24        0.895004       0.905634       0.884374       0.012275  \n",
       "14        0.893293       0.902717       0.883868       0.010883  \n",
       "52        0.805157       0.820677       0.789637       0.017921  \n",
       "40        0.865347       0.879487       0.851207       0.016327  \n",
       "8         0.902693       0.918672       0.886714       0.018451  \n",
       "29        0.884500       0.900606       0.868395       0.018597  \n",
       "38        0.866667       0.880210       0.853124       0.015638  \n",
       "28        0.892000       0.910245       0.873756       0.021067  \n",
       "13        0.900351       0.912826       0.887876       0.014405  \n",
       "44        0.855290       0.869896       0.840684       0.016866  \n",
       "46        0.847187       0.863897       0.830476       0.019296  \n",
       "27        0.882519       0.900347       0.864691       0.020586  \n",
       "19        0.892327       0.915511       0.869143       0.026770  \n",
       "48        0.845150       0.866591       0.823708       0.024758  \n",
       "15        0.889300       0.920258       0.858343       0.035746  \n",
       "37        0.865171       0.896773       0.833568       0.036492  \n",
       "49        0.843008       0.880992       0.805024       0.043860  \n",
       "55        0.795682       0.829887       0.761477       0.039496  \n",
       "35        0.867775       0.904903       0.830646       0.042872  \n",
       "59        0.620360       0.661140       0.579580       0.047089  \n",
       "45        0.844350       0.885222       0.803478       0.047195  \n",
       "39        0.859075       0.902932       0.815217       0.050642  \n",
       "21        0.858327       0.916067       0.800587       0.066673  \n",
       "56        0.719180       0.782917       0.655442       0.073598  \n",
       "57        0.700829       0.785834       0.615824       0.098155  \n",
       "47        0.806723       0.882874       0.730571       0.087932  \n",
       "50        0.756632       0.883260       0.630003       0.146218  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"df.sort_values(by=[\\\"split_auc_std\\\"], ascending=True)\";\n",
       "                var nbb_formatted_code = \"df.sort_values(by=[\\\"split_auc_std\\\"], ascending=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.sort_values(by=[\"split_auc_std\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
